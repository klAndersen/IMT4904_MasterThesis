============================================================================================================================
Note! This is a short summary of said reports, based on relevance for my master thesis. Not all quotations are marked ("quote"), 
and is not intended to be a "self-written" document. Merely as a helpful summary for myself when referencing to said report in 
my thesis.
============================================================================================================================


A Classification of Questions using SVM and Semantic Similarity Analysis 
(Jinzhong Xu, Yanan Zhou, Yuan Wang)

Question-answering (qa) system which includes question analysis, information retrieval and answer extraction. 
Question category is an important part of the question analysis, the same goes for follow-up process, since it 
affects the accuracy of the answer extraction. Challenging, since few language processing tools handle Chinese characters. 
Better to classify based on the domain of the question characteristics. The developed system is a qa for tourism.
System performance is based on accuracy of qa classification. For the application in question, svm and question semantic 
similarity was used. The SVM was trained on coarse categories, and the question semantic similarity on the sub-categories.

Original method for question classification is usually rule-based approach, where the rules decide the category.
Rule-based approach uses interrogative words and word combinations with other features of the rules extracted by experts.
Difficult to extract these rules and impossible to make all of them, which will have an impact on classification results.
Another method is using statistics (e.g. by using SVM). See p. 2 for numerical values from (Zhang and Li); first paragraph.

The question database was based on questions asked by the users of the existing system. Divided questions into 13 coarse 
categories and 150 sub-categories. Coarse category was just a category identifier, whereas sub-categories was built on sentences.
First the user question is classified into one of the 13 categories by using SVM. Thereafter it was checked if it matched 
one of those in the sub-categories.

SVM explanation on p. 2, Section IV. 
Used RBF kernel in this experiment. Notes that the selection and organization of features is the main issue when working 
with classification. Features and feature space can have a significant impact on both accuracy and efficiency of the classifier.
Compared with rule-based, question features for a specific domain can have greater benefits. To improve performance, the 
classification must improve the question characteristics. Their feature selection was based on semantic analysis and lexical analysis.
They also needed to add a step for word segmentation and Part-Of-Speech (POS) tagging (since it was Chinese text processing).

To improve classification efficiency, domain knowledge was added by using domain term concept hiearchy (basically if words 
had the same meaning or concept, then they were added to a feature vector). They used LIBSVM for coarse classification.
The sub-classification was done by measuring the similarity between the users question and those in the sub-categories 
(calculated by utilizing word similarity based on term concept hierarchy). Function words (e.g. preposition, conjunction words) 
is not relevant when measuring semantic similarity, so function words should be pruned. Terms concept hierarchy knowledge can 
be utilized to calculate the semantic similarity of two questions. When pre-processing, if there were different words in 
different questions that had the same concept, the words were replaced so that they all had the same word for that concept.

The corpus contained a training set of 3000 questions and the test set had 1500 questions. Questions were segmented, POS-tagged 
and then features were extracted. For the SVM, they used cross-validation, where the training set was dvided into five parts of 
equal size. After a classifier had been trained on the first four, the last was used to get the cross-validation accuracy (to 
ensure correct classification).

Two different experiments, the first compared feature sets, the second compared Bayesian, SVM and the two-level classification 
methods in the paper. When comparing different feature sets, the first feature was the body of question, interrogative word and 
attached elements, and the second was terms concept hierarchy based feature extraction by methods in the paper. 

The results showed that the use of terms concept hierarchy was more suitable for question classification in specific tourism domain, 
and the method of two level question classification based on SVM and question semantic similarity could improve the 
results of question classification.




A support vector machine-based context-ranking model for question answering 
(Show-Jane Yen, Yu-Chieh Wu, Jie-Chi Yang, Yue-Shi Lee, Chung-Jung Lee, Jui-Jung Liu)

QA is a method for finding the answer to a given question from an unknown amount of documents. This paper uses machine learning qa 
framework, which integrates a question classifier, simple document/passage retrievers, and context-ranking models. The question 
classifier categorizes the answer based on the given question by re-ranking passages in the context-ranking model. The model uses 
sequential labeling of tasks, which is combined with features to predict if the input passage is relevant to the question type. 

The goal of most automatic qa systems is to find the exact answer to the question asked by the user. 
Existing QA technology involves two main steps: information retrieval (IR) and information extraction (IE).
IR is used to retrieve the relevant documents after completed analysis or classification. IE then processes 
the retrieved documents to find the answers the user is looking for. High quality systems generally requires 
multiple external components.

The purpose of question classification is to detect the answer type of the input question.
    Interesting paragraph (first one) in Section 2.1 on results in regards to question classification and svm.

"
Lexical information is essential in question classification because it gives the classifier the ability to identify unknown words.
"Earlier research (Li and Roth) found that human-made related words improved classifiers, where word clusters became classifier features.
Traditional approach was using bag-of-words (BOWs) model, but this has a 1-1 relationship and cannot map when new words appear.
However, the proposed cluster-based model had n-1 relationship. where similar words were mapped to the same group. 
"

Looking for answers in a small set is easier then searching all documents. By using a document retriever, 
only documents related to the question is retrieved, where the passage retriever divides the documents into 
paragraphs and ranks them based on the given evaluation metric(s).

Extracted the following features:
- Lexical (Word form): The word form
- Part of Speech (POS): The part-of-speech tag of the word 
- Named-Entity Class (NE): The named entity tag of the word
- Term-match degree: match degree indicates how effectively the word matches one of the question terms in stem form or synonym form. seven 
    match degrees (from degree = 7 to degree = 1): named-entity match, question first noun phrase match, question term exact match, stem match, 
    synonym match, hyponym match, and hypernym match
- Token feature: The token feature involves identifying specific symbols and punctuations within the given 
    token (e.g. "$" or "%")

Combining the WordNet clusters clearly leads to higher accuracy than using the original method (from 84.40% 
to 85.60% for fine-grained and from 89.40% to 90.00% for coarse-grained). This prediction performance 
increases by combining both WordNet clusters and the related words (88.60%). Using approximately 3000 
training questions, the question classifier achieves more than 82% accuracy. 

The accuracy rates reach 84.4% and 85.6% of the accuracy rates without and with the ''auto-derived'' word 
clusters, respectively.




An Analysis of a High-Performance Japanese Question Answering System 
(HIDEKI ISOZAKI)

Fine-grained answer taxonomy improves qa performance, but difficult to create accurate answer extraction because ML methods require a lot of 
training data and training rules. In this paper, they built a fine-grained system by using a coarse-grained named entity recognizer and a 
Japanese lexicon "Nihongo Goi-taikei." They found that named entity/numerical expression recognition and word sense-based answer
extraction contributed to system performance.

The mainstream of the system is composed of four modules: Question Analysis, Document 
Retrieval, Answer-Extraction, and Answer Evaluation (AEv has several sub-modules).

The question analysis module normalized question and determined answer types. Question 
normalization was used to simply the "answer-type determination rules.". Useless expressions 
were removed from the question. Expressions with the same meaning were nromalized into one. 
After normalization, a morphological analyzer (ALTJAWS; based on Japanese lexicon) was used to divide question into words (since Japanese don't use spaces). 
"
ALTJAWS also provides part-of-speech tags, word senses, bunsetsu chunks,
and normal forms. Nihongo Goi-taikei classifies 300,000 words into about 3000
categories (word senses). Bunsetsu is a basic unit of Japanese grammar, which
consists of one or more content words optionally followed by a sequence of
function words.
"
The analyzer then detects question words (e.g. who, what, where, when) and a "focus word" 
for answer-type classification ("wa"). {the word before 'wa' is the topic, everything after is 
asking a question about/description of topic [Japanese From Zero 1, p. 42]}

Mapping of answer types; word sense ID 51 is "infant," 52 is "male infant," 53 is "female
infant," 54 is "child," 55 is "boy," and 56 is "girl." This table maps 51 and 54 to
PERSON, 52 and 55 to MALE, and 53 and 56 to FEMALE (where the numbers are the ID in a mapping 
table). These were then attached to each word.

Retrieval module is important since answer must be found within limited time (and you can't 
look through all documents). Normalized all versions (kanji, hiragana, and katakana) to kanjii.
Abandoned use of TF-IDF based paragraph retrieval becuase the paragraphs were to short to cover 
all query terms. Same problem can occur when using  passage retrieval modules. If it is too
short terms will not be covered, if it is too long, passage scores will not reflect the density distribution.
"
Many QA systems employ passage retrieval modules that split
documents into fixed-size passages of 300 words or three sentences. 
"

Answer extraction separates based on the candidate it belongs to (e.g. Location, organization, school, hospital, etc). 
Suffixes are then used to indicate which candidate group it belongs to. 
If it belongs to more than one, it is added in both. This is called suffix constraint rule. 
Use of noun-phrases in combination with suffixes.

"
Each answer candidate in a document is then represented by four compo-
nents: an answer string, its full name, its document ID, and its location inside
the document. If an answer candidate for PERSON is too short, it must be a sur-
name or a first name. In order to disambiguate the candidate, the extraction
module searches for its full name in the document. The full name is used when
the system outputs the best answers.
"
Scoring function for Answer evaluation on p. 8-9




Analysis of the Reputation System and User Contributions on a Question Answering 
Website: StackOverflow 
(Dana Movshovitz-Attias, Yair Movshovitz-Attias, Peter Steenkiste and Christos Faloutsos)

Study of the user system on StackOverflow. Analysis of SO's reputation system. 
Although most of the questions are asked by low reputation users, the average 
shows that high reputation users asks more questions. 

qa sites provides a place where people can ask either general or domain specific 
questions. Domain specific qa sites requires users to be familiar with both the 
site, rules and questions that can be asked. qa sites also works as an archive of 
knowledge which can be accessed later on. Mostly expert users that answers questions.

Active users are rewarded with reputation (rep. score depends on activity). 
Before, asking questions gave more reputation, now giving answers give more 
reputation. Expert users can be identified not only by reputation, but also by 
the badges they have.

"
Q&A based knowledge sharing communities can also be studied in the context of 
information retrieval, in which a question is a query, and the answers are its results. 
"

Showed that users are awarded more reputation for good answers than good questions. 
Users with high reputation is considered as expert users (since answers give more rep). 
 "System reputation can be considered as a measure of expertise".
 
    https://blog.stackoverflow.com/2009/04/a-day-in-the-penalty-box/


    

A Practical Guide to Support Vector Classification 
(Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin)

Documentation of SVM and libSVM; cookbook approach. Recipe for acceptable results 
(not intended for SVM researchers). Classification usually involves separating data 
into training and test set. Training set contains labeled data (expected outcome), 
whereas test set is the data we want to predict the labels for. 
SVM has four basic kernels; linear, polynomial, radial basis function (RBF), and 
sigmoid. Suggested steps for procedure when transforming and training svm. 

Recommends the use of gridsearch on C and gamma using cross-validation.
Should do a coarse grid search first, then narrow it down by basing it on 
the results from coarse search




A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type 
Methods (Hsuan-Tien Lin and Chih-Jen Lin) 

-> Skipped for now; 32 pages...




An Empirical Study on Developer Interactions in StackOverflow
(Shaowei Wang, David Lo, Lingxiao Jiang)

Framework for answering research questions, using data from SO as input. 
Looking at topics and topic modeling, to assign questions to topics (where topic labels are unknown). 
Text and graph analysis. Every question could contain normal text and code snippet. Separate processing 
of text and code. 

Three contributions:

1. We investigate the distribution of questioners and answerers and investigate their behaviors.
2. We analyze the role bias of people in StackOverflow: whether most people are predominantly questioners or answerers.
3. We employ topic modeling to assign topics to tens of thousands of questions from StackOverflow.

"
Topic modeling is a way to unsupervisedly group a pool of words into groups. It is unsupervised as there is no need for
users to provide a labeled training data containing words assigned to predefined labels.
"

Framework input: question, asked_by, list_of{answer, answered_by}. 

Data processor generates three outputs:

1. statistics: Number of times each developer answers questions, posts questions, proportion of a developer's posts that are questions.
2. Help/Directed Graph: Each node in the graph is a developer. A directed edge from developer D1 to D2 in the graph denotes that developer D1 helps developer D2.
3. Question content: Normal text and code snippet.

The second processing component analyzes question content. Two sub-steps; content pre-processing and topic modeling.
"
The content pre-processing step takes in both normal text and code, performs tokenization, stop word removal, and stemming. 
Tokenization breaks a paragraph into word tokens. Stop word removal removes commonly used words like: is, are, I, you, etc. 
Stemming reduces a word to its root form, e.g., reading to read, etc. For the code, we remove reserved keywords such as: if, while, etc.,
curly brackets, etc, and extract identifiers and comments. These are then subjected to tokenization, stemming, and stop word removal too.
"

Research questions:

What are the distributions of developers that post questions?
What are the distributions of developers that answer questions?
Do developers that ask questions answer questions too?
Do developers receiving help returns the favor?
What topics do developers ask about and what are the distributions of the topics?




A Taxonomy of Questions for Question Generation
(Rodney D. Nielsen1, Jason Buckingham, Gary Knoll, Ben Marsh and Leysia Palen)

Question generation depends on the system to which it is embedded. 
Questions are intended to evaluate knowledge, understanding and skills. 
Socratic tutoring: Questions should help students understand something they did not understand before. 
Help system: System should learn what resulted in the request for help.
Paper focuses on describing question taxonomy ofr tutoring systems (validated via human tutoring transcript analyses).
Goal was to detail HCI design. Mapping from classification of learner interactions to tutor response.
Revised the taxonomies considering redundancy, usefulness, and completeness.
If two labels had the same meaning they were merged. When categories differed, the most generic was selected.
-> Mostly primary taxonomy, p. 2, Figure 1 that is relevant for my thesis


Boosting for Text Classification with Semantic Features
(Stephan Bloehdorn, Andreas Hotho)

-> Skipped for now; 18 pages...




Chaotic time series prediction using fuzzy sigmoid kernel-based support vector machines
(liu han, liu ding, deng ling-feng)

Chaos theory, chaos system control and chaos signal prediction.
Chaos signal comes from nonlinear system as a type of broad band noise, div-dimensional, sensitive to initial conditions, 
and difficult to make long-term prediction. [In the time the paper was written], chaos theory became more important in 
communication, radar, control, signal processing, etc. Chaotic time series prediction is based on embedding theorem and 
phase space reconstruction theory, to regain chaos attractors in a higher-dimensional space.

SVM use because it has better generalization (principal of structural risk minimization), and by transforming the problem of 
optimal classification plane into problem of convex quadratic programming, svm can resolve practical problems (e.g. small data samples, 
non-linearity, high-dimension, local minimization, etc).  

When using svm, data needs to be mapped into a high-dimension featured space by using symmetric and positive semi-definite (PSD) 
kernel function to transform linear non-separable problems into linear separable. Mathematically, kernel functions must meet 
Mercer conditions (https://en.wikipedia.org/wiki/Mercer%27s_theorem). Kernels that can achieve this are linear, polynomial, Gauss 
and sigmoid (sigmoid has some restrictions, and only works under specific constraints). Sigmoid kernel is a non-linear function, 
making hardware implementation difficult and timeconsuming.

In this paper, sigmoid function is combined with fuzzy logic. Experimented on two types of chaotical time series predictions.
Found that CPU time can be decreased at the cost of prediction accuracy. 

Advantages of fuzzy sigmoid kernel:

1. Kernel function is differentiable at any point in entire region.
2. Membership functions can be easily implemented by a microprocessor (e.g. DSP).
3. Nonlinear kernel function can be approximated by a number of fuzzy membership functions with different complexities.

In the experiments, C in svm and alpa, beta kernel function was determined by using 10-fold crossvalidation. 
svm was trained using iterated re-weighted least square (IRWLS), which was more effective than quadratic programming (in 
regards to execution time and memory consumption). 




Discovering Value from Community Activity on Focused Question Answering Sites: A Case Study of Stack Overflow
(Ashton Anderson,  Daniel Huttenlocher, Jon Kleinberg, Jure Leskovec)

Many Q&A sites employ voting and reputation mechanisms as centerpieces of their design to help users identify the trustworthiness and accuracy of the content.
Since a lot of qa sites are focusing on having at least some domain experts, the questions and answers now have a lasting value. Since these are archived 
and quality measured by voting system, search engines can rank the answers. This way, users who have never been to the site can find questions similar to 
their problem, and in addition see suggestions to other solutions from the others who may have posted their answer.

Authors questions together with the set of corresponding answers. Two aspects; question level and full-site level.
The ability to see all answers (qa: 1-n) give a better context for solution(s) rather than viewing one at a time (qa: 1-1).
Also mentions reputation and vote system (question quality). Two tasks they want to solve; 

1. Prediction of long-lasting value: given the activity on a question within a short interval after it was posed, 
can we tell whether it will continue to draw attention long into the future? 

2. Prediction of whether a question has been sufficiently answered: given the answers to a question so far, and 
the activity around the question, can we tell whether the needs of the question-asker have been met yet?

First principle:
Generally expert users answers first. Latent "pyramid", where expert users are at the top. 
Question starts at the top, being looked at by expert users. Then it gets filtered down if unanswered.

Second principle:
Higher activity level signals not only question interest, but it is also beneficial to all answers given (evaluation and reputation). 
High activity can correspond to lasting value of a question. 

Subjective questions are frowned upon by SO community. Deep expertise and domain knowledge is thus often essential to
providing a good answer. 

The longer a question goes unanswered, the more likely it is that no satisfactory answer will be given (i.e. no answer will be accepted).
Expert users are more prone to be "answer-dominant", gaining reputation from answering questions. Users with over 100K reputation earn 
more from accepted and less from up-votes. Idiosyncrasy on SO, can only gain 200 rep points daily, after that you can only gain reputation 
by accepted answers or bounty. Only high-rep users hit the daily cap.

Questions on Stack Overflow are supposed to be answerable factually and objectively (if not, they are marked as a "Community Wiki" question 
and actions on them do not count towards reputation score). This creates an incentive to answer quickly, since it is likely that the first 
correct answer may well be accepted.

Feature list: 

1. Questioner features (SA ), 4 features total: questioner reputation, # of questioner's questions and answers, questioner's percentage of 
accepted answers on their previous questions.

2. Activity and Q/A quality measures (SB ), 8 features total: # of favorites, # of page views, # positive and negative votes on question, 
# of answers, maximum answerer reputation, highest answer score, reputation of answerer who wrote highest-scoring answer.

3. Community process features (SC ), 8 features total: average answerer reputation, median answerer reputation, fraction of sum of answerer 
reputations contributed by max answerer reputation, sum of answerer reputations, length of answer by highest-reputation answerer, # of comments 
on answer by highest-reputation answerer, length of highest-scoring answer, # of comments on highest-scoring answer.

4. Temporal process features (SD ), 7 features total: average time between answers, median time between answers, minimum time between answers, 
time-rank of highest-scoring answer, wall-clock time elapsed between question creation and highest-scoring answer, time-rank of answer by 
highest-reputation answerer, wall-clock time elapsed between question creation and answer by highest-reputation answerer.

Only registered users can favorite a question, whereas the full Internet population contributes to its pageviews.




Does Online Q&A Activity Vary Based on Topic: A Comparison of Technical and Non-technical Stack Exchange Forums
(Saif Ahmed, Seungwon Yang, Aditya Johri)

Growing participation in online qa forums. Classify online communities on StackExchange into two genres; technical and non-technical. 
Examines the effect of contribution by comparing differences between technical and non-technical communities (user participation). 
Started with qa for computer programming (SO, started in 2008), which later expanded by using the SO model, which is today known as StackExchange (SE).
Forums can be categorized into technical (largely professional or problem based) and non-technical (largely hobbyists or interest based).
A few highly active users are responsible for the majority of participation and overall users can be characterized as lurkers, help-seekers (askers) and givers (responders)

In (AndersonHuttenlocherKleinbergEtAl2012), authors found that, the probability of a response being chosen as the best one, depends on temporal characteristics of its arrivals, 
like response speed which could be applicable to predict long-term value of a post. Besides, the other work (SinhaManiGupta2013) shows that, successfully of being answered about 
expert topic is not only because of the technical design, but also the regular involvement of design team of that community. Mentions reputation system (motivation factor). 

Compared the forum for code-review forum (CR) as technical instance and bicycle forum (BC) as non-technical one.
Found strong correlation between number of answers given and reputation score achieved by a user which is quite similar for both the forums.




FINDING A GROWTH BUSINESS MODEL AT StackOverflow, inc.
M. Sewak et al.

As pointed out on StackOverflow's career website, by earning reputation points, "Stack Overflow Careers help top developers get great jobs at great companies."
[http://careers.stackoverflow.com/]
Stack Overflow describes itself as a fusiona wiki, blog, forum, and social rating website, emphasizing itself as a platform for open and free exchange of information.
[http://stackoverflow.com/about]

Spolsky and Atwood built Stack Overflow to provide high-quality, relevant information in a freely accessible way. They envisioned that Stack Overflow would be a combination of 
collaboration technologies, including open editing (like Wikipedia), feedback driven user ranking (like Reddit or Digg), moderated content (like blogs), and forums to create a 
distinctive format.

Users are rewarded for responding and voting with Badges and Karma. This system encourages users to return and make positive contributions to the site, greatly increasing the quality 
of responses posted. Stack Overflow immediately received positive reviews like "it's a Q&A site where the right answer isn't buried on page fifty, it's almost always at the top."
[http://blog.stackoverflow.com/2010/05/announcing-our-series-a/]

SO was optimized with Search Engine Optimization (SEO) so that pages could be indexed by search engines.

In October 2009, the company announced Stack Exchange, a service that offered whitelabel installations of the Stack Overflow platform for use by third parties on a software-as-a-service 
basis. In December 2009, Stack Exchange was selected by Google to power its Android developer support forums.

Spolsky and Atwood studied the behavior of users on similar Q&A sites. They analyzed the quality of answers, the kinds of questions asked and the visitor demographics. For instance, they 
found that Yahoo! Answers primarily attracted teenagers and Mahalo Answers contained questions related to free or discounted products and services. In contrast, Spolsky and Atwood wanted 
StackOverflow to target professional programmers. 

Building blocks:

1. Voting: To increase the quality of answers, visitors are encouraged to vote on answers they found useful. The answers are arranged by the number of votes received. 
This process ensures the best answers are displayed at the top, quickly showing the best solution in the presence of multiple possible answers. Users gain reputation 
points when others vote for their questions or answers.

2. Tags: To facilitate an easy retrieval of previous questions that have been answered, each question is associated with a variety of tags. Tags help direct questions 
to experts from a specific field, improving the quality and speed of responses. Users may also customize the Stack Overflow website by specifying tags of interest and 
filtering out uninteresting ones, making their search experience more efficient.

3.Badges: To encourage participation and improve the quality of discussion, StackOverflow introduced the concept of badges. These awards, shown on the user's profile, 
are given to users who consistently provide high quality answers or ask popular and relevant questions.

4.Karma and Bounties: Karma is a type of currency system that encourages and rewards participation. Users who earn enough karma enjoy special privileges. For example, 
only a user with sufficient karma can comment on answers and even modify questions. Users win karma by selecting the correct and most relevant answer to their questions.

5.Data Dump: Stack Overflow provides a "data dump", which stores all the site's user-generated content under a Creative Commons license.3.3 This license allows people to
share and adapt content as long as it is correctly attributed to its author. Under the Creative Commons license, any derivative works must also be distributed under an 
"open" license. Monthly snapshots of the database are available for download via BitTorrent.

6.Pre-Search: Pre-Search provides a list of potentially related questions to a user before the user is allowed to post a new question. This prevents having multiple copies 
of the same question, enabling efficient retrieval of relevant answers. 

7.Search Engine Optimization: It is critical for Stack Overflow to score top results in searches to increase its user base and to build its reputation as the "one stop shop" 
for all programming related questions. Therefore, it uses various search engine optimization techniques to increase its visibility on search engines. For instance, the URL of 
a particular page on the site contains keywords from the corresponding topic.

8.Critical Mass: Atwood and Spolsky used their blogs to promote Stack Overflow and direct traffic to it, ensuring the presence of an initial community. Atwood answered questions 
so quality answers were available on the website from the outset. 

9.Performance: Built on a Microsoft software stack, Stack Overflow achieves high performance at a very low cost. Spolsky boasts that Stack Overflow's performance is comparable to 
websites with similar traffic, but only requires one-tenth of the hardware.

As Spolsky described in a Google Tech Talk, 
"Our hope is that a large body of the questions that are in Stack Overflow will become the canonical place on the Internet to learn about very very narrow specific questions 
about very very narrow and specific programming topics."
[http://www.youtube.com/watch?v=NWHfY_lvKIQ] -> 18:15-18:29




How Do Programmers Ask and Answer Questions on the Web?
(Christoph Treude, Ohad Barzilay, Margaret-Anne Storey)

Analysis of data from SO to categorize questions that are asked. Exploration of answered/unanswered questions. 
Findings indicate qa websites are particularly effective at code reviews and conceptual questions.

"
Letovsky [8] identified five main question types: why,
how, what, whether and discrepancy. Fritz and Murphy
[4] provide a list of questions that focus on issues that occur
within a project. Sillito et al. [12] provide a similar list focusing
on questions during evolution tasks. LaToza and Myers
[7] found that the most difficult questions from a developer's
perspective dealt with intent and rationale. In their study
on information needs in software development, Ko et al. [6]
found that the most frequently sought information included
awareness about artifacts and coworkers.
"

9 SO design descisions:

1. Voting
2. Tags
3. Editing
4. Badges (karma)
5. Pre-search
6. Google UI
7. Performance (search engine optimization)
8. Critical mass: several programmers were explicitly asked to contribute in the early stages of Stack Overflow.

Research questions:

1. What kinds of questions are asked on Q&A websites for programmers?
2. Which questions are answered and which ones remain unanswered?
3. Who answers questions and why?
4. How are the best answers selected?
5. How does a Q&A website contribute to the body of software development knowledge?

qualitative coding of questions and tags; tags for topics, question coding for question nature.

Defines successful and unsuccessful questions as follows: A successful question has an accepted answer, 
and an unsuccessful question has no answer.

The first limitation lies in the small amount of data we analyzed in our random sample. However, by triangulating our findings 
through qualitative coding of tags and questions, we were able to mitigate some of these concerns. Our definitions of successful 
and unsuccessful questions are limited. They offer a first approximation, but in future research we will have to analyze the answers 
in more detail to understand the differences between successful and unsuccessful uses of Q&A websites




Information selection and use in hypothesis testing: What is a good question, and what is a good answer?
(LOUISA M. SLOWIACZEK, JOSHUA KLAYMAN, STEVEN J. SHERMAN, RICHARD B. SKOV; 1992)

Hypothesis testing includes information selection (asking questions) and the use of information (answers to questions asked).
Two experiments using different kinds of inferences (category membership of individuals and composition of sampled populations).
Third experiment showed that certain information-gathering tendencies and insensitivity to answer diagnosticity can contribute to a 
tendency toward preservation of the initial hypothesis. These results illustrate the importance of viewing hypothesistesting
behavior as an interactive, multistage process that includes selecting questions, interpreting data, and drawing inferences.

Critical elements in hypothesis testing is having a good question, and knowing what to do with the answer. 
Gives example of a fictional planet which contains only two creatures. These can only answer 'yes' or 'no', but they will 
answer truthfully. Based on a given feature list, what question would you ask to be able to classify the creature you encounter?
(ironically, I choose the same features as others had in the referenced study)

When using information-gathering strategies, results show that people are influenced by different characteristics for a question, 
when choosing what to ask (sometimes in contradiction to statistical prescriptions). Clear preference to questions that are diagnostic; 
questions which distinguish hypothesis from the alternative. 

Sensitivity to the diagnosticity of questions is important because it leads to an efficient selection of tests.
Efficient information gathering does not guarantee efficient or unbiased use of the information gathered. 
Whereas optimal information selection depends on the diagnosticity of the question, optimal revision of initial beliefs
depends on the diagnosticity of the specific answers received.

Given that different answers can have different diagnosticities, how might one measure the overall diagnostic value of a question? 
The measure must account for the chance of obtaining all possible answers and the value (e.g. probability) that answer would provide.
One way of doing this is to compare the a priori likelihood of correctly accepting or rejecting the hypothesis with and without asking 
the question (Baron, 1985, chap. 4). 
The probability of guessing the correct race would increase based on the question asked, and the answer given. E.g. starting at 0.5, 
asking Feature4; then its a 0.7 chance it will say 'yes' (and 0.3 it will say 'no'). The classification accuracy for that creature would 
then be 0.64 if it says 'yes' and 0.83 if it says 'no' (presuming that 'yes'/'no' racial guess is based on the given race with the highest 
probability for said answer). This gives a priori chance of correct guess when asking about Feature4 as: (.70 x .64) + (.30 x .83) = .70, 
an increase of .20 over the naive state (naive state was 0.5).

This problem can be re-formulated into a Hypothesis, where H0 and H1 is either 'yes' or 'no' (or more defined, the race is...).
Selection of hypothesis based on percentage/probability distribution, e.g. choosing those features that stands most out/is highly separated.
Also a question on what type of answers (and how many) one can expect from each indiviual.
Easy to assume that a good question will get a good answer (e.g. overestimation of yes/no when deciding which race creature belongs to).
Sensitivity in regards to the yes/no answer given (e.g. treating all cases as identical, etc).

Beach hypothesized that subjects regarded smaller probabilities as generally less informative, and their judgments were therefore correlated with 
the sum of the probabilities. However, a reanalysis shows that the mean responses in Beach's Figure 1 (p. 60) are better predicted by the difference
between the probabilities (26%and 8%). In other words, when judging the diagnosticity of information, Beach's subjects were influenced by the differences 
in percentages, which corresponds to the diagnosticity of the question, and not just by the appropriate ratios. The same process, we propose, underlies 
insensitivity to answer diagnosticity. People underestimate differences in the diagnosticity of "yes" and "no" answers to the same question, because the 
difference in percentages is the same for both answers.

Insufficient sensitivity to the difference in diagnosticity of different answers to a question can lead subjects to over- or underestimate the information
value of a datum. The net effect of this insensitivity depends on the questions asked. Features with symmetrical probabilities (e.g., present in 30% of one group and
70% of the other) present no difficulty: "yes" and "no" answers are in fact equally diagnostic, in opposite directions. With asymmetrical features, however, one answer
is always more diagnostic than the other (e.g., Feature 4" discussed earlier, in which the feature is present in 90% of one group and 50% of the other). There is evidence
that people prefer to ask about features with one extreme probability, compared with symmetrical questions of equal diagnosticity (see Baron et al., 1988, with regard toveertainty
bias"). Thus, people may tend to select exactly those questions for which insensitivity to answer diagnosticity leads to inaccurate inferences.

Skov and Sherman (1986) found that subjects' preference for extreme probabilities was also hypothesisdependent. Subjects tend to select questions about features
that are either very likely or very unlikely under the working hypothesis, in preference to features with similarly extreme probabilities with regard to the alternative.
 
If, as we propose, people are insufficiently sensitive to the difference in diagnosticity between different answers, the net effect will be that they will overestimate the impact of
the hypothesis-eonfirming answers relative to hypothesisdisconfirming answers. On balance, then, inferential errors will tend to be in favor of the working hypothesis. 
Thus, the combination of a preference for extreme probabilities, given the hypothesis, and insensitivity to answer diagnosticity may contribute to hypothesis preservation.

It has also been suggested that people may give more weight to data that confirm the hypothesis being tested than they do to disconfirming data.

In general, then, the subjects greatly underestimated the difference between moderately diagnostic and highly diagnostic answers. A normative difference of 19% in probability
estimates was judged as a difference of only 6%. In particular, the subjects seemed to underestimate the diagnosticity of the more diagnostic answers (i.e., when the
normative probability was .83). Individual-subject analyses confirm that most subjects did not consistently distinguish the high- and low-diagnosticity answers: Of 168
subjects, 83 (49%) at least once assigned a lower probability estimate to a high-diagnosticityanswer than to a low diagnosticity answer. 

The results of Experiments 1 and 2 suggest that when people evaluate the impact of evidence, they are insufficiently sensitive to the diagnosticity of the specific answer
received. This produces systematic errors in inferences drawn from certain kinds of questions. What are the expected consequences of these inferential errors in the evaluation of a hypothesis?

Findings also illustrate the importance of looking at a phenomenon such as insensitivity to answer diagnosticity in relationship to the broader context of processes at different stages of hypothesis
testing. These processes include gathering information (selecting questions to ask), assessing the information value of data (estimating the confidence associated with individual answers), and drawing 
inferences based on sets of data (estimating the makeup of underlying populations). People conform to normative statistical models in some aspects of hypothesis testing (e.g., sensitivity to the 
diagnostic value of questions).

People show preferences that are not dictated by normative considerations but do not necessarily lead to errors in judgment (e.g., preferences for questions for which the likely answer confirms the 
hypothesis). And in some aspects, people clearly deviate from normative standards (e.g., insufficient sensitivity to the diagnostic value of different answers to the same question). Moreover, judgments and
preferences at early stages in the hypothesis-testing process can combine with judgments at later stages to influence final inferences and beliefs and to produce effects that do not arise from any single 
component. Our findings suggest, for example, that people may arrive at an unwarranted belief that the world is as it was hypothesized to be-not necessarily because of a motivation to perceive the expected 
but because of a particular sequence of judgments and inferences, some normatively appropriate, some not.


Focal hypothesis =====> Focal hypothesis explanation, taken from: "Selective hypothesis testing" (SANBONMATSU, DAVID M and POSAVAC, STEVEN S and KARDES, FRANK R)
		A focal hypothesis is generated on the basis of some criteria or set of standards. The focal hypothesis, in turn, serves to guide the gathering and assimilation of evidence. 
		When the evidence for the focal hypothesis is sufficient or satisficing(Simon, 1956, 1979), confirmation occurs, and the search for further information is truncated. 
		Conversely, if the gathered evidence fails to support the hypothesis, a new alternative is generated. The process of testing and re-generating continues until a suitable response 
		is found or until the perceived benefits of gathering additional evidence are outweighed by the costs (Gettys & Fisher, 1979) 




Is Question Answering fit for the Semantic Web?: a Survey
(Vanessa Lopez, Victoria Uren, Marta Sabou and Enrico Motta)

Semantic web (sw); search and query has become difficult and challenging. Need user friendly UI for query and data exploration. qa ontology based on structured semantic information. 
New interest in search technologies, since we are close to reaching critical mass for large-scale distributed semantic web. 
Two most common uses of sw is interpretation of web queries/resources based on described background knowledge, and searching through large datasets/KB as alternative/complacent to current web.
Difficult for end-users to understand the complexity of the logic-based sw. Challenging to process and querying content, hard to scale models to cope with available semantic data. 

Paper presents survey of ontology-based qa systems. Looks at novel research area from two perspectives. The first is contribution to qa systems in general, and second is the potential for end-users 
to beyond the sw interfaces to help brigde gap between users and sw. Classification of a qa system or approaches to sw content query according to four dimensions  based on the type of questions (input), 
the sources (unstructured data such as documents, or structured data in a semantic or non-semantic space), the scope (domain-specific, open-domain), and the traditional intrinsic problems derived from 
the search environment and scope of the system.

The goal of qa systems is to allow users to ask questions the way they want to by using natural language (NL) to receive a related answer to their problem. 

We can classify a QA system, and any semantic approach for searching and querying SW content, according to four interlinked dimensions: 
(1) the input or type of questions it is able to accept (facts, dialogs, etc); 
(2) the sources from which it can derive the answers (structured vs. un-structured data); 
(3) the scope (domain specific vs. domain independent), 
(4) how it copes with the traditional intrinsic problems that the search environment imposes in any non-trivial search system (e.g., adaptability and ambiguity).

Input is related to the question asked by the user. The goal is to allow the user to enter their question without having to understand the complexity, e.g. SQL-queries. 
The best feature for keyword-based search is simplicity, but simplicity can also be a problem. The reason is that this simplicity removes the ability to map the relation 
between the words and context needed to intepret the entered keyword(s).

Five increasingly sophisticated types: 
- systems capable of processing factual questions (factoids), 
- systems enabling reasoning mechanisms, 
- systems that fuse answers from different sources, 
- interactive (dialog) systems 
- systems capable of analogical reasoning.

Most qa systems focuses on factoids, where you have 'WH-' queries (who, what, how many, etc.), commands (name all, give me, etc.) requiring an element or list of elements as an answer, 
or affirmation / negation questions. As pointed out in (Hunter, 2000) more difficult kinds of factual questions include those which ask for opinion, like Why or How questions, 
which require understanding of causality or instru-mental relations, What questions which provide little constraint in the answer type, and definition questions.
Focus in this paper is on factoid, namely 'What' queries. In the SW context factual QA means that answers are ground facts as typically found in KBs and provides an initial foun-dation to 
tackle more ambitious forms of QA.

qa systems can be domain-specific (closed domain) or domain-independent (open domain).

In order to take full advantage of the inherent characteristics of the semantic information space to extract the most accurate answers for the users, 
QA systems need to tackle various traditional intrinsic problems derived from the search environment, such as:

- Mapping the terminology and information needs of the user into the terminology used by the sources (1. evaluation, 2. portability/adaptability, 3. correct answer)
- Disambiguating between all possible interpretations of a user query. Ambiguity. Issues with interpretation in open-domain system, e.g. words can have different meanings. 
- Answer and source quality/trust, e.g. fusion of similar answers
- Scalability; there is a trade-off between the complexity of the querying process and the amount of data sys-tems can use in response to a user demand in a reasonable time.

What does the user know about the question being asked, and the system processing the question? E.g. natural language processing vs. users expectations to what they can ask

NL was used to access databases as early as the late 60's, early 70's. The first qa systems were domain specific, and developed in the 60's as NL interfaces to expert systems 
(BASEBALL: US baseball league and LUNAR: geological analysis of rocks from the Apollo missions).

---> NLIDB: Natural Language Interfaces to Databases

early NLIDB relied on pattern matching.  a rule says that if a user's request contains the word "capital" followed by a country name, the system should print the capital which 
corresponds to the country name, so the same rule will handle "what is the capital of Italy?", "print the capital of Italy", "could you please tell me the capital of Italy". 
drawback was that they were built for particular databases, so they were not adaptable for other domains. Configuration was troublesome due to domain-specific grammar, 
hardwired knowledge or hand-written mapping rules developed by domain experts.

next generation of NLIDBs used intermediate representation language, where user input was separated from the database structure. Since the (domain-independent) linguistic process (user input) 
was separated from the (domain-dependent) mapping process (database operations), frontends could be more portable.

The formal semantics approach presented in (De Roeck et al., 1991) follows this paradigm and clearly separates between the NL front ends, which have a
very high degree of portability, from the back end. The front end provides a mapping between sentences of English and expressions of a formal semantic
theory, and the back end maps these into expressions, which are meaningful with respect to the domain in question. Adapting a developed system to a new 
application requires altering the domain specific back end alone.

MASQUE/SQL (Androutsopoulos et al., 1993) is a portable NL front end to SQL databases. It first translates the NL query into an intermediate logic
representation, and then translates the logic query into SQL. The semi-automatic configuration procedure uses a built-in domain editor, which helps 
the user to describe the entity types to which the database refers, using an is-a hierarchy, and then to declare the words expected to appear in the 
NL questions and to define their meaning in terms of a logic predicate that is linked to a database table/view.

PRECISE maps questions to the corresponding SQL query by identifying classes of questions that are understood in a well defined sense: the paper defines 
a formal notion of semantically tractable questions. Questions are translated into sets of attribute/value pairs and a relation token corresponds to either 
an attribute token or a value token. Each attribute in the database is associated with a wh-value (what, where, etc.). PRECISE uses a lexcion to find synonyms. 
The database elements selected by the matcher are assembled into a SQL query, if more than one possible query is found, the user is asked to choose between the 
possible interpretations. However, in PRECISE the problem of finding a mapping from the tokenization to the database requires all tokens to be distinct; questions 
with unknown words are not semantically tractable and cannot be handled. As a consequence, PRECISE will not answer a question that contains words absent from its 
lexicon. 

Using the example suggested in (Popescu et al., 2003), the question "what are some of the neighbourhoods of Chicago?" cannot be handled by PRECISE because the word 
"neighbourhood" is unknown. When tested on several hundred questions, 80% of them were semantically tractable questions, which PRECISE answered correctly, and the other 
20% were not handled.

NLI have attracted considerable interest in the Health Care area. In the approach presented in (Hallet et al., 2007) users can pose complex NL queries to
a large medical repository, question formulation is facilitated by means of Conceptual Authoring. A logical representation is constructed using a query
editing NL interface, where, instead of typing in text, all editing operations are defined directly on an underlying logical representation governed by a 
predefined ontology ensuring that no problem of interpretation arises.

To reduce the formal complexity of creating underlying grammars for different domains, (Minock et al., 2008), and most recently C-PHRASE (Minock et al., 2010) 
present a state-of-the-art authoring system for NLIDB. The author builds the semantic grammar through a series of naming, tailoring and defining operations within a
web-based GUI, as such the NLI can be configured by non-specialized, web based technical teams. In that system queries are represented as expressions in an extended 
version of Codd's Tuple Calculus, which may be directly mapped to SQL queries or first-order logic expressions. Higher-order predicates are also used to support 
ranking and superlatives.

Work on qa different from querying structured data. IR perspective, processing text to find and retrieve the answer.
ARDA's Advanced Question Answering for Intelligence funded the AQUAINT program; multi-project effort for qa system performance improvement over free large heterogeneous 
collections of structured and unstructured text or media. 

Linguistic problems are common in most NL systems. Some use shallow keyword-based techniques to find interesting sentences (based on words that refers to entities that are 
the same as the answer type). Ranking is based on syntactic features such as word order or similarity to the query. Templates can be used to find answers that are just 
reformulations of the question. Most systems classify queries based on answer type (e.g. name, quantity, dates, etc). Question classes arranged hierarchically in taxonomies, 
requiring different strategies based on the question type. Utilize word knowledge through resources like WordNet or ontologies Suggested Upper Merged Ontology (SUMO) to 
get the qa type. Other options are also: named-entity (NE) recognition, relation extraction, co-reference resolution, syntactic alternations, word sense disambiguation (WSD), 
logical inferences and temporal-spatial reasoning.

qa application with text has two steps:

1. identifying the semantic type of the entity sought by the question
2. determining additional constraints on the answer entity

Hierarchies of question types based on answer looked for.

LASSO (Moldovan et al., 1999) a question type hierarchy was constructed from the analysis of the TREC-8 training data, and a score of 55.5% for short answers and 64.5% for long answers was achieved. 
Given a question, LASSO can find automatically 
(a) the type of the question (what, why, who, how, where), 
(b) the type of the answer (person, location, etc.), 
(c) the focus of the question, defined as the "main information required by the interrogation" (useful for "what" questions, which usually leave implicit the type of the answer which is sought),
(d) the relevant keywords from the question. Occasionally, some words of the question do not occur in the answer (for example, the focus "day of the week" is very unlikely to appear in the answer). 
Therefore, LASSO implements NE recognition heuristics for locating the possible answers.

FALCON the semantic categories of the answers are mapped into categories covered by a NE Recognizer. When the answer type is identified, it is mapped into an answer taxonomy, where the top categories are 
connected to several word classes from WordNet. In an example presented in (Harabaigiu et al., 2000), FALCON identifies the expected answer type of the question "what do penguins eat?" as food because "it 
is the most widely used concept in the glosses of the sub-hierarchy of the noun synset {eating, feeding}". All nouns (and lexical alterations), immediately related to the concept that determines the answer 
type, are considered among the other query keywords. Also, FALCON gives a cached answer if the similar question has already been asked before; a similarity measure is calculated to see if the given question 
is a reformulation of a previous one.

DIMAP, extracts "semantic relation triples" after a document is parsed, converting a document into triples. The DIMAP triples are stored in a database in order to be used to answer the question. The semantic 
relation triple described consists of a discourse entity, a semantic relation that characterizes the entity's role in the sentence and a governing word to which the entity stands in the semantic relation. The 
parsing process generates an average of 9.8 triples per sentence in a document. The same analysis was done for each question, generating on average 3.3 triples per sentence, with one triple for each question 
containing an unbound variable, corresponding to the type of question (the system categorized questions in six types: time, location, who, what, size and number questions).

Web qa systems have the same main components as factoid systems;

(1) query formulation mechanism converting NL query into IR query
(2) web search engine searching through related documents (instead of IR engine)
(3) answer extraction module (e.g. using WordNet or NE to classify answer type)

Mulder (Kwok al., 2001) is a QA system for factual questions over the Web, which relies on multiple queries sent to the search engine Google. To form the right queries for the search engine, the query is classified 
using WordNet to determine the type of the object of the verb in the question (numerical, nominal, temporal), then a reformulation module converts a question into a set of keyword queries by using different strategies: 
extracting the most important keywords, quoting partial sentences (detecting noun phrases), conjugating the verb, or performing query expansion with WordNet. In Mulder, an answer is extracted from the snippets or 
summaries returned by Google, which is less expensive than extracting answers directly from a Web page. Then, to reduce the noise or incorrect information typically found on the Web and improve accuracy, Mulder clusters 
similar answers together and picks the best answer with a voting procedure. Mulder takes advantage of Google ranking algorithms base on PageRank, the proximity or frequency of the words, and the wider coverage provided by 
Google: "with a large collection there is a higher probability of finding target sentences". An evaluation using the TREC-8 questions, based on the Web, instead of the TREC document collection, showed that Mulder's
recall is more than a factor of three higher than AskJeeves.

AskJeeves looks up the user's question in its database and returns a list of matching questions that it knows how to answer, the user selects the most appropriate entry in the list, and he is taken to the Web pages where the 
answer can be found. AskJeeves relies on human editors to match question templates with authoritative sites.

mentions FAQFINDER

Google itself is also evolving into a NL search engine, providing precise answers to some specific factual queries, together with the Web pages from which the answers have been obtained. However, it does not yet distinguish 
between queries such as "where Barack Obama was born" or "when Barack Obama was born"  (as per May 2011).

Current trend is to use semantics to look for web pages based on query words meaning, instead of matching keywords and using page rank (popularity). 
Also approaches focusing on obtaining structured answers to user queries from pre-compiled semantic information, used to understand and disambiguate intended meaning and relationship of query words.

START (Katz et al., 2002) answers questions about geography and the MIT infolab, with a performance of 67% over 326 thousand queries. It uses highly edited KBs to retrieve tuples in the subject-relation-object form, 
as pointed out by (Katz et al., 2002), although not all possible queries can be represented in the binary relational model, in practice these exceptions occur very infrequently. START compares the user query against 
the annotations derived from the KB. However, START suffers from the knowledge acquisition bottleneck, as only trained individuals can add knowledge and expand the system's coverage (by integrating new Web sources). 

Commercial systems include PowerSet, which tries to match the meaning of a query with the meaning of a sentence in Wikipedia. Powerset not only works on the query side of the search (converting the NL queries into database 
understandable queries, and then highlighting the relevant passage of the document), but it also reads every word of every (Wikipedia) page to extract the semantic meaning. It does so by compiling factzs - similar to triples, 
from pages across Wikipedia, together with the Wikipedia page locations and sentences that support each factz and using Freebase and its semantic resources to annotate them. The Wolfram Alpha knowledge inference engine builds 
a broad trusted KB about the world by ingesting massive amounts of information (approx. 10TBs, still a tiny fraction of the Web), while True Knowledge relies on users to add and curate information.

Ontology-based semantic QA systems (also called semantic qa systems in paper), takes NL queries and ontology as input, and returns answers from KB (where KB is based on passed ontology). Allows user to have no prior knowledge 
to vocabulary or onotology structure.

Two different main aspects for ontology-based qa systems:

(1) degree of domain customization required (correlates to retrieval performance)
(2) subset of NL understanding  (full grammar-based NL, controlled or guided NL, pattern based). Needed to reduce omplexity and the habitability problem. Main issue hindering successful use of NLI.

At one end of the spectrum, systems are tailored to a domain and most of the customization has to be performed or supervised by domain experts. For instance QACID (Fernandez et al., 2009) is based on a collection of queries 
from a given domain that are analyzed and grouped into clusters, where each cluster, containing alternative formulations of the same query, is manually associated with SPARQL queries. In the middle of the spectrum, a system 
such as ORAKEL (Cimiano et al., 2007) requires a significant domain-specific lexicon customization process, while for systems like the e-librarian (Linckels, 2005) performance is dependent on the manual creation of a 
domain dependent lexicon and dictionary. At the other end of the spectrum, in systems like AquaLog (Lopez et al., 2007), the customization is done on the fly while the system is being used, by using interactivity to learn the 
jargon of the user over time. GINSENG (Bernstein and Kauffman et al., 2006) guides the user through menus to specify NL queries, while systems such as PANTO (Wang, 2007), NLP-Reduce, Querix (Kaufmann et al., 2006) and QuestIO 
(Tablan et al., 2008), generate lexicons, or ontology annotations (FREya by Damljanovic et al.), on demand when a KB is loaded. 

see p. 9--13 for a summary of various Semantic ontology-based qa systems (listed in paragraph above); perhaps reference these as: "...various systems using different implementations are listed in \cite[p.~9--13]{XXX}. 
with a comparison of these on \cite[p.~12]{XXX}..."

qa systems vary in portability, customization and ontology domain. Common for most ontology-aware systems is that they are domain restricted (either us of one at a domain or a large one covering limited sets). 
User still has to tell the system which ontologies to use. Difficult to predict the feasibility of these models to scale up to open and heterogeneous environments, where an unlimited set of topics is covered. 

Domain-specific grammar-based systems: 
Grammar is used to syntactically analyze and interpret (presuming no linguistic ambiguities; interpretation checks how terms link to each other) the structure of a NL query.
Difficult to devise grammars that are sufficiently expressive. Grammar is often limited to syntactic structure they can understand, or they are domain independent. Since users often use limited language in interfaces, 
grammar does not need to be complete. Systems requiring users to provide domain-specific grammar is not suitable for a multi-ontology open scenario.

Pattern-matching (pm) or bag-of-words (bow) approaches:
Searches for patterns in the users query. "the more flexible and less controlled a query language is, the more complex a system's question analyzing component needs to be to compensate for the freedom of the query language"
 (Kaufmann and Bernstein, 2007). Naive and flexible pattern-matching systems work well in closed scenarios, where complexity is reduced to a minimum by only employing two basic NLP techniques: stemming and synonym expansion. 
The best feature is ontology independence, where even ungrammatical and ill-formed questions can be processed. However, lack of semantics and disambiguation hinders scalability in regards to  a large open scenario. pm or bow 
combined with the freedom of NL can result in too any interpretations of the words relation. This increases the risk of not finding correct translations and suffers from the habitability problem. (Hildebrand et al., 2007): 
"Naïve approaches to semantic search are computationally too expensive and increase the number of results dramatically, systems thus need to find a way to reduce the search space". 

Guided interfaces: 
Guided and controlled interfaces generate dynamic grammar rules for each class, property and instance, and presents the users with word options (e.g. Google search engine; word suggestion). 
All possible completions to user query is not feasible in a large multi-ontology scenario. 
"It is important to note that the vocabulary grows with every additional loaded KB, though users have signaled that they prefer to load only one KB at a time" (Kaufmann, 2009; describing GINO).

Disambiguation by dialogs and user interaction: 

Dialogs are popular and convenient for resolving ambiguous queries, especially when context and ontology semantics are not enough to choose interpretation.
However, asking users for help everytime ambiguity arises, makes it hard to use the system for qa processing in a multi-domain scenarios with multiple ontologies  
"the task of creating and ranking the suggestions before showing them to the user is quite complex, and this complexity arises [sic] as the queried knowledge source grows" (Damljanovic et al., 2010). 

Domain dependent lexicons and dictionaries:
At the expense of portability, high performance can be achieved by using domain dependent dictionaries. 
However it is not feasible to manually build, or rely on the existence of domain dictionaries in an environment with a potentially unlimited number of domains.

Lexicons generated on demand when a KB is loaded:

Efficiency of automatically generating lexicons, decreases with the onotology size and is challenging if multiple large-scale ontologies are to be queried simultaneously.
In contrast with the structured indexes, entity indexes can benefit from less challenging constraints in terms of index space, creation time and maintenance.
However, ignoring the remaining context provided by the query terms can ultimately lead to an increase in query execution time to find the adequate mappings.

Benefits of onotology-based qa (p. 19):

- Ontology independence: 
Solved portability problem by combining the knowledge encoded in ontology with the (often shallow) domain-independent syntactic parsing, 
since these are the primary sources for understanding the user query (no need to encode specific domain-dependent rules). Therefore these 
systems are practically ontology independent, less costly to produce, and require little effort to bring in new sources. Can also optimize 
performance based on manual configuration or automatic learning mechanisms based on user feedback.

- Able to handle unknown vocabulary in the user query: 
If query term is lexically dissimilar from the from the ontology's vocabulary, and it is not in any manually or automatically created 
lexicon, you can study the ontology "neighborhood" of  other query terms to find the value/relation looked for. In some cases, that may 
be all the information needed.

- Deal with ambiguities:

When ontologies are directly used to give meaning to the queries expressed by the user and 
retrieve answers, the main advantage is the possibility to link words to obtain their meaning based 
on the ontological taxonomy and inherit relationships, and thus, to deal with ambiguities more efficiently.

the main benefits of ontology-based QA systems are that they make use of the semantic information to interpret and provide precise answers to questions posed in NL and are 
able to cope with ambiguities in a way that makes the system highly portable.

Although linguistic and ambiguity problems are common in most kinds of NL understanding systems, building a QA system over the SW has the following advantages:


- Balancing relatively easy design and accuracy: 
current state of the art open systems to query documents on the Web require sophisticated syntactic, 
semantic and contextual processing to construct an answer, including NE recognition. These systems 
classify queries using hierarchies of question types based on the types of answers sought (e.g., person, location, date, etc.) 
and filter small text fragments that contain strings with the same type as the expected answers. 
In ontology-based QA there is no need to build complex hierarchies, to manually map specific answer types to WordNet conceptual hierarchies or 
to build heuristics to recognize named entities, as the semantic information needed to determine the type of an answer is in the publicly available 
ontology (ies). As argued in (Mollá and Vicedo, 2007) a major difference between open-domain QA and ontology-based QA is the existence of domain-dependent 
information that can be used to improve the accuracy of the system.


- Exploiting relationships for query translation: 
NE recognition and IE are powerful tools for free-text QA, but scaling well discovering relationships between entities is problematic.
IE methods does not always capture all the information (e.g. semantics, unknown answer forms, disregarding patterns, lack of WordNet coverage for question verb(s)).
QA systems over semantic data can benefit from exploiting the explicit ontological relationships and the semantics of the ontology schema (e.g., type, subclassOf, 
domain and range), to understand and disambiguate a query. WordNet is only used for query expansion, to bridge the gap between the vocabulary of the user and the 
ontology terminology through lexically related words (such as synonyms).

- Handling queries in which the answer type is unknown: 
What queries, in which the type of the expected answer is unknown, are harder than other types of queries when querying free text (Hunter, 2000). However, the ontology 
simplifies handling what-is queries because the possible answer types are constrained by the types of the possible relations in the ontology.

- Structured answers are constructed from ontological facts: 
Mapping arbitrary query concepts to existing ontology entities, where answers are obtained by extracting semantic entities matching the facts, or fulfill the ontological 
triples or SPARQL queries. The approach to answer extraction in text-based QA requires first identifying entities matching the expected answer in text, e.g., using the 
WordNet mapping approach. Second, the answers within these relevant passages are selected using a set of proximity-based heuristics, whose weights are set by a 
machine-learning algorithm (Pasca, 2003). Although IR methods scale well, valid answers in documents that do not follow the syntactic patterns expected by the QA system 
can be easily disregarded.

- Combining multiple pieces of information: 

Ontological semantic systems can exploit the power of ontologies as a model of knowledge to give precise, focused answers, where multiple pieces of information (that may 
come from different sources) can be inferred and combined together. In contrast, QA systems over free text cannot do so, as they retrieve pre-written paragraphs of text 
or answer strings (typically NPs or named entities) extracted verbatim from relevant text (Pasca, 2003).	

It is costly to produce large amounts of domain background knowledge for proprietary open domain approaches (domain background knowledge is a requirement).
Even though they are based on semantics, these systems does not take full advantage of the all the information available from the SW. Key difference, since they 
use an internal structure for their knowledge, and they claim ownership of a trusted and curated homogeneous KB instead of supporting users exploring distributed knowledge 
sources on the Web.

There are still several issues when attempting to benefit from using Web data to get the most accurate answer to the users query:

- Heterogeneity and openness: 
High ambiguity can cause lack of context for precision, because there are many alternatives for translations and interpretations.

- Dealing with scalability as well as knowledge incompleteness: 
Need filtering and ranking techniques to scale large data amounts. Often a large number of onotological hits with different domains across the same dataset which can be mapped to user query terms.
Un-feasible to explore all solutions to get the semantically sound mappings, but filtering and domain-coverage heuristics to shift focus to precisions requires certain assumptions about source quality. 
With too strict filtering and heuristics, recall will be affected in a noisy environment where sources contain redundant and duplicated terms, and incomplete information. The cause can be that not all 
ontological elements are populated at instance level, or lack of schema information (no domain and range for properties, or type for classes, difficult to parse literals, etc.).

- Sparseness: 
The potential is overshadowed by the sparseness and incompleteness of the SW when compared to the Web (Polleres, 2010). During the search process, it may happen that a) there are no available ontologies 
that cover the query, or b) there are ontologies that cover the domain of the query but only contain parts of the answer.



Large Scale Semi-supervised Linear SVM with Stochastic Gradient Descent
(Xin ZHOU, Conghui ZHU, Sheng LI, Mo YU)

Issue with training time over large scale data. Focus on reducing training time by using semi-supervised learning and transductive support vector machines (TSVMs).
Stochastic gradient descent (SGD), also referred to as stochastic approximation algorithms [11] has been shown to have great promise for large scale learning.
Paper is mostly equations and mathematical proofs of proposed theory; not applicable for thesis.



Latent Semantic Analysis for Question Classification with Neural Networks
(Babak Loni, Seyedeh Halleh Khoshnevis, Pascal Wiggers)

Question classification (qc): predict the entity type of the answer of a natural language question, mostly achieved by using machine learning.
Used Latent Semantic Analysis (LSA) technique to reduce the large feature space of questions to a much smaller and efficient feature space.
Two different classifiers: Back-Propagation Neural Networks (BPNN) and Support Vector Machines (SVM). Found that using LSA on question classification 
made it more time efficient and improved classification accuracy by removing redundant features. Discovered that when the original feature space is compact and efficient,
its reduced space performs better than a large feature space with a rich set of features. They also found that in the reduced feature space, BPNN was better than SVM. 
Competitive with state-of-the-art, even though they used smaller feature space.

qa is about getting an answer to a question, not a list of documents. qc maps a question to a category (pre-defined), which specifies the expected answer type.
Focus is on fact based questions. Class determination can reduce feature space, in addition to specify the search strategy. A problem with qc learning systems 
is the high dimensional feature space (typically caused by the n-grams over all the vocabulary words).

SVMs are very successful on high dimensional data since they are timely efficient especially when the feature vectors are sparse, but they still suffer from the redundant features. 
Question classification has also been done by Maximum Entropy models [7], [10], Sparse Network of Winnows (SNOW) [11] and language modeling [12].

Back-propagation neural networks are multi-layer feed forward neural networks which are trained with the backpropagation
learning rule [15]. They consist of an input layer, an output layer and one or more hidden layers. Each neuron
has a forward connection to all neurons in the subsequent layer and the importance of connections are reflected by weight
parameters. The input of each neuron is weighted sum of its input signals and the output is calculated as a function of input
signals and an optional threshold parameter.

In qc, question is represented using vector space model, the question is a vector which is described by the words inside it. Therefore, a question
x is represented by vector x = (x1, ..., xd) in which xi is the frequency of term i in x and d is the total number of terms. This representation is also referred as bag-of-words or
unigrams which is the simplest type of features that can be extracted from a question and is the most widely used feature space in document classification

Bigrams
If any two consecutive words in a question is considered as a feature, the resulting feature space is called bigrams. It is
an special case of n-grams in which any n-consecutive words are considered as a single feature. Very high dimensional, since it uses word-pairs (2 words) as feature set, 
which can cause a lot of redundant data to be included. Found that using the first words in the question as bigram feature performed as good as using all bigrams (and it reduced feature space).

the combination of wh-word and the immediate word next to it, is an informative feature in most cases. For example most of the questions which starts with “what is/are” are asking for a definition.
adopted 8 wh-words; what, which, when, where, who, how, why and rest.

word shapes (e.g. numeric, case, etc). defined shape of all the words in the question and added them to feature vector.

headwords; e.g. "What is the oldest city in Canada?" the headword is "city".

hypernyms; words with equal meaning -  a child category. hypernyms allow one to abstract over specific words
since adding the hypernyms of all the words can introduce noisy information, we only add the hypernyms of the question's headword to the feature vector.

Tested all 4 kernels of SVM, linear had the best results. Error penalty was set to 1.



Learning Question Classifiers: The Role of Semantic Informationyz
(Xin Li and Dan Roth)

Purpose of qa is to get a factual answer from a large collection of text, instead of a list of documents.
Getting answer based on type (e.g. city) or getting answer based on what the question asks for (context, definition, reasoning).
Reformulations and syntactic structure can make it hard to create a manual classifier.
The goal of the paper is to categorize questions into different semantic classes based on the possible semantic types of the answers.
They developed a hierarchical classifier guided by a layered semantic hierarchy of answer types that makes use of a sequential model for multi-class classification and the SNoW learning architecture.
qc can be seen as a text classification task, but some characteristics make it different. Questions are short, therefore contains less text. However, having short text improves accuracy and analysis.

Many important natural language inferences can be viewed as problems of resolving ambiguity, either syntactic or semantic, based on properties of the surrounding context.
E.g. POS-tags, context-sensitive spelling correction, word-sense disambiguation, word choice selection in machine translation and identifying discourse markers, etc.

Two layered taxonomy, where the hiearchy contains 6 coarse classes (ABBREVIATION, DESCRIPTION, ENTITY, HUMAN, LOCATION and NUMERIC VALUE) and 50 fine classes.
Ambiguity issue, no clear boundary between classes. Uses multi-class labels to avoid this problem. During evaluation, only top-ranked coarse and fine class were counted as correct.

- If a query starts with Who or Whom: type Person.
- If a query starts with Where: type Location.
- If a query contains Which or What, the head noun phrase determines the class, as for What X questions.

Developed a hierarchical learning classifier based on the sequential model of multi-class classification. The goal of the model is to reduce the set of candidate labels for a given question by concatenating 
a sequence of simple classifiers. The output of the classifier (class labels) is used as input for the next. Classifier output activation is normalized into a density over the class labels and is thresholded 
so that it can output more than one class label. qc is built by combining sequence of two simple classifiers; first=coarse class, second=fine class. 

In addition to words, the syntactic features for each question include POS tags, chunks (non-overlapping phrases in a sentence as defined in (Abney1991)), head
chunks (e.g., the first noun chunk and the first verb chunk after the question word in a sentence).

Lexical semantic information sources: 
(1) named entities, 
(2) word senses in WordNet (Fellbaum 1998), 
(3) manually constructed word lists related to specific answer types 
(4) automatically-generated semantically similar words for every common English word based on distributional similarity

if a word w occurs in a question, the question representation is augmented with the semantic category(ies) of the word.

Performance is evaluated by the global accuracy of the classifiers for all the coarse or fine classes (Accuracy), 
and the accuracy of the classifiers for a specific class c (Precision[c]), defined as follows:

Accuracy = # of correct predictions  /  # of predictions

precison[c] = # of correct predictions of class c  /  # of predictions of class c

Note to self: Map graph over feature impact (unprocessed, singular, all)
Also add in estimated training time for exhaustive search (e.g. ~120 minutes for SVC vs ~100 for SGD over 16k questions (since 4k = test)).

observed that classifying questions just based on question words (1) does not correspond well to the desired taxonomy, and (2) is too crude
since a large fraction of the questions are 'What questions'.

misclassifications: lack of context sensitivity in semantical analysis, necessity of identifying the question focus by analyzing syntactic
structures and ambiguity (ambiguity causes the classifier not to output equivalent term as the first choice)

goal of the work is to show that a sensible incorporation of semantic features can improve the quality of question classification significantly.




Learning Surface Text Patterns for a Question Answering System
(Deepak Ravichandran and Eduard Hovy)

Explores power of surface text patterns for open-domain question answering systems. Developed a method for learning the patterns automatically.
Tagged corpus built from Internet (hand-crafted examples of each question to Altavista). Automatical pattern extraction and standardization of returned documents.
Calculates precision for each pattern and average precision for each question type.

Use of phrases to find correct answer; e.g. "<NAME> was born in <BIRTHDATE>" <NAME> "(<BIRTHDATE>–" as regular expression to find correct answer.
Assumes each sentence to be a simple sequence of words and searches for repeated word orderings as evidence for useful answer phrases.
uses suffix trees (based on computational biology, e.g. DNA sequences) for extracting substrings of optimal length.
observed need for matching part of speech and/or semantic types, and could not handle long-distance dependencies due to wildcard matching.
cannot work for types of question that require multiple words from the question to be in the answer sentence. Lack of case-sensitivity.




Learning Word Vectors for Sentiment Analysis
(Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts)

Even though expressive content and descriptive semantic content are distinct, sentiment information can be lost. It can learn that two words are close 
(similar/equal meaning), but cannot understand that two other words are the complete opposite (negative) of those words. Dataset based on IMDB reviews.
semantic component of model shares its probabilistic foundation with LDA, but is factored in a manner designed to discover word vectors rather than latent topics.
Vector space models (VSMs) seek to model words directly. Using term frequency (tf) and inverse document frequency (idf) weighting to transform the values
in a VSM often increases the performance of retrieval and categorization systems.
model does not attempt to model individual topics, but instead directly models word probabilities conditioned on the topic mixture

25,000 movie reviews from IMDB, including at most 30 reviews from any movie in the collection. built a fixed dictionary of the 5,000 most frequent
tokens, but ignored the 50 most frequent terms from the original full vocabulary. Stemming was not applied because the model learned similar representations for words 
of the same stem when the data suggests it. Allowed non-word tokens (e.g. '!', ':-)'), since it could be sentiment relevant. Mapped ratings to [0,1].
Semantic component did not require labels.

Applied truncated SVD to a tf.idf weighted, cosine normalized count matrix, which is a standard weighting and smoothing scheme for VSM induction.
Left LDA hyperparameters at their default values. Used 10-fold CV.
Constructed dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy




LIBLINEAR: A Library for Large Linear Classification
(Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin)

large-scale linear classification, supports logistic regression and linear support vector machines. 
Inherits many of the features from libsvm.
supports two popular binary linear classifiers: LR and linear SVM.



LIBSVM: A Library for Support Vector Machines
(Chih-Chung Chang and Chih-Jen Lin)

Support Vector Machines (SVMs) are a popular machine learning method for classification, regression, and other learning tasks. Started work in 2000. Supports:

1. SVC: support vector classification, (two-class and multi-class). 
2. SVR: support vector regression. 
3. One-class SVM.

Classification: Accuracy = (# correctly predicted data  /  # total testing data) * 100
Regression: outputs MSE (mean squared error) and r2 (squared correlation coefficient).

Mostly equations and mathematical theorems and proof in relation to system documentation. 




Mining Stack Exchange: Expertise Is Evident from Initial Contributions
(Daryl Posnett, Eric Warburg, Premkumar Devanbu, Vladimir Filkov)

Participants are not compensated for their services and anyone can freely gain value from the efforts of the users; Stack Exchange is therefore a gift economy.
Since users are rewarded with votes and reputation, Stack Exchange functions as a learning community (creating a valuable qa archive). 
Wants to find out if the product quality (delivered answers improves) and if the answers provided by users improve over time.
Found that overall answer scores decrease and users relationship to the community is unrelated to answer quality. Proves that answering skill (getting high average answer scores) 
is there from the beginning, and those with bad answers/low scores are continous (no indication of change in quality).

Stackexchange ranked 2nd among reference sites, 4th among computer science sites, and 97th overall among all websites.
Jeff Atwood stated that the goal the community is not just to provide quality answers to the users, but also, when it pertains to software programmers,
to trick them into becoming better communicators. Since programmers constitute most of its users, one of the original goals of Stack Exchange is to improve over time people's
abilities to provide better answers.

"Linus' Law" - claiming that "with many eyeballs, all bugs are shallow"; suggesting that program bug-fixing efforts are facilitated by the participation of more users.

Stack Exchange can be viewed as an instance of an online learning community, where individuals help each other gain expertise, both in terms of domain knowledge, and in
answering questions well and to the point. It has long been argued that people learn better in co-operating groups than on their own.
Important design goals of learning communities: help users find relevant content; help users find people with relevant knowledge and motivate people to learn.

Used SE dataset to construct posting history for each user over time. Used data from 'serverfault.com' (they didn't use SO due to its size being to computationally expensive).




Modeling Problem Difficulty and Expertise in StackOverflow
(Benjamin V. Hanrahan, Gregorio Convertino, Les Nelson)

Focus is on developing indicators for problems and experts. Examine how complex problems are handled and shared between experts. 
Answering questions should be parallell to the experts knowledge, e.g. complex problems are handled by (domain) expert users, and basics by those that know the answer.

Duration between the when a question was first posted and when an answer was accepted by the poster as a proxy for difficulty.
Extracted events for the questions, to construct a language to describe a given questions activity. Used the following grammar:

- Actor Type: Questioner (user posting the question), Accepted Answerer (user posting accepted answer), Unaccepted Answerer (users posting the other answers), and Someone 
		(participants in the question (e.g. comments)). 
- Action: Posting, Commenting, Editing, Answering, Accepting, Voting Up, and Voting Down. 
- Object: Question or an Answer.




N-Gram-Based Text Categorization
(William B. Cavnar and John M. Trenkle)

Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form.
Paper is about a N-gram-based system for text categorization that is tolerant of textual errors. The system worked well for language classification,
in one test it got 99.8% classification rate on Usenet newsgroup articles (written in different languages). It also worked well for computer-oriented articles (subject) 
where the highest classification rate was 80%.

text categorization: an incoming document is assigned to some pre-existing category. characteristics:

- categorization must work reliably in spite of textual errors.
- categorization must be efficient, consuming as little storage and processing time as possible, because of the sheer volume of documents to be handled.
- categorization must be able to recognize when a given document does not match any category, or when it falls between two categories. This is because
  category boundaries are almost never clearcut.

N-gram is an N-character slice of a longer string. the term can include the notion of any co-occurring set of characters in a string (e.g., an N-gram made up of the first and third character of a word), 
but in this paper they use the term for contiguous slices only. Their system use N-grams of several different lengths simultaneously. Appends space to beginning/end of string (_ = space):
bi-grams: _T, TE, EX, XT, T_
tri-grams: _TE, TEX, EXT, XT_, T_ _
quad-grams: _TEX, TEXT, EXT_, XT_ _, T_ _ _
---> a string of length k, padded with blanks, will have k+1 bi-grams, k+1tri-grams, k+1 quad-grams, etc.

Re-statement of Zipf's Law: The n'th most common word in a human language text occurs with a frequency inversely proportional to n.
This means that there is always a set of words which dominates most of the other words of the language in terms of usage frequency.
smooth continuum of dominance from most frequent to least. implies that classifying documents with N-gram frequency statistics will 
not be very sensitive to cutting off the distributions at a particular rank. if we are comparing documents from the same category they 
should have similar N-gram frequency distributions.

determined the true classification for each test sample semi-automatically.

- classification procedure works a little better for longer articles
- classification procedure works better the longer the category profile it has to use for matching. there were some interesting anomalies (using more N-gram
frequencies actually decreased classification performance). caused by some articles having multiple languages even though mixed types were removed

also used the classification system to identify the appropriate newsgroup for newsgroup articles (collected article samples from five Usenet newsgroups).
Chose these five because they were all subfields of computer science, and wanted to test how the system could confuse newsgroups that were somewhat closely related
removed the usual header information, such as subject and keyword identification, leaving only the body of the article (to prevent influental matches). 

primary advantage of this approach is that it is ideally suited for text coming from noisy sources such as email or OCR systems.
Possible to achieve similar results using whole word statistics (using frequency statistics for whole words), but several issues exists;

1. The system would be much more sensitive to OCR problems (one wrong character, and the word is counted separately). 
2. Certain articles could be too short to get representative word statistics. there are simply more N-grams in a given
   passage than there are words, and there are consequently greater opportunities to collect enough N-grams to be significant for matching.
3. By using N-gram analysis, you get word stemming for free, since the plurals will all contain the base form. To get the same results 
   with whole words, the system has to do word stemming, requiring knowledge about the language the documents were written in. 
   Whereas N-gram frequency approach provides language independence for free.
4. Other advantages of this approach are the ability to work equally well with short and long documents, and the minimal storage and computational requirements.




One-Class SVMs for Document Classification
(Larry M. Manevitz, Malik Yousef)

One-class training uses only positive examples.  

First, all the words in the training documents were stemmed (to avoid duplicates and remove basic "stop words") and then sorted by document-frequency (i.e. the number of documents it appears in).
Thereafter, the top m words were selected based on "dictionary" frequency (called "keywords"). The following are the different represenations that were used for document classification:

1. Binary representation: Choose the m dimensional binary vector where the i'th entry is 1 if the i'th keyword appears in the document and 0 if it does not.
2. Frequency representation: Choose the m dimensional real valued vector, where the i'th entry is the normalized frequency of appearance of the it'h keyword in the specific document.
3. Tf-idf representation: Choose the m dimensional real valued vector, where the i'th entry is given by the formula:

	tf - idf(keyword) = frequency(keyword) * [log( n  /  N(keyword) ) + 1]
	
where n is the total number of words in the dictionary and N is a function giving the total number of documents the keyword appears in.
4. Hadamard representation (discovered experimentally): Consists of the m dimensional vector where the i'th entry is the product of the frequency of the i'th keyword in the document and its frequency 
over all documents (in the training set).

Used Reuters dataset, which consists of preclassified, short articles. used 25% of the positive data from the training set to train; and then tested the filters on the remaining 75% of the data set.
Used libsvm v2.0, with standard parameters. One-class SVM requires the number of features, a kernel and the necessary kernel parameters. Tried linear, sigmoid, polynomial and radial basis kernels 
with standard parameters. Feature represenation used the 4 listed above, with the following feature count:  10, 20, 40, 60 and 100.

Compared one-class svm against previous research done by the authors, where they proposed a one-class neural network classification scheme.
Sch¨olkopf algorithm was sensitive to parameters, but could under proper choices give best results. Worked best under binary representation, extremely poor under the others.




Predict Closed Questions on StackOverflow
(Galina E. Lezina Artem M. Kuznetsov)

goal of paper is to build a classifier that predicts whether or not a question will be closed given the question as submitted, along with the reason that the question was closed.
Work was based on task posted on Kaggle (dataset was retrieved (and created?) by Kaggle). Their solution was submitted after deadline, and the best position was 5th place (0.31467 points).
Questions on StackOverflow can be closed as off topic (OT), not constructive (NC), not a real question (NRQ), too localized (TL) or exact duplicate. Exact duplicate reason
was excluded from competition because it depends on posts history (present in SO db dump, but size~=6GB requiring too many resources to analyze). 

Posts are closed based on user votes (vote for close). User feedback = post quality. Link-analysis(https://en.wikipedia.org/wiki/Link_analysis); If answerers who write good answers, can the 
same be said for questions? E.g. if a user never asked a bad question, would they ever ask one in the future? 

Compared Random Forests, SVM and LibLinear. Used Vowpal Wabbit (VW), a modified stochastic gradient descend algorithm. VW streams examples to an online learning algorithm instead of using parallelization.

Feature list (p. 3-4): 
- Users: describes user parameters on StackOverflow server such as reputation, personal information completeness and interaction between all users.
- Posts: information about the title and body contents. In addition to direct representation the text as a vector using tf-idf or LDA  .
- Tags: information about the tags affixed to the posts by its owner.




Predicting Tags for StackOverflow Posts
(Clayton Stanley, Michael D. Byrne)

Used the SO dataset to develop an ACT-R inspired Bayesian probabilistic model that could predict hashtags based on post author. 
The model is 65% accurate when tasked to predict one tag per post on average. The model chooses the tag that has the highest log odds of being correct, 
based on the tag's log odds of occurrence and adjusting for the log likelihood ratio of the words in the post being associated with the tag.

Modeling tag creation accurately (as opposed to modeling tag growth and behavior after creation) could allow a system to predict the hashtag that a user should use, given the contextual clues available.
Introudcing users to tags (and related communities) that could be of interest. Can be used in tag cleanup systems to correct mistags, or give suggestions. 
Choose SO dataset because the tags are constrained. Used dataset from April 2011.

Tag prediction model is a cognitively-inspired Bayesian probabilistic model based on ACT-R's declarative memory retrieval mechanisms. 
Hypothesis: Tags used for posts are based on a tag's history of prior use and the strength of association between the tag and the content of words in the title and body of the post.




Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood methods
(John C. Platt)

Modifications to SVM yielding posterior probabilities, while maintaining sparseness. Training SVM, then train parameters using sigmoid function to map SVM output into probabilities. 
Comparison of classification error rate and likelihood score for SVM and sigmoid vs kernel method training with regularized likelihood error function. Tested on three data-mining-style 
data sets. Experiment performed using model-trust minimization algorithm. Re-use of training data for SVM, but issues with bias. For linear SVM, bias is usually not severe. This is not 
the case for non-linear SVM, since it can lead to disastrously biased fits. Therefore un-biased training set must be used for non-linear SVM. 
Two experiments; Comparing SVM+Sigmoid against plain SVM misclassification rate and comparing SVM+Sigmoid against a kernel machine trained to explicitly maximize a log multinominal likelihood. 
Found three interesting results; 1) Sigmoid sometimes improves error rate of raw SVM, 2) Sigmoid produces probabilities of roughly comparable quality to the regularized likelihood kernel method, 
and 3) neither SVM+Sigmoid nor regularized kernel machine is completely dominant method for either error rate or log likelihood.



Question Answering as Question-Biased Term Extraction: A New Approach toward Multilingual QA
(Yutaka Sasaki)

Question Answering (QA) architecture is a cascade of the following building blocks:
- Question Analyzer: analyzes a question sentence and identifies the question types (or answer types).
  Question types consist of named entities(e.g. PERSON, DATE, and ORGANIZATION), numerical expressions (e.g., LENGTH, WEIGHT, SPEED), and class names (e.g., FLOWER, BIRD, and FOOD).
- Document Retriever: retrieves documents related to the question from a large-scale document set.
- Answer Candidate Extractor: extracts answer candidates that match the question types from the retrieved documents.
- Answer Selector: ranks the answer candidates according to the syntactic and semantic conformity of each answer with the question and its context in the document.

question-type system restricts the range of questions that can be answered by the system, which makes it problematic for QA system developers to carefully
design and build an answer candidate extractor that works well in conjunction with the question type system. This problem is particularly difficult
when the task is to develop a multilingual QA system to handle languages that are unfamiliar to the developer.

The paper regards qa as Question-Biased Term Extraction (QBTE). This new QBTE approach liberates QA systems from the heavy burden imposed by question types.
employs the machine learning technique Maximum Entropy Models (MEMs) to extract answers to a question from documents based on question features, document features, and the combined features.

Question analysis is a classification problem that classifies questions into different question types. Answer candidate extraction is also a classification problem that classifies words into answer types
(i.e., question types), such as PERSON, DATE, and AWARD. Answer selection is an exactly classification that classifies answer candidates as positive or negative. 
In the QBTE approach, these three components, i.e., question analysis, answer candidate extraction and answer selection, are integrated into one classifier.

Three groups of features as features of input data:
- Question Feature Set (QF): A set of features extracted only from a question sentence. This feature set is defined as belonging to a question sentence.
- Document Feature Set (DF): Feature set extracted only from a document. Using only DF corresponds to unbiased Term Extraction (TE).
- Combined Feature Set (CF): Contains features created by combining question features and document features.




Question answering using statistical language modelling
(Matthias H. Heie, Edward W.D. Whittaker, Sadaoki Furui)

statistical approach to qa; building a robust systems for many languages without the need for highly tuned linguistic modules. 
Word tokens and web data are used extensively but .neither explicit linguistic knowledge nor annotated data is incorporated.
Focuses on factoid questions.

The earliest qa system was developed in the 1960s (e.g. Baseball); these were usually narrow and relying on database queries to find answers.
Database query front-ends, were typical for systems developed from the 1960s through the 1980s. NLP gave way to more empirically driven research 
in the 1990s, open-domain QA systems were developed, which relied on more shallow linguistic processing and IR on unstructured data corpora. 
An early example is Murax (Kupiec, 1993), an open-domain QA system which combined robust linguistic methods with an IR system in order to find answers in
an online encyclopaedia. TREC classified questions into two types; Factoid and List. Factoid questions are questions that ask about a simple fact, and
typically can be answered with only a few words (e.g. "How far is it from Earth to Mars?"). List questions require the system to return the set of entities 
that satisfy a given criterion (e.g. "Name albums released by The Beatles"). NLP techniques employed by QA systems typically include part-of-speech (POS) 
tagging, named entity (NE) extraction, parsing and query expansion.

The authors takes a statistical, noisy-channel approach and treat QA as a whole classification problem, presenting a fully data-driven mathematical model for 
estimating the probability of a candidate answer given a question. They only use word tokens in the system and do not employ NE extraction or any other linguistic 
information, e.g. from semantic analysis or question parsing; nor hand-crafted or annotated lexical resources such as WordNet.

Used the factoid questions from the TREC QA tracks between the years 2002 and 2006 in their experiments. 
A mathematical model for Answer Extraction was derived, which estimates P(A|Q), the probability of an answer candidate A given a question Q, and is decoupled
into two independent models: a retrieval model and a filter model. Results showed that best performance is achieved when using web data, implying the system's 
ability to exploit data redundancy. Achieved what they call "data expansion", as opposed to query expansion; the answer occurs in the surroundings of several possible combinations of query terms.
Analysis showed that the retrieval model is only able to get a correct answer for 52% of the questions when retrieving 100 answer candidates.
QA model presented in this paper uses classes of q-and-a pairs for training.




Question Classification using HeadWords and their Hypernyms
(Zhiheng Huang, Marcus Thint, Zengchang Qin)

Proposes head word feature and present two approaches to augment semantic features of such head words using WordNet. 
Their linear SVM and Maximum Entropy (ME) models reaches an accuracy of 89.2% and 89.0% over a standard benchmark dataset. Used Libsvm.

Question classification is not trivial; simply using question wh-words can not achieve satisfactory results. Difficult to classify "what" and "which" type questions.
Even though multiple questions ask for the same answer, wording and syntactic structures can make it difficult to classify.


Due to the large number of features in question classification, one may not need to map data to a higher dimensional space. It has been commonly accepted that the linear kernel of
	K(x_i, x_j ) = x_i^T*x_i 
is good enough for question classification.


Used five binary feature sets; 

1. Question wh-words (eight types): what, which, when, where, who, how, why, and rest (rest being the type does not belong to any of the previous type).
2. Head words: one single word specifying the object that the question seeks to avoid misclassification of entities. Requires syntactic parser.
3. WordNet semantic feature for head words: Use of WordNet for word similarity distance, computing the similarity between the head word of such question and each description word in a question categorization.
4. Word N-grams: Used to provide word sense disambiguation for questions
5. Word shapes: Five word shape features; all upper case, all lower case, mixed case, all digits, and other.

WordNet is a large English lexicon in which meaningfully related words are connected via cognitive synonyms (synsets). 
The WordNet is a useful tool for word semantics analysis and has been widely used in question classification.
Using WordNet for hypernyms; Y is a hypernym of X if every X is a (kind of) Y.

For example, the hierarchies for a noun sense of domestic dog is described as: 
dog -> domestic animal -> animal, 
while another noun sense (a dull unattractive unpleasant girl or woman)  is organized as:
dog -> unpleasant woman -> unpleasant person.

Unigram forms the bag of words feature, and bigram forms the pairs of words feature, and so forth. Considered unigram, bigram, and trigram.

Two experiments to test classifier accuracy; evaluation of individual contribution of different feature types to question classification accuracy and in the second feature sets were 
incrementally fed to the SVM and ME (default parameter values for both).




Question Classification using Support Vector Machines
(Dell Zhang, Wee Sun Lee)

Experimented with five machine learning algorithms: Nearest Neighbors (NN) (simplified version of kNN), Naive Bayes (NB), Decision Tree (DT), Sparse Network of Winnows (SNoW), and Support Vector Machines (SVM) 
using two kinds of features: bag-of-words and bag-of ngrams.

Using a search engine to retrieve list of documents vs getting the actual answer to the question. To answer a question, you need to understand what answer the question asks for. 
Classifying the question into semantic categories to understand what answer type is expected.
Common words like 'what', 'is', etc. should be neglected for document classification, but these "stop-words" are actually very important for question classification.
Writing heuristic rules for question classification can take time, and easily become complicated.

Considers only the surface text feature for each question, where two kinds of features are extracted; bag-of-words and bag-of-ngrams (all continuous word sequences in the question). 
Every question is represented as binary feature vectors, because the term frequency (tf) of each word or ngram in a question usually is 0 or 1. Uses libsvm.

NB is regarded as one of the top performing methods for document classification.
Sparse Network of Winnows (SNoW) algorithm is specifically tailored for learning in the presence of a very large number of features and can be used as a general purpose multiclass
classifier. The learned classifier is a sparse network of linear functions.
Parameters for all algorithms was default values. 

SVM: observed that the bag-of-ngrams features are not much better than the bag-of-words features. The SVM based on linear kernel turned out to be as good as the SVM based on polynomial kernel, 
RBF kernel and sigmoid kernel.

Syntactic structures; proposed using a special kernel function called tree kernel to enable the SVM to take advantage of the syntactic structures of questions.




Random Search for Hyper-Parameter Optimization
(James Bergstra, Yoshua Bengio)






























































