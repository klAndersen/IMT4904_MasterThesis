%\chapter{Methodology}
\label{chap:chapter3}

\section{Dataset and MySQL Database}
\label{sec:dataset_and_db}

\subsection{Dataset}
The dataset contains all information that is currently available in the \gls{se} community (at the time the dataset was created). 
The following is a list of the tables found in the dataset:
\begin{itemize}
	\item Badges: Badges awarded to users.
	\item Comments: Comments given either to a question or an answer.
	\item Posts: Posts on \gls{se}, this contains both questions and answers.
	\item Posthistory: The history of a given post (e.g. edits, reason for closing, etc.).
	\item Postlinks: Link to other Posts (e.g. duplicates).
	\item Users: Information about the given user registered at the given community.
	\item Votes: Type of vote given to a Post (e.g. up/down, vote to close, etc.).
\end{itemize}
In the beginning, the dataset that was used was downloaded in August 2015. 
However, since this turned out to be outdated, the latest dataset was downloaded from (\url{https://archive.org/details/stackexchange}) on 30. March 2016. 
The dataset comes in zip-files, where each zip-file contains all the rows found in the given table. 
These rows are presented in an XML file, as shown in Listing \ref{lst:so_xml_file}.
% listing
\begin{lstlisting}[caption={Content in stackoverflow.com-Tags.xml}, label={lst:so_xml_file}, language={XML}] 
<?xml version="1.0" encoding="utf-8"?>
<tags>
<row Id="1" TagName=".net" Count="227675" 
ExcerptPostId="3624959" WikiPostId="3607476" />
<row Id="2" TagName="html" Count="511091" 
ExcerptPostId="3673183" WikiPostId="3673182" />
...
</tags>
\end{lstlisting}

\subsection{MySQL Database}
In the beginning, the issue was getting access to the file and see how it looked like. 
Since most of these XML files had a large file size (ranging from 3,9 MB to 71,9 GB) none of the editors could open them. 
Attempting to open them through Python code also failed, since there was not enough memory to process everything. 
The only solution was therefore to create a MySQL database that could contain all the data. 
\vspace{0.5em}\newline
Setting up the MySQL database was not a straight forward process. 
The operative system I was running was Arch Linux, where they had switched from using Oracle's MySQL to 
MariaDB\footnote{
	See \url{https://wiki.archlinux.org/index.php/MySQL}.
	}. 
One of the main problems was the available storage space\footnote{
	The HDD with Arch Linux installed had a disk size of 500 GB, with four partitions; root, var, swap and home. 
	40 GB was used for /root and /var, 12 GB was used for swap and the remainder was used for /home.
	} 
and the varying file sizes. 
Some of the issues were mainly connection timeout, no more disk space and connection loss (e.g. "Error Code: 2013. Lost connection to MySQL server during query"). 
To avoid losing the connection to the database, the timeout values had to be changed in MySQL Workbench (shown in Figure \ref{fig:mysql_wb_settings}).
% figure showing the settings for MySQL Workbench timeout values
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{mysql_wb_settings}
	\caption{MySQL Workbench: Setting timeout values to avoid connection loss}
	\label{fig:mysql_wb_settings}
\end{figure}
% space

\noindent
The next problem was the lack of disk space. 
MySQL by default stores all databases and belonging tables in /var/lib/mysql/, and it also creates temporary backup files 
(where the file size is equal to the size of the current database). 
Since the default folder for temporary files was on /root, the disk space was used up in less than 30 minutes. 
Therefore, two things needed to be done. First, disable the storage of temporary files, and secondly change the storage location for the database. 
The problem when tinkering with the configuration file is that things easily break. 
Which is what happened, and a clean install was needed for both MariaDB and MySQL (the changed settings can be seen in Listing  \ref{lst:mariadb_config_file}). 
The final step was to create symbolic links that linked the database to the  location where the tables were stored 
(this has to be done before creating the tables, if not MySQL  Workbench will store the tables in /var/lib/mysql/)\footnote{
	It should be noted that after an upgrade of MariaDB, MySQL and MariaDB could no longer find the tables, even if they still were in the /home/mysql/ folder. 
	It is therefore advisable to dump the database after inserting all the tables, since it goes a lot faster to restore the database from dump rather than insertion from XML files.
	}. 
% listing of changes made to config file (MariaDB)
\begin{lstlisting}[caption={Changes made to config file: /etc/mysql/my.cnf}, label={lst:mariadb_config_file}] 
# disable storage of temporary files
#tmpdir = /tmp/		  
# disable storage of log files
#log-bin = mysql-bin  

# set directory for storing database files
datadir = /home/mysql 
\end{lstlisting}

% Listing showing the SQL for loading XML row data into MySQL
\begin{lstlisting}[caption={Load XML file into a table in the MySQL database}, label={lst:load_xml_file}, language={SQL}] 
LOAD XML LOCAL INFILE 
path_to_xml_file
INTO TABLE db_table
ROWS IDENTIFIED BY '<row>';
\end{lstlisting}

\noindent
Listing \ref{lst:load_xml_file} shows how the files were loaded into the tables, and the complete database can be seen in Appendix \ref{app:mysql_database}, p.~\pageref{app:mysql_database}. 
Since the Posts table is large (\textasciitilde 29,5 million rows) and it contains both questions and answers, two new tables were created; 
"posvote\_Posts"\footnote{
	The Posts table has a file size of \textasciitilde 43,6 GB, 
	whereas posvote\_Posts file size is \textasciitilde 11,2 GB. 
	negvote\_Posts has a file size of \textasciitilde 1,33 GB.
	} 
and "negvote\_Posts". 
posvote\_Posts contains questions with a score higher then zero (score > 0) and negvote\_Posts contains all questions with a score lower then zero (score < 0).

\section{Development process} 
\label{sec:development_process}
When starting the development, the focus was on retrieving the data from the database, and processing it for text analysis. 
To be able to store all the retrieved columns and the belonging rows without creating object classes, the pandas.DataFrame\footnote{
	Pandas: \url{http://pandas.pydata.org/}.
	} 
was used. 
% add something a bit more detailed about pandas; what makes it good, etc.

The questions retrieved needed to be processed before any analysis could be done. 
The reason for this is because the questions was written as HTML (including HTML entities). 
An example is shown in Listing \ref{lst:unprocessed_question}. 
Every question starts with the <p> tag, and if the question contains code samples, these are wrapped with a <code> tag. 
To convert the HTML text into readable text, a HTML parser class was created (based on answer by \cite{Eloff2009}).
\newpage\noindent
% listing with question before HTML is removed
\begin{lstlisting}[caption={Question before HTML is removed (Question ID: 941156)}, 
label={lst:unprocessed_question}, basicstyle=\small] 
<p>
Why do we need callbacks in ASP.NET or any server side technology?
</p>&#xA;&#xA;<p>One answer can be, to achieve asynchronous calls. 
</p>&#xA;&#xA;<p>But I am not satisfied with this answer.</p>
&#xA;&#xA;<p>Please explain it in a different way.
</p>
&#xA;
\end{lstlisting}
% space

\noindent
To process the questions, CountVectorizer from scikit-learn was used. 
CountVectorizer uses the vocabulary found in the text and counts the frequency of each word \cite{Scikitlearn.org2016b} \cite[see~4.2.3]{Scikitlearn.org2016}. 
When looking at this vocabulary, a lot of of un-important words was found (a lot which came from the code samples) in some of the questions. 
At first all code samples were removed from the text, but later on they were replaced with the value 'has\_codeblock', indicating that this question contained one or more code samples. 
This was achieved by using a combination of lxml\footnote{lxml: \url{http://lxml.de/}} and bs4\footnote{BeatifulSoup: \url{https://www.crummy.com/software/BeautifulSoup/}} (BeatifulSoup). 
lxml was used to construct an XML tree containing all the tags (to be able to retrieve the content by searching for a given tag), and bs4 was used for beautifying the HTML 
(since in some cases an error was thrown complaining about "Missing end tag").
\vspace{0.5em}\newline
However, for some questions, part of the text was lost, and for others, some <code> tags was not removed. 
On inspection, it was found that the trailing text following the <code> samples was stored in a .tail attribute. 
Since the <code> was removed, the .tail attribute was also removed. 
This was fixed by storing the the content of the <code> .tail attribute into its <parent>\footnote{
	It was also necessary to check if the <parent> had a .tail, if not, the .tail attribute had to be set for the <parent> to avoid the error: 
	"NoneType + str: TypeError".
	} 
(where <parent> is the tag that contained the given <code></code>) .tail attribute. 
As for the non-complete removal of <code> tags, this error mostly occurred for code samples that contained XML or HTML code\footnote{
	One example is this question: \\
	\url{http://stackoverflow.com/questions/19535331/print-page-specific-area-or-element}.
	}, 
because the lxml parser failed. 
The solution was to replace the lxml parser with bs4 and just change the content of the <code> tag to the value 'has\_codeblock'.
\vspace{0.5em}\newline
Considering the size of the dataset, and that the source code was hosted on GitHub, I was hesitant to store the training data in a separate file. 
However, when loading 20,000 samples from the database with a 'WHERE' parameter, things tend to go more slow. 
At this point, it was decided to try to dump the loaded data from the database to a file. 
This was achieved by using pandas.DataFrame.to\_csv\footnote{
	pandas.DataFrame.to\_csv: \\
	\url{http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html}.
	}. 
At a later point, the unprocessed dataset was also dumped to a CSV file for replicability.
\vspace{0.5em}\newline
Further examination showed that the vocabulary contained a lot of numerical and hexadecimal values, but also a lot of non-English words. 
The numerical and hexadecimal values were replaced using regular expressions to 'has\_hexadecimal' and 'has\_numeric'. 
The non-English words were a bit more troublesome to handle, since these were mainly used to prove a point or show an example of the issue they were having\footnote{ 
	\url{http://stackoverflow.com/questions/856307/wordwrap-a-very-long-string}.
	}. 
Attempts were made to filter them out by using corpus.words.words() and corpus.wordnet.synset() 
from \gls{nltk}\footnote{\url{http://www.nltk.org/}}, and PyEnchant\footnote{\url{http://pythonhosted.org/pyenchant/}}. 
However, WordNet does not have a complete database of all English words, and they all claimed some words were not English even though they were.
The solution turned out to be a lot simpler. 
Instead of creating filters, the CountVectorizer already had one built in. 
By adjusting the minimum document frequency (min\_df) and setting it to 0.01, words that appeared in less 1\% of all documents were ignored.
\vspace{0.5em}\newline
To be able to run the system without relying on an \gls{ide}, making it run from the Terminal using basic command setup seemed like a good idea. 
At first optparse was used, which ironically turned out to be deprecated and replaced by argparse. 
However, the problem was that you could only run one command at a time, whereas I wanted the program to be able to run until exited.
The reason for this was because it needs to load a model before it can make a prediction, in addition the user might want to predict multiple questions. 
This was therefore replaced with a basic while loop that runs until the users enters the exit command. 
The setup used for argparse was kept, so users from *nix system might be more familiar with similar commands (shown in Listing \ref{lst:system_menu}).
At the end, there were some commands that were not added to the menu, since these were mostly used for testing. 
% listing with the available menu commands
\begin{lstlisting}[caption={System menu}, 
label={lst:system_menu}, basicstyle=\small] 
Menu: 
d: Loads default model (if exists) from ./pickle_models
e: Exit the program
h: Displays this help menu
l: Load user created model. Arguments: 
	path: Path to directory with model(s) (e.g. /home/user/my_models/) 
	filename: The models filename 
	suffix: File type - Optional (default: '.pkl')
p: Predict the quality of the entered question. Arguments: 
	question: Question to predict quality of 
t: Train a new model based on an existing (or new) data set. Arguments: 
	path: Path to directory with training data (e.g. /home/user/my_data/) 
	filename: Filename for data set (model name will be the same as this) 
	db_load: Load from database (Enter 0: No, 1: Yes) 
	limit: Limit for database row retrieval (integer) - Optional unless 'db_load' is '1'
u: Create an unprocessed data set based on database content (from database set in dbconfig.py). 
Arguments: 
	filename: Filename for data set (model name will be the same as this) 
	limit: Limit for database row retrieval (integer) 
	feature_detectors: Create singular feature detectors based on data set? (Enter 0: No, 1: Yes) 
	create_model: Create classifier model(s) based on data set? 
	0: No, 1: Unprocessed model, 2: Feature detector model(s) 3: Both (1 and 2)
\end{lstlisting}

\begin{comment}
	Timeline:
		1. Created retrieval method for bad questions from db
			1.1. Using pandas to store data retrieved from db
		2. Added HTML class to remove tags
			Answer source: \url{http://stackoverflow.com/a/925630}
			Source: \url{http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python}
			Accepted answer: "answered May 29 '09 at 11:47 Eloff"
			And comment to this answer by 'pihentagyu aka James Doepp' (May 21 '15 at 17:49)
		3. Tutorial by: \url{http://radimrehurek.com/data_science_python/}
		4. Removal of codeblocks from the question
			4.1 Using BeatifulSoup to processes tags without end-tags			
			4.2 Issues with losing the tail (text after codeblock); attaching it to parent
			4.3 Issue "NoneType + str: TypeError". Caused by parent not having tail. 
				Fixed by adding a .tail and setting it to '' on the <parent>.
			4.4 HTML w/ XML causing issues; parser fails (because parser was also xml, adding xml 
				to beginning of text). Example: 
				\url{http://stackoverflow.com/questions/19535331/print-page-specific-area-or-element}
			4.5 Filtering out questions that failed by adding their index to a temp list, and then 
				removing them from dataset
		5. Issue: Scikit-learn documentation is for v17.1, whereas I was using developer version v18.0
		   (part of the issue was that it wasn't installable, so it was installed from GitHub repo instead)
		   considered switching to libsvm (which wasn't installable through pypi in the beginning)
		6. Instead of removing codeblocks, the text 'has\_codeblock' was added to indicate code was used
			6.1 Stored the processed questions to .csv
		7. Discovered data missing in some questions by looking at the CSV. 
		   Some <code> snippetes weren't removed. Content found after EOF
		   Issue 4.4 was fixed by removing all lxml conversion and using bs4 instead
		8. Questions start with <p>, this was removed by setting all text to be on one line
		9. Removal of numeric and hexadecimal values from text. Issue with trash data, e.g:
			\url{http://stackoverflow.com/questions/856307/wordwrap-a-very-long-string}
		10. Trash data: Using NLTK, NLTK.WordNet and enchant
			10.1 Tested out WordNet, could not discover english words. Does not contain all english words
			10.2 Tested out enchant, but could not in all cases either discover all english words
			10.3 Using Porter stemming to reduce features
			10.? \url{http://www.irfanelahi.com/data-science-document-classification-python/#Lexical-
			Analysis-of-the-Text-Data}
			10.? Setting min\_df=0.01 (ignore those occuring in less then 1\%) - helped a lot 
		11. Stemming of data, model creation and gridsearch with SGD			
		12. Model based on 20,000 samples (10k good, 10k bad) - took approx ~3hours for both models
			Model 1 was for all data (no test set). Model 2 was based on train\_test\_split
		13. Added tags column to the used dataset. Added the unprocessed dataset (but html is removed)
		14. Tested out different SVM algorithms (SVC, SGD and LinearSVC)
			Issue was that I managed to overwrite the .csv, so I had to do everything over again
			took approximately 24-36 hours to complete (+ 3days before hand to make everything run smoothly).
		15. Attempted to use optparse, argparse to make it executable. 
			Problem is that it exits after command is run. Need it to continue running to be able to:
				a) create new (or load existing) model
				b) use model from a) to predict quality of entered question (rinse/repeat)
		16. Used while loop instead (loop until exit entered). shortcuts are the same as with argparse, but 
			without the '-' in front (e.g. instead of -e, just press 'e' to exit)
			
		Additional Notes (which might be relevant for the next section):
		
			- issues with setting up environment, installations, etc
			- switching from "good"/"bad" to +/-1 for class label (bcz libsvm)
			- switching from -10/+50 to -5/+50 to get more separation (and more results for bad)
			
		Potentially useful links:
		
		http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html
		http://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn
		http://stackoverflow.com/questions/35382657/my-pipeline-configuration-for-text-classification-using-
		sklearn-in-python
\end{comment}

\section{Feature sets, attributes and processing}
\label{sec:feature_sets}
When retrieving the questions from the database, the vote score was set to less than -10 for bad question and greater than 50 for good questions (retrieval limit set to 10,000; 20,000 total). 
However, the vote score was set too low for the bad questions, since only 683 rows was returned. 
Therefore, the score was then set to less than -5. 
What was also found when using pandas.Categorical to get an overview (code snippet in Listing \ref{lst:pandas_categorical} and result in Table \ref{tab:pandas_categorical}), 
one can see that for the 10,000 bad questions, the average vote score was -7. This could be an indicator that when a question has a vote score below -5, they are ignored.
\begin{lstlisting}[caption={Getting Categorical data from pandas.DataFrame}, 
label={lst:pandas_categorical}, language={Python}, basicstyle=\small] 
from pandas import DataFrame, Categorical

# get statistics from pandas.DataFrame
temp_df = __so_dataframe.loc[:, ("Score", "Body", "Title", 
"AnswerCount", "length")]
temp_df.loc[:, CLASS_LABEL_KEY] = Categorical(__so_dataframe.loc[:, 
"label"])

# prints out the questions AnswerCount, Score and length
print(temp_df.groupby("label").describe())
# prints all selected columns
print(temp_df.groupby("label").describe(include='all'))
\end{lstlisting}

\clearpage
\begin{table}[tbp]
	\centering
	\begin{tabular}{| c | c | c | c |c |}
		\hline
		Class 		& Statistics	& AnswerCount		& Score 		& Question length 	\\ \hline
		~ 			& ~ 			& ~  				& ~ 			& ~					\\ \hline
		% Bad Questions
		-1 			& mean 		& 2.0483 			& -7.0275 		& 319.226 		\\ \hline
		~ 			& std 		& 1.3129 			& 2.676 		& 382.115  		\\ \hline
		~ 			& min 		& 0.0 				& -147.0 		& 13.0  		\\ \hline
		~ 			& 25\% 		& 1.0 				& -7.0 			& 153.0 		\\ \hline
		~ 			& 50\% 		& 2.0 				& -6.0 			& 239.0  		\\ \hline
		~ 			& 75\% 		& 3.0 				& -6.0 			& 379.0  		\\ \hline
		~ 			& max 		& 20.0 				& -6.0 			& 13673.0  		\\ \hline
		% space
		~ 			& ~ 		& ~  				& ~ 			& ~				\\ \hline
		% Good Questions
		1 			& mean 		& 11.9379 			& 182.5483 		& 459.329		\\ \hline
		~ 			& std 		& 13.707824			& 317.47217 	& 531.187559  		\\ \hline
		~ 			& min 		& 0.0 				& 51.0 			& 13.0  		\\ \hline
		~ 			& 25\% 		& 6.0 				& 67.0 			& 189.0 		\\ \hline
		~ 			& 50\% 		& 9.0 				& 96.0 			& 328.0  		\\ \hline
		~ 			& 75\% 		& 14.0 				& 173.0 		& 558.0  		\\ \hline
		~ 			& max 		& 518.0 			& 9432.0 		& 18867.0  		\\ \hline
		
	\end{tabular}
	\caption{Results from pandas.DataFrame and pandas.Categorical. -1 is for bad questions (votes < -5), 
		and 1 are for good questions (votes > 50).}
	\label{tab:pandas_categorical}
\end{table}
\clearpage
\begin{table}[tbp]
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline
		Step & Text processing  & Vocabulary count & CountVectorizer  \\ \hline
		1 	& None 									& 69766 	& analyzer="word" \\ \hline
		2 	& Stop words 							& 69462 	& 
			\shortstack{analyzer="word", \\ stop\_words="english"} \\ \hline
		3 	& \shortstack{Removal of code, 
			hexadecimal \\ and numerical values} 	& 27624 	& 
			\shortstack{analyzer="word", \\ stop\_words="english"} \\ \hline
		4 	& Minimum document frequency 			& 440 		& 
				\shortstack{analyzer="word", min\_df=0.01, \\ stop\_words="english"} \\ \hline
	\end{tabular}
	\caption{Feature reduction steps before and after text was processed.}
	\label{tab:feature_reduction}
\end{table}

To be able to develop some theories on what the difference between good and bad questions was, a total of 200 questions were reviewed (by sorting questions based on 
 votes\footnote{\url{http://stackoverflow.com/questions?sort=votes}}). 
It was easier to see certain patterns in down-voted questions rather than those that were up-voted. 
A repetitive pattern was that many had either no code example, or poorly written code. 
These questions could also show indications of not having tried anything, or that they were based on either homework or school assignments. 
This in turn lead to a hypothesis that if a question contains indicator of word synonyms for homework\footnote{\url{http://www.thesaurus.com/browse/homework}}, 
it would be considered a bad question. 
In addition, some code examples had syntax errors, which made the minimum working example (MWE) not executable. 
Some questions also contained links, either to external resources or indicators of potential duplicates. 
Therefore links was also considered a potentially useful feature. 
Tags was also considered as a feature, which was divided into two: Attached and External tag. 
Attached tags are tags which the user has linked to the question, whereas external tags are all the tags available on \gls{so}.
Version numbering was also considered, but this was not included due to the complexity of writing a proper filtering method to account for all possible variations.
\vspace{0.5em}\newline
Features were added in the same manner as was done for code samples, numerical and hexadecimal values. 
However, there were some issues when attempting to replace the tags and the synonyms for homework.
At first, WordNet was used for synonyms (using wordnet.synset()). 
The only problem was that for the word 'homework', wordnet.synset() only returns ['homework', 'prep', 'preparation'].
Whereas Thesaurus\footnote{\url{http://www.thesaurus.com/browse/homework}} had a lot more suggestions, and was therefore used instead.
Words were selected based on whether or not it was plausible that they could be used in programming related question setting.
A new problem now arose, namely the issue that the word "assignment" did not necessarily need to occur in a homework setting, since it could also be used as a programming word
(e.g. assignment operator\footnote{\url{http://stackoverflow.com/questions/5368258/the-copy-constructor-and-assignment-operator}}).
Therefore features for homework were split into two types: 'has\_homework' and 'has\_assignment'.
\vspace{0.5em}\newline
Tags were without a doubt one of the most annoying features to detect and replace. 
Site tags (or external) are single text values in the database, whereas the question can have up to five tags attached. 
Those attached tags are then separated in the following format: "<c><multi-threading>", which had to be processed by removing the '<' and the '>'.
After the removal, each tag value was added to a list, so that all attached tags was indexed based on the question they belonged to. 
Furthermore, a combination of string replacement and regular expression was needed. 
The regular expression was used for single character tags (e.g. 'C'), and word replacement for longer words.
The reason for this was that when using string replacement, single character tags replaced occurrences even if they appeared in the middle of a word.
If the tags contained characters that could be interpreted as a regular expression (e.g. C++), it would give error about multiple repetitions.
In addition, the tags needed to be sorted based on their length, since for questions that contained tags which included both <C> and <C++>, 
if <C> came first, it replaced the <C++> with 'has\_*\_tag'++. The text also had to be converted to lower-case to ensure proper tag matching. 

\begin{lstlisting}[caption={Replacing tags in the question}, 
label={lst:pandas_categorical}, language={Python}, basicstyle=\small] 
for word in word_set:
	if len(word) == 1:
		# if its only one character (e.g. 'C'), ensure that it is a singular word by using regex
		text = re.sub(r"\b%s\b" % word, replacement_text, text, flags=re.IGNORECASE)
	else:
		text = text.replace(word, replacement_text)
\end{lstlisting}

\section{Selecting estimator and parameters for classification}
\label{sec:estimator_parameter_selection}
Two different classifiers were used, \gls{svc} and \gls{sgd}.
The parameter values that were used for these two are shown in Listing \ref{lst:param_svc} and  \ref{lst:param_sgd}.
These are the default values used in Scikit-learns tutorials \cite{Scikitlearn.org2016k, Scikitlearn.org2016l} (\cite{Huang2008, Maas2011, Zhang2003} used default values for training their system).
\gls{svc} was chosen because this is what I started with in the beginning, based on the tutorial written by \cite{Rehurek2014}.
\gls{sgd} was used in some of Scikit-learns tutorials, and are mostly used for sample sizes greater than 100,000 (see Scikit-learns "roadmap" in Appendix \ref{app:ml_map}, Figure \ref{fig:ml_map}  
 p.~\pageref{app:ml_map}).
Since the sample size in this thesis was only 20,000, the expectation was that \gls{svc} would present better predictions.
\clearpage
% Listing for the SVC parameters
\begin{lstlisting}[caption={Parameters for SVC}, label={lst:param_svc}, language={Python}] 
param_svm = [
{'clf__C': [1, 10, 100, 1000], 'clf__kernel': ['linear']},
{'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['rbf']},
{'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['sigmoid']},
]
\end{lstlisting}
% Listing for the SGD parameters
\begin{lstlisting}[caption={Parameters for SGD}, label={lst:param_sgd}, language={Python}] 
grid_parameters = {
'vect__min_df': (0.01, 0.025, 0.05, 0.075, 0.1),
'vect__max_df': (0.25, 0.5, 0.75, 0.95, 1.0),
'tfidf__use_idf': (True, False),
'tfidf__norm': ('l1', 'l2'),
'clf__alpha': (0.00001, 0.000001),
'clf__penalty': ('l1', 'l2', 'elasticnet'),
'clf__n_iter': (10, 50, 75, 100),
# 'clf__loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'),
}
\end{lstlisting}
% editor space
Instead of simply selecting random values for the classifier, exhaustive grid search was selected. 
The downside with using grid search is that it takes a lot of time to train, since all parameters are matched against each other to find the best combination \cite{Bishop2006, Markham2015a}.
An example is presented in Equation \ref{eq:fit_sgd} and \ref{eq:fit_sgd}, showing how many fits the exhaustive search must do before it is completed.
% Equation for the amount of fits for SVC
\begin{equation}\label{eq:fit_svc}
\begin{split}
\text{count(C)} + (\text{count(C)} \cdot \text{count(gamma)}) + (\text{count(C)} \cdot \text{count(gamma)}) \\
\Longrightarrow  \text{result} \cdot \text{cross-validation} = \text{fit\_amount} \\
\Longrightarrow  4 + (4 \cdot 2) + (4 \cdot 2) = 20 \cdot 5 = 100 
\end{split}
\end{equation}
% Equation for the amount of fits for SGD
\begin{equation}\label{eq:fit_sgd}
 5 \cdot 5 \cdot 2 \cdot 2 \cdot 2 \cdot 3 \cdot 4 = 2400 \cdot 5 = 12,000
\end{equation}
% editor space
From Scikit-learn, there are two options for exhaustive search: GridSearchCV and RandomizedSearchCV.
The main difference is that GridSearchCV matches all parameters, and RandomizedSearchCV only uses a selection of the parameters. 
RandomizedSearchCV is faster than GridSearchCV, but as stated in \cite{Markham2015a}, it may give lower scores.
\textcite{Markham2015a} also suggests to start GridSearchCV and then compare the results against RandomizedSearchCV.
Since I wanted to find the best parameters from the selection, GridSearchCV was used. 
However, if the goal was to fine-tune the parameters, RandomizedSearchCV could be more fitting. 
E.g. start with a high spread of values, and after each completed training, select a new set of values based on the best results. 
If the score was then too low, or if you wanted better results, you could use GridSearchCV to find the best parameters.
\vspace{0.5em}\newline
Before running the grid search, the questions were split into two parts, a training set and a test set by using 
 train\_test\_split\footnote{\url{http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html}}.
train\_test\_split splits the data randomly, but if the random\_state has a fixed value, data will be split the same way (replicability).
For the grid search, cross validation was also included, using StratifiedKFold with 5-folds.
StratifiedKFold was selected because it is often used for classification tasks \cite{Kononenko2007}.
Cross-validation splits data into k-folds, where training is done on k-1 folds, and then evaluated against the last fold \cite{Bishop2006}.
This means that only 80\% of the 16,000 questions is used for training (12,800 questions for training, and 3,200 for evaluation).
After the training was completed, the classifier models prediction accuracy was evaluated by using the test set.
%\vspace{0.5em}\newline
