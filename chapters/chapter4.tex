%\chapter{Results}
\label{chap:chapter4}

\section{Experimental setup}
\label{sec:experimental_setup}
A total of six features were detected and converted from each question. 
These features were code samples\footnote{
	Nothing was done to the content of the code sample, it was only removed and replaced with 'has\_codeblock'.
}, 
hexadecimal, numerical, synonyms for homework and tags (homework and tags contained two feature types). 
One problem with the external tags (all tags listed on the given site), was that it replaced even normal words\footnote{
	For a visualization of how bad it was, see Figure \ref{fig:tag_features_raw} and \ref{fig:tag_features_external} in Appendix \ref{app:various_screenshots}, p.~\pageref{app:various_screenshots}.
	See also the files "\emph{training\_data\_10000\_unprocessed\_UP\_has\_tags.csv}" and "\emph{training\_data\_10000\_unprocessed\_has\_tags.csv}", found in \\ 
	\emph{./IMT4904\_MasterThesis\_Code/extraction\_sets/}.
} (e.g. "this", "can", "let", etc).
Since the external tags and the word "assignment" was conflictive, these were only included when creating classifier models for the singular feature detectors\footnote{
	The singular feature detectors are based on the premise that each feature should be tested individually, and none of the other feature detectors should be used in the model creation process.
}.
\vspace{0.5em}\newline
To be able to compare if a feature would have an impact, the creation of classifier models was divided into four parts. 
In the first part, the raw, unprocessed data set was used to create a classifier model by using GridSearchCV. 
The only modification done to the data was removing the HTML and converting the text to lower-case. 
\vspace{0.5em}\newline
In the second part, the best parameter values from the unprocessed classifier model was used to create new models for the singular feature detectors.
This ensures that a correct comparison can be made, since the results for both models are based on the same values. 
If they were all trained separately with GridSearchCV, there is a risk that it would select other parameters for the model.
In addition, a training set was also created by selecting only the questions containing the given feature.
That way, one would also have a comparison of the features accuracy for the question in which they appeared.
A new classifier model based on the unprocessed data set was created for each feature using grid search. 
This was done to account for the number of question related to each of the singular features. 
%\vspace{0.5em}\newline
\clearpage\noindent
In the third part, a comparison was made between the unprocessed classifier model and the classifier model based on all the features\footnote{
	Two features were excluded, 'has\_external' and 'has\_assignment'.
}, using GridSearchCV to select the best parameters.
Porter stemming was also included in one of the models using all the features, to see what effect stemming would have. 
\vspace{0.5em}\newline
In the last part, \gls{sgd} was used to create three classifier models.
The first two was based on the unprocessed data set, and the last used all the features.
The difference between these is that one of the unprocessed models used grid search with all losses, and the other two had loss='log'.
The reason for setting loss='log', is because this allows the model to also present not only a prediction, but also a probability to the user.
% maybe move this to Results?
The confusion matrices for each classifier model is presented in Appendix \ref{app:confusion_matrix}, \pageref{app:confusion_matrix}.

\section{Results}
\label{sec:results}


\subsection{Comparison of features based on unprocessed data set and all questions}
\label{sec:comparison_all_questions_unprocessed}
The results for the first and second part of the training is shown in Table \ref{tab:singular_feature_detector_so}.
In this table, all classifiers have been trained using the same parameter settings, based on parameters found from the grid search on the unprocessed data set.
Without doing anything to the questions, the accuracy is already at 79.9\% (403 misclassified as bad, 401 misclassified as good).
The two features that sticks out the most is Numerical and Tags. 
Numericals accuracy score is 80.55\% (375 misclassified as bad, 401 misclassified as good).
Tags accuracy score is almost 4\% lower than the unprocessed (467 misclassified as bad, 486 misclassified as good).
% table showing the results for all singular feature detectors, for all questions
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c |}
		\hline
		~ 				& Accuracy Score	\\ \hline
		Code block 		& 79.17\%			\\ \hline
		Hexadecimal		& 79.90\%			\\ \hline
		Homework 		& 79.90\%			\\ \hline
		Links			& 79.35\%			\\ \hline				
		Numerical		& 80.55\%			\\ \hline
		Tags			& 76.17\%			\\ \hline
		All features	& 79.07\%			\\ \hline
		Unprocessed		& 79.90\%			\\ \hline
	\end{tabular}
	\caption{Classifier results based on the parameters found for the raw (unprocessed) questions. Classifier=SVC, with Kernel=RBF, C=1000 and Gamma=($\gamma$) 0.0001}	
	\label{tab:singular_feature_detector_so}
\end{table}


\subsection{Comparison of features based on unprocessed data set and occurrence}
\label{sec:comparison_feature_occurence_unprocessed}
An overview of how many questions contained each feature is given in Table \ref{tab:amount_of_singular_questions_processed}.
The number used for training and evaluation is shown in Table \ref{tab:questions_used_for_singular_training}.
The results are presented in Table \ref{tab:comparison_of_feature_occurences_only}.
Numerical now has a worse accuracy than the unprocessed, but there are two things worth mentioning. 
First, the classifier for Numerical is only trained on 9,024 questions, instead of 20,000.
Second, as can be seen in Table \ref{tab:questions_used_for_singular_training}, only 30\% of the training data are good questions. 
When also looking at the confusion matrix (Appendix \ref{app:conf_matrix_singular_only}, p.~\pageref{app:conf_matrix_singular_only}), it predicts more questions as bad.
% amount of questions used for training and evaluation
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c | c |}
		\hline
		~ 					& Bad: -1			& Good: 1		& Total		\\ \hline
		Code block			& 5,090				& 4,765			& 9,855		\\ \hline
		Hexadecimal			& 109				& 51			& 160		\\ \hline
		Homework			& 261				& 113			& 374		\\ \hline
		Links				& 778				& 1,798			& 2,575		\\ \hline
		Numerical			& 5,804				& 3,220			& 9,024		\\ \hline
		Tags				& 9,987				& 9,980			& 19,967	\\ \hline
		All features		& 8,466				& 9,092			& 17,558	\\ \hline
	\end{tabular}
	\caption{The number of questions containing the given feature.}
	\label{tab:amount_of_singular_questions_processed}
\end{table}
% overview of the amount of questions used to train each feature (using only questions containing the given feature)
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c | c | c | c |}
		\hline
		~ 					& Training: -1		& Training: 1		& Evaluation: -1	& Evaluation: 1	\\ \hline
		Code block			& 4102				& 3782				& 988				& 983			\\ \hline
		Hexadecimal			& 88				& 40				& 21				& 11			\\ \hline
		Homework			& 205				& 94				& 56				& 19			\\ \hline
		Links				& 627				& 1433				& 151				& 365			\\ \hline
		Numerical			& 4650				& 2569				& 1154				& 651			\\ \hline
		Tags				& 8002				& 7971				& 1985				& 2009			\\ \hline
		All features		& 6795				& 7251				& 1671				& 1841			\\ \hline
	\end{tabular}
	\caption{The number of questions used for evaluation. Bad questions: -1, Good questions: 1.}
	\label{tab:questions_used_for_singular_training}
\end{table}
% editor space
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c | c | c | c |}
		\hline
		~				& Unprocessed			& Feature						& C				& Gamma ($\gamma$)	& Kernel	\\ \hline
		Code block	 	& Accuracy: 78.84\%		& Accuracy: 79.04\%				& 1				& N/A				& Linear 	\\ \hline
		Hexadecimal		& Accuracy: 81.25\%		& Accuracy: 81.25\%				& 1				& N/A				& Linear	\\ \hline
		Homework 		& Accuracy: 84.00\%		& Accuracy: 82.67\%				& 1				& N/A				& Linear	\\ \hline
		Links			& Accuracy: 83.72\%		& Accuracy: 81.78\%				& 1				& N/A				& Linear	\\ \hline		
		Numerical		& Accuracy: 80.22\%		& Accuracy: 79.66\%				& 1000			& 0.0001			& RBF		\\ \hline
		Tags			& Accuracy: 79.36\%		& Accuracy: 76.46\%				& 1000			& 0.0001			& RBF		\\ \hline
		All features	& Accuracy: 79.24\%		& Accuracy: 79.15\%				& 1000 			& 0.0001			& RBF 		\\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and singular features, for questions containing the given feature. Classifier: SVC.}
	\label{tab:comparison_of_feature_occurences_only}
\end{table}




% TODO: Come up with better, more descriptive section name
\subsection{Comparison of classifier for unprocessed and stemmed}
\label{sec:comparing_unprocessed_all_features}
In addition to comparing the combination of all features against the unprocessed classifier, stemming was also included. 
There were two stemmers that were considered, Porter and Lancaster.
Lancaster is based on Porter, but is more aggressive, and Lancaster replaced words which was not even of the same root\footnote{
	E.g. the words "user" and "using" both became "us" with Lancaster, and with Porter it became "user" and "use".
}.
Therefore, Porter was used as the stemmer.
\vspace{0.5em}\newline
The expectation was that stemming would enhance the prediction and increase the prediction accuracy, but this was not the case. 
The stemmed feature detector only achieved an accuracy score of 75.97\%, which is 3.93\% lower than the unprocessed, and 3.15\% lower than the non-stemmed classifier.
When comparing the stemmed feature detector against the unprocessed and the other two feature detectors including all features, it has the highest misclassification\footnote{ 
	Based on the confusion matrices displayed in Appendix \ref{app:confusion_matrix}, p.~\pageref{app:confusion_matrix}.
}.
However, the non-stemmed feature detector that was trained based on grid-search, performs better than the feature detector using the unprocessed parameters.
This is because it is better at predicting bad questions.


\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c |}
		\hline
		~ 							& Score		& Gamma ($\gamma$)		\\ \hline
		All features (stemmed)		& 75.97\%	& 1e$^{-03}$ (0.001)	\\ \hline
		All features (no stemming)	& 79.12\%	& 1e$^{-03}$ (0.001)	\\ \hline
		All features (unprocessed)	& 79.07\%	& 1e$^{-04}$ (0.0001)	\\ \hline
		Unprocessed					& 79.90\%	& 1e$^{-04}$ (0.0001)	\\ \hline
	\end{tabular}
	\caption{Comparison of the classifier for the raw (unprocessed) questions vs. questions with all features. Classifier: SVC, Kernel=RBF and C=1000.}
	\label{tab:unprocessed_vs_all_feature_detectors_svc_so}
\end{table}











% TODO: Come up with better, more descriptive section name
\subsection[Comparison of SVC and SGD]{Comparison of \glsentrylong{svc} (\glsentryshort{svc}) and \glsentrylong{sgd} (\glsentryshort{sgd})}
\label{sec:comparing_svc_sgd}
Table \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so} shows a comparison between unprocessed and all features (stemmed) using the \gls{sgd} classifier.
A full exhaustive search for all losses was also ran on the unprocessed, but this is omitted because it resulted in the same values as for loss='log'.
\vspace{0.5em}\newline
\gls{svc} did perform slightly better than the \gls{sgd}, which can be seen from comparing Table \ref{tab:unprocessed_vs_all_feature_detectors_svc_so} and \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so}.
The classifier for the unprocessed \gls{svc} achieved a score of 79.90\% vs. \gls{sgd} which achieved 79.87\% (only 0.3\% difference).
When comparing the feature detector containing all features (where both were stemmed), \gls{svc} achieved a score 75.97\% and \gls{sgd} achieved 75.55\% (0.42\% difference). 
This could indicate that if one were to increase the sample size to 30,000 or higher, \gls{sgd} could be a better classifier than \gls{svc}.
%\vspace{0.5em}\newline	
\clearpage\noindent
Based on the confusion matrices, if one only were interested in predicting bad questions, \gls{sgd} would be a better option, given that it has the lowest misclassification.
However, as seen in Table \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so}, it uses a max document frequency of 0.5, meaning it excludes words appearing in over 50\% of the questions.

% table showing comparison of the unprocssed and all features classifier using loss='log'
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c |}
		\hline
		~				& Unprocessed (loss='log')	& All features (loss='log')	\\ \hline
		Score 			& 79.87\%					& 75.55\%					\\ \hline
		Min DF 			& 0.01						& 0.01						\\ \hline
		Max DF 			& 0.5						& 0.75						\\ \hline
		Use IDF			& False						& True						\\ \hline
		Alpha 			& 1e$^{-05}$ (0.00001)		& 1e$^{-05}$ (0.00001)		\\ \hline
		Normalization 	& l2						& l2						\\ \hline		
		Penalty 		& elasticnet				& l2						\\ \hline
		Iteration 		& 50						& 100						\\ \hline
		Loss 			& log						& log						\\ \hline		
	\end{tabular}
	\caption{Comparison of the classifier for the raw (unprocessed) questions vs. questions with all features. Classifier: SGD.}
	\label{tab:unprocessed_vs_all_feature_detectors_sgd_so}
\end{table}




\subsection{Applicability}
\label{sec:applicability}
Since this thesis was mainly focused on programming questions and \gls{so}, one could ask the question whether or not this is a closed domain system. 
The answer is that it is an open domain system for \gls{se}.
\gls{se} uses the same database format for all their data, which is listed in the ReadMe.txt included in the data set\footnote{
	It is also included in the source code repository on GitHub.
	The database tables were also based on the ReadMe.
}.
\vspace{0.5em}\newline
To see if the system was expandable, the data for Tex.StackExchange\footnote{
	The Tex.StackExchange data is based on a data set that was downloaded in August 2015, but the newest question in the data set is from 2014.
} was extracted and added to a database. 
Given the amount of data available in \gls{so} vs. the amount of data in Tex.StackExchange, which would be more predictive\footnote{
	One critique is that the number of questions is not comparable, as shown in Table \ref{tab:dataset_overview_tex}.
}?
However, this also helps prove a point in relation to whether or not data matters when training a classifier.
The results show that \gls{so} is more predictive than Tex.StackExchange.
Tex.StackExchange got an accuracy score of 99\%, but it misclassified all 15 bad questions as good (and in total, it only contains 93 bad questions).

%description
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c | c | c |}
		\hline
		~				& Amount		& Oldest		& Newest		& Vote (lowest)		& Vote (highest)	\\ \hline
		Votes < 0		& 93			& 22.08.2010	& 10.09.2014	& -14				& -1				\\ \hline
		Votes = 0		& 5,078			& 26.07.2010	& 14.09.2014	& 0					& 0					\\ \hline
		Votes > 0		& 65,919		& 18.08.2008	& 14.09.2014	& 1					& 448				\\ \hline
		All questions	& 71,090 		& 18.08.2008	& 14.09.2014	& -14				& 448				\\ \hline
	\end{tabular}
	\caption{Overview of the questions in the Tex.StackExhange (August 2015 data set)}
	\label{tab:dataset_overview_tex}
\end{table}

% editor space
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c |}
		\hline
		~ 			& Unprocessed		& Features	\\ \hline
		Score 		& 0.99				& 0.99		\\ \hline
		C			& 1					& 1			\\ \hline
		Kernel		& Linear			& Linear	\\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and feature detectors for Tex.StackExhange (August 2015 data set). Vote score was < 0 for bad and > +7 for good.}
	\label{tab:singular_feature_detector_tex}
\end{table}

% description
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c |}
		\hline
		Actual 		& Predicted: -1	& Predicted: 1	\\ \hline
		-1			& 0			& 15				\\ \hline
		1			& 0			& 2004				\\ \hline
	\end{tabular}
	\caption{Confusion Matrix for Tex.StackExhange.}
	\label{tab:confusion_matrix_tex}
\end{table}

% description
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c | c |}
		\hline
		~				& Precision		& Recall	& F1-score		& Support	\\ \hline
		-1      		& 0.00			& 0.00		& 0.00			& 15		\\ \hline
		1       		& 0.99			& 1.00		& 1.00			& 2004		\\ \hline
		avg / total		& 0.99			& 0.99		& 0.99			& 2019		\\ \hline
	\end{tabular}
	\caption{Classification report for Tex.StackExhange (August 2015 data set).}
	\label{tab:tex_classification_report}
\end{table}
