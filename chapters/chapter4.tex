%\chapter{Results}
\label{chap:chapter4}

\section{Experimental setup}
\label{sec:experimental_setup}
A total of six features were detected and converted from each question. 
These features were code samples\footnote{
	Nothing was done to the content of the code sample, it was only removed and replaced with 'has\_codeblock'.
}, 
hexadecimal, numerical, synonyms for homework and tags (homework and tags contained two feature types). 
One problem with the external tags (all tags listed on the given site), was that it replaced even normal words\footnote{
	For a visualization of how bad it was, see Figure \ref{fig:tag_features_raw} and \ref{fig:tag_features_external} in Appendix \ref{app:various_screenshots}, p.~\pageref{app:various_screenshots}.
	See also the files "\emph{training\_data\_10000\_unprocessed\_UP\_has\_tags.csv}" and "\emph{training\_data\_10000\_unprocessed\_has\_tags.csv}", found in \\ 
	\emph{./IMT4904\_MasterThesis\_Code/extraction\_sets/}.
} (e.g. "this", "can", "let", etc).
Since the external tags and the word "assignment" was conflictive, these were only included when creating classifier models for the singular feature detectors\footnote{
	The singular feature detectors are based on the premise that each feature should be tested individually, and none of the other feature detectors should be used in the model creation process.
}.
\vspace{0.5em}\newline
To be able to compare if a feature would have an impact, the creation of classifier models was divided into four parts. 
In the first part, the raw, unprocessed data set was used to create a classifier model by using GridSearchCV. 
The only modification done to the data was removing the HTML and converting the text to lower-case. 
\vspace{0.5em}\newline
In the second part, the best parameter values from the unprocessed classifier model was used to create new models for the singular feature detectors.
This ensures that a correct comparison can be made, since the results for both models are based on the same values. 
If they were all trained separately with GridSearchCV, there is a risk that it would select other parameters for the model.
In addition, a training set was also created by selecting only the questions containing the given feature.
That way, one would also have a comparison of the features accuracy for the question in which they appeared.
A new classifier model based on the unprocessed data set was created for each feature using grid search. 
This was done to account for the number of question related to each of the singular features. 
%\vspace{0.5em}\newline
\clearpage\noindent
In the third part, a comparison was made between the unprocessed classifier model and the classifier model based on all the features\footnote{
	Two features were excluded, 'has\_external' and 'has\_assignment'.
}, using GridSearchCV to select the best parameters.
Porter stemming was also included in one of the models using all the features, to see what effect stemming would have. 
\vspace{0.5em}\newline
In the last part, \gls{sgd} was used to create three classifier models.
The first two was based on the unprocessed data set, and the last used all the features.
The difference between these is that one of the unprocessed models used grid search with all losses, and the other two had loss='log'.
The reason for setting loss='log', is because this allows the model to also present a probability in addition to the prediction.
% maybe move this to Results?
The confusion matrices for each classifier model is presented in Appendix \ref{app:confusion_matrix}, \pageref{app:confusion_matrix}.


% TODO: add comment/section about Tex.StackExchange here

\section{Results}
\label{sec:results}




% TODO: Come up with better, more descriptive section name
\subsection{Using all questions}
\label{sec:comparison_all_questions_unprocessed}
The results for the first and second part of the training is shown in Table \ref{tab:singular_feature_detector_so}.
In this table, all classifiers have been trained using the same parameter settings, based on parameters found from the grid search on the unprocessed data set.
Without doing anything to the questions, the accuracy is already at 79.9\% (403 misclassified as bad, 401 misclassified as good).
The two features that sticks out the most is Numerical and Tags. 
Numericals accuracy score is 80.55\% (375 misclassified as bad, 401 misclassified as good).
Tags accuracy score is almost 4\% lower than the unprocessed (467 misclassified as bad, 486 misclassified as good).
% table showing the results for all singular feature detectors, for all questions
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c |}
		\hline
		~ 				& Accuracy Score	\\ \hline
		Code block 		& 79.17\%			\\ \hline
		Hexadecimal		& 79.90\%			\\ \hline
		Homework 		& 79.90\%			\\ \hline
		Links			& 79.35\%			\\ \hline				
		Numerical		& 80.55\%			\\ \hline
		Tags			& 76.17\%			\\ \hline
		All features	& 79.07\%			\\ \hline
		Unprocessed		& 79.90\%			\\ \hline
	\end{tabular}
	\caption{Classifier results based on the parameters found for the raw (unprocessed) questions. Classifier=SVC, with Kernel=RBF, C=1000 and Gamma=($\gamma$) 0.0001}	
	\label{tab:singular_feature_detector_so}
\end{table}




% TODO: Come up with better, more descriptive section name
\subsection{Using questions with occurrence only}
\label{sec:comparison_feature_occurence_unprocessed}
An overview of how many questions contained each feature is given in Table \ref{tab:amount_of_singular_questions_processed}.
The number used for training and evaluation is shown in Table \ref{tab:questions_used_for_singular_training}.
The results are presented in Table \ref{tab:comparison_of_feature_occurences_only}.
Numerical now has a worse accuracy than the unprocessed, but there are two things worth mentioning. 
First, the classifier for Numerical is only trained on 9,024 questions, instead of 20,000.
Second, as can be seen in Table \ref{tab:questions_used_for_singular_training}, only 30\% of the training data are good questions. 
When also looking at the confusion matrix (Appendix \ref{app:conf_matrix_singular_only}, p.~\pageref{app:conf_matrix_singular_only}), it predicts more questions as bad.
% amount of questions used for training and evaluation
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c | c |}
		\hline
		~ 					& Bad: -1			& Good: 1		& Total		\\ \hline
		Code block			& 5,090				& 4,765			& 9,855		\\ \hline
		Hexadecimal			& 109				& 51			& 160		\\ \hline
		Homework			& 261				& 113			& 374		\\ \hline
		Links				& 778				& 1,798			& 2,575		\\ \hline
		Numerical			& 5,804				& 3,220			& 9,024		\\ \hline
		Tags				& 9,987				& 9,980			& 19,967	\\ \hline
		All features		& 8,466				& 9,092			& 17,558	\\ \hline
	\end{tabular}
	\caption{The number of questions containing the given feature.}
	\label{tab:amount_of_singular_questions_processed}
\end{table}
% overview of the amount of questions used to train each feature (using only questions containing the given feature)
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c | c | c | c |}
		\hline
		~ 					& Training: -1		& Training: 1		& Evaluation: -1	& Evaluation: 1	\\ \hline
		Code block			& 4102				& 3782				& 988				& 983			\\ \hline
		Hexadecimal			& 88				& 40				& 21				& 11			\\ \hline
		Homework			& 205				& 94				& 56				& 19			\\ \hline
		Links				& 627				& 1433				& 151				& 365			\\ \hline
		Numerical			& 4650				& 2569				& 1154				& 651			\\ \hline
		Tags				& 8002				& 7971				& 1985				& 2009			\\ \hline
		All features		& 6795				& 7251				& 1671				& 1841			\\ \hline
	\end{tabular}
	\caption{The number of questions used for evaluation. Bad questions: -1, Good questions: 1.}
	\label{tab:questions_used_for_singular_training}
\end{table}
% editor space
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c | c | c | c |}
		\hline
		~				& Unprocessed			& Feature						& C				& Gamma ($\gamma$)	& Kernel	\\ \hline
		Code block	 	& Accuracy: 78.84\%		& Accuracy: 79.04\%				& 1				& N/A				& Linear 	\\ \hline
		Hexadecimal		& Accuracy: 81.25\%		& Accuracy: 81.25\%				& 1				& N/A				& Linear	\\ \hline
		Homework 		& Accuracy: 84.00\%		& Accuracy: 82.67\%				& 1				& N/A				& Linear	\\ \hline
		Links			& Accuracy: 83.72\%		& Accuracy: 81.78\%				& 1				& N/A				& Linear	\\ \hline		
		Numerical		& Accuracy: 80.22\%		& Accuracy: 79.66\%				& 1000			& 0.0001			& RBF		\\ \hline
		Tags			& Accuracy: 79.36\%		& Accuracy: 76.46\%				& 1000			& 0.0001			& RBF		\\ \hline
		All features	& Accuracy: 79.24\%		& Accuracy: 79.15\%				& 1000 			& 0.0001			& RBF 		\\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and singular features, for questions containing the given feature. Classifier: SVC.}
	\label{tab:comparison_of_feature_occurences_only}
\end{table}




% TODO: Come up with better, more descriptive section name
\subsection{Comparing unprocessed against all features}
\label{sec:comparing_unprocessed_all_features}
In addition to comparing the combination of all features against the unprocessed classifier, stemming was also included. 
There were two stemmers that were considered, Porter and Lancaster.
Lancaster is based on Porter, but is more aggressive, and Lancaster replaced words which was not even of the same root\footnote{
	E.g. the words "user" and "using" both became "us" with Lancaster, and with Porter it became "user" and "use".
}.
Therefore, Porter was used as the stemmer.
\vspace{0.5em}\newline
The expectation was that stemming would enhance the prediction and increase the prediction accuracy, but this was not the case. 
The stemmed feature detector only achieved an accuracy score of 75.97\%, which is 3.93\% lower than the unprocessed, and 3.15\% lower than the non-stemmed classifier.
When comparing the stemmed feature detector against the unprocessed and the other two feature detectors including all features, it has the highest misclassification\footnote{ 
	Based on the confusion matrices displayed in Appendix \ref{app:confusion_matrix}, p.~\pageref{app:confusion_matrix}.
}.
However, the non-stemmed feature detector that was trained based on grid-search, performs better than the feature detector using the unprocessed parameters.
This is because it is better at predicting bad questions.


\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| l | c | c |}
		\hline
		~ 							& Score		& Gamma ($\gamma$)		\\ \hline
		All features (stemmed)		& 75.97\%	& 1e$^{-03}$ (0.001)	\\ \hline
		All features (no stemming)	& 79.12\%	& 1e$^{-03}$ (0.001)	\\ \hline
		All features (unprocessed)	& 79.07\%	& 1e$^{-04}$ (0.0001)	\\ \hline
		Unprocessed					& 79.90\%	& 1e$^{-04}$ (0.0001)	\\ \hline
	\end{tabular}
	\caption{Comparison of the classifier for the raw (unprocessed) questions vs. questions with all features. Classifier: SVC, Kernel=RBF and C=1000.}
	\label{tab:unprocessed_vs_all_feature_detectors_svc_so}
\end{table}











% TODO: Come up with better, more descriptive section name
\subsection{Comparing SVC and SGD}
\label{sec:comparing_svc_sgd}



Table \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so} shows a comparison between unprocessed and all features (stemmed) using the \gls{sgd} classifier.
A full exhaustive search was also ran on the unprocessed, but this is omitted because it resulted in the same values as for loss='log'.
The reason loss was set to 'log' is because you can then get a probability score when using prediction.
Probability was desired, because when a user then enters a new question to predict, the output would also contain the probability and not just "This is a good/bad question".
\vspace{0.5em}\newline
\gls{svc} did perform slightly better than the \gls{sgd}, which can be seen from comparing Table \ref{tab:unprocessed_vs_all_feature_detectors_svc_so} and \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so}.
The classifier for the unprocessed \gls{svc} achieved a score of 79.90\% vs. \gls{sgd} which achieved 79.87\% (only 0.3\% difference).
When comparing the feature detector containing all features (where both were stemmed), \gls{svc} achieved a score 75.97\% and \gls{sgd} achieved 75.55\% (0.42\% difference). 
This could indicate that if one were to increase the sample size to 30,000 or higher, \gls{sgd} could be a better classifier than \gls{svc}.
Based on the confusion matrices, if one only were interested in predicting bad questions, \gls{sgd} would be a better option, given that it has the lowest misclassification.
However, as seen in Table \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so}, it uses a max document frequency of 50\%, meaning that words appearing in over 50\% of the questions are excluded.
This could be related to it filtering out code samples and dummy words, since the classifier using all features uses up to 75\%.
Another difference is that for the \gls{svc}, a constant value was set for max document frequency to exclude words used in more than 95\% of the questions. 
This was mainly because it would be more interesting to use the less used words, but these results shows that it would have been better for the grid search to select that value.
% table showing comparison of the unprocssed and all features classifier using loss='log'
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c |}
		\hline
		~				& Unprocessed (loss='log')	& All features (loss='log')	\\ \hline
		Score 			& 79.87\%					& 75.55\%					\\ \hline
		Min DF 			& 0.01						& 0.01						\\ \hline
		Max DF 			& 0.5						& 0.75						\\ \hline
		Use IDF			& False						& True						\\ \hline
		Alpha 			& 1e$^{-05}$ (0.00001)		& 1e$^{-05}$ (0.00001)		\\ \hline
		Normalization 	& l2						& l2						\\ \hline		
		Penalty 		& elasticnet				& l2						\\ \hline
		Iteration 		& 50						& 100						\\ \hline
		Loss 			& log						& log						\\ \hline		
	\end{tabular}
	\caption{Comparison of the classifier for the raw (unprocessed) questions vs. questions with all features. Classifier: SGD.}
	\label{tab:unprocessed_vs_all_feature_detectors_sgd_so}
\end{table}





