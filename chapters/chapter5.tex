%\chapter{Discussions}
\label{chap:chapter5}

\todo[inline]{Large parts of this chapter will be re-written for increased consistency and make it less repetitive. }
\todo[inline]{add this in relation to data set section: \cite{Klein2016,SpaceMachine.net2016,Wissner-Gross2016}}
\section{Data set and Question selection}
\label{sec:data_and_testing}
The dataset was retrieved from Stack Exchange Archive \cite{StackExchange2016}, and contains all the data posted from the beginning. 
In this thesis, only the data for \gls{so} was used, specifically only the posted questions.
A simplified overview is shown in Table \ref{tab:dataset_overview_so}, which lists questions based on score. 

\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c | c | c |}
		\hline
		~				& Amount		& Oldest		& Newest		& Vote (lowest)		& Vote (highest)	\\ \hline
		Votes < 0		& 659,955		& 06.08.2008	& 06.03.2016	& -147				& -1				\\ \hline
		Votes = 0		& 5,256,105		& 06.08.2008	& 06.03.2016	& 0					& 0					\\ \hline
		Votes > 0		& 5,286,971		& 31.07.2008	& 06.03.2016	& 1					& 13845				\\ \hline
		All questions	& 11,203,031	& 31.07.2008	& 06.03.2016	& -147				& 13845				\\ \hline
	\end{tabular}
	\caption{Overview of the questions in the Stack Overflow dataset.}
	\label{tab:dataset_overview_so}
\end{table}
As previously mentioned in Section \ref{sec:feature_sets}, the value used to extract the questions was set to -5 and +50. 
The problem with these values are that they were simply selected. 
Originally, the value for bad questions was -10, which were then changed to -5 because -10 only retrieved 683 rows (and I wanted 10,000 samples for both question types).
However, as can be seen from Table \ref{tab:dataset_overview_so}, +50 is most likely a to low value for the good questions. 
To get better results based on desired quality, there are three options to consider which could improve this.
\vspace{0.5em}\newline
The first is the easiest. If you are only interested in a set sample size, then you could simply retrieve the amount sorted by score.
If you then want 10,000 of each, you would then get the 10,000 that are scored highest (or lowest).
The second option would be to get a set limit based on the actual score, by using the mean or average.
For all the questions of a given type, retrieve the average and then select questions which has a score higher (or lower) than the average.
The third option would be to use quartiles. 
Quartiles is measured by a given percentage of the observations, where each quartile represents 25\%: Q$_{1}$ (25\%), M (median, 50\%) and Q$_{3}$ (75\%) \cite{Hagen2011}.
The equation to calculate the quartiles are shown in Equation \ref{eq:quartile1} - \ref{eq:quartile3}.
% equation: Quartile 1
\begin{equation}\label{eq:quartile1}
Q_{1} = \frac{(n + 1)}{4}
\end{equation}
% equation: Quartile 2
\begin{equation}\label{eq:quartile2}
M = \frac{(n + 1)}{2}
\end{equation}
% equation: Quartile 3
\begin{equation}\label{eq:quartile3}
Q_{3} = \frac{3 \cdot (n + 1)}{4}
\end{equation}
As previously mentioned in Section \ref{sec:feature_sets}, there was no clear indicator for the good questions. 
Some consisted only of two sentences, e.g. "I committed the wrong files to Git. How can I undo this commit?"\footnote{
	This question currently have a score of 10,406: \\ 
	\url{http://stackoverflow.com/questions/927358/how-do-you-undo-the-last-commit}.
}. Common for most of the top-voted are that they are mostly short, however you also have those that are long\footnote{
	One example is this question, with a score of 3,009: \\
	\url{http://stackoverflow.com/questions/826782/css-rule-to-disable-text-selection-highlighting}.
}.
There is also what I would call a bias factor. 
If enough people have the same problem, then it will automatically become a good question. 
Not because of the questions quality, but simply because many will encounter it (e.g. Bugs, \gls{ide} behaviour, tweaks, etc).
The same can be said for bad questions. 
In many cases, a question is bad not because of the question that is asked, but simply because it does not follow the guidelines. 
If a question is a duplicate, it is automatically voted for closure, and will in most cases receive multiple down-votes. 
Questions which gives a hint of being school related will also receive down-votes, mainly because \gls{so} targets professionals and experts. 
\gls{so} is neither a fan of unnecessary text like greetings and gratefulness \cite{CommunityWiki2016a,Heyer2012}.
\vspace{0.5em}\newline
Is \gls{so} fit for measuring question quality?
In a closed domain setting, the answer is yes. 
However, as mentioned, what type of quality being measured must be taken into consideration. 
As it is, the system would not be useful for an educational setting, because it bases its prediction on questions asked on \gls{so}.
If a student were to ask the system a question, it might respond saying that this is a bad question (but the result may be based on the fact that it is a duplicate).
For an educational setting, a better solution would be to develop a system similar to the one in \cite{Lezina2013}.
With the current state of the system (if it were to be used), it would be more appropriate to use it as a measurement tool for new \gls{so} questions, rather then general question quality.

\section{Development}
\label{sec:development}
There was a lot of back and forth during the development, with a lot of time being invested in properly understanding how to use Scikit-learn.
The most useful resource were of course Scikit-learns tutorials \cite{Scikitlearn.org2016j}\footnote{
	The examples that were worked through can be seen here (as stated in ReadMe, the only thing I have altered is comments and adjustments for v0.18.dev0): \\
	\url{https://github.com/klAndersen/scikit-learn_tutorials}.
	} and documentation \cite{Scikitlearn.org2016,Scikitlearn.org2016h}, 
but there were also two other tutorials that proved helpful (\cite{Rehurek2014} and \cite{Elahi2016}).
The main problem was that Scikit-learns documentation and most of the tutorials written by others is based on v0.17.1.
Due to various problems with installation in regards to Numpy and Scipy, Scikit-learn had to be installed from GitHub (which is v0.18.dev0).
In the development version, a lot of functions and modules had been re-written and moved, making many tutorials and examples outdated.
One example is the code snippet in \cite["In $\lbrack$42$\rbrack$"]{Rehurek2014}, where '\emph{cv=StratifiedKFold(label\_train, n\_folds=5)}' fails, 
because it expects the first argument to be n\_folds\footnote{
	Documentation for v0.17.1: \\
	\url{http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html}. \\
	Documentation for v0.18.dev0: \\
	\url{http://scikit-learn.org/dev/modules/generated/sklearn.model_selection.StratifiedKFold.html}.
}.
\vspace{0.5em}\newline
\todo[inline]{either write more here, or remove this paragraph}
\textcite{Rehurek2014} used in his tutorial \gls{svc} to train the \gls{svm}, but gave no explanation to why these parameter values\footnote{
	I later found that these were just the basic numbers used in one of their examples for the grid search: 
	\url{http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html}.
},
were selected, nor how he selected the best parameter from the printout\footnote{
	The score, parameters and estimator can be found by using these attributes: '\emph{best\_score\_}', '\emph{best\_params\_}' and '\emph{best\_estimator\_}'.
} (mine is shown in Listing \ref{lst:svc_tutorial}).

\begin{lstlisting}[caption={Output from SVC tutorial (testing with linear, rbf and sigmoid)}, label={lst:svc_tutorial}] 
[mean: 0.80232, std: 0.01056, params: {'classifier__kernel': 'linear', 'classifier__C': 1}, 
mean: 0.78451, std: 0.01750, params: {'classifier__kernel': 'linear', 'classifier__C': 10}, 
...(15 lines removed)...
mean: 0.71416, std: 0.00101, params: {'classifier__gamma': 0.0001, 'classifier__kernel': 
'sigmoid', 'classifier__C': 1000}]
\end{lstlisting}

\begin{comment}
Should this be used, is this relevant? Seems a bit like whining over everything that failed, etc.

Another issue was that since few tutorials had the data sets available, it was hard to compare my results against theirs to (considering version changes).
In Scikit-learns tutorial for text data \cite{Scikitlearn.org2016h}, they have at the end task suggestions, with attached solutions in the Scikit-learn library.
When looking at the solution for the second task (sentiment analysis), I saw that they used \gls{sgd} (see \cite{Scikitlearn.org2016f}).
Looking at the suggested "road map" (see Appendix \ref{app:ml_map}, Figure \ref{fig:ml_map} on p.~\pageref{app:ml_map}), 
I accidentally misread a zero and decided to go with \gls{sgd} instead of \gls{svc}.
\vspace{0.5em}\newline
When understanding more about how GridSearchCV worked and how one could select the best parameters from it, things got back on track. 
However, when comparing it against the \gls{svc}, it turned out that the \gls{svc} got a higher score\footnote{
A comparison was done on \gls{sgd}, \gls{svc} and LinearSVC using GridSearchCV.
}. 
\end{comment}

% something about parameter selection
% something about the results
% something about using probability prediction (e.g. taking twice the time, worse results for SGD, etc.)

\section{Parameter selection for GridSearchCV and the results}
\label{sec:grid_search_and_results}
Two different classifiers were used, \gls{svc} (parameters shown in Listing \ref{lst:param_svc}) and \gls{sgd} (parameters shown in Listing \ref{lst:param_sgd}).
The parameter values used for training both the \gls{svc} and \gls{sgd}, are the default values shown in Scikit-learns tutorials \cite{Scikitlearn.org2016k, Scikitlearn.org2016l} 
(\cite{Huang2008, Maas2011, Zhang2003} used default values for training their system). 
\gls{sgd} was included because of Scikit-learns "roadmap" (shown in the Appendix \ref{app:ml_map}, Figure \ref{fig:ml_map} on p.~\pageref{app:ml_map}).
Although \gls{sgd} are targeted at sample sizes greater than 100,000, it would still be interesting to compare it against \gls{svc} to see which one got the best results. 
Given the that the sample size was below the threshold, the expectation was that in this case, \gls{svc} would get better results.

% Listing for the SVC parameters
\begin{lstlisting}[caption={Parameters for SVC}, label={lst:param_svc}, language={Python}] 
param_svm = [
{'clf__C': [1, 10, 100, 1000], 'clf__kernel': ['linear']},
{'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['rbf']},
{'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['sigmoid']},
]
\end{lstlisting}
% Listing for the SGD parameters
\begin{lstlisting}[caption={Parameters for SGD}, label={lst:param_sgd}, language={Python}] 
grid_parameters = {
'vect__min_df': (0.01, 0.025, 0.05, 0.075, 0.1),
'vect__max_df': (0.25, 0.5, 0.75, 0.95, 1.0),
'tfidf__use_idf': (True, False),
'tfidf__norm': ('l1', 'l2'),
'clf__alpha': (0.00001, 0.000001),
'clf__penalty': ('l1', 'l2', 'elasticnet'),
'clf__n_iter': (10, 50, 75, 100),
# 'clf__loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'),
}
\end{lstlisting}
% editor space
Only 20,000 questions were selected, and a larger sample size would probably have helped getting better predictions from the classifier.
However, one need to take in consideration the time available, and the time it takes to process and train all the classifiers. 
When using exhaustive search it matches all the parameters against each other, to see which one has the best combination. 
This means that if you have 30 different parameters, and a cross-validation of 10, then the total amount of fits is 300 \cite{Markham2015a, Bishop2006}.
The fit count is shown in Equation \ref{eq:fit_svc} for \gls{svc} and in Equation \ref{eq:fit_sgd} for \gls{sgd} (in this thesis, cross-validation was set to 5).
% Equation for the amount of fits for SVC
\begin{equation}\label{eq:fit_svc}
\begin{split}
\text{count(C)} + (\text{count(C)} \cdot \text{count(gamma)}) + (\text{count(C)} \cdot \text{count(gamma)}) \\
\Longrightarrow  \text{result} \cdot \text{cross-validation} = \text{fit\_amount} \\
\Longrightarrow  4 + (4 \cdot 2) + (4 \cdot 2) = 20 \cdot 5 = 100 
\end{split}
\end{equation}
% Equation for the amount of fits for SGD
\begin{equation}\label{eq:fit_sgd}
\begin{split}
\text{count(min\_df)} \cdot \text{count(max\_df)} \cdot \text{count(use\_idf)} \cdot \text{count(norm)} \cdot \text{count(alpha)} \\ 
\cdot ~ \text{count(penalty)} \cdot \text{count(n\_iter)} \cdot \text{count(loss)} \Longrightarrow  result \cdot \text{cross-validation} = \text{fit\_amount} \\
\Longrightarrow  5 \cdot 5 \cdot 2 \cdot 2 \cdot 2 \cdot 3 \cdot 4 = 2400 \cdot 5 = 12,000
\end{split}
\end{equation}
% editor space
%These results must then be multiplied with two, because if you want a probability for the prediction it needs to run twice to calculate this.
A faster alternative to GridSearchCV would be RandomizedSearchCV, but as stated in \cite{Markham2015a}, it may give lower scores. 
His suggestion is to start with GridSearchCV, and then compare the results against RandomizedSearchCV.
This was not done, as this type of comparison was not relevant for this thesis.
If one were to fine-tune the parameters to get as close to optimal as possible, RandomizedSearchCV would be an excellent choice. 
You could start with a wide spread of values, and when there is little change in the resulting score, GridSearchCV could be used for fine-tuning.
\vspace{0.5em}\newline
A total of six features were detected and converted from each question. 
These features were code samples\footnote{
	Nothing was done to the content of the code sample, it was only removed and replaced with 'has\_codeblock'.
}, 
hexadecimal, numerical, synonyms for homework and tags. 
As mentioned in Section \ref{sec:feature_sets}, homework and tags contained two feature types. 
One problem with the external tags (all tags listed on the given site), was that it replaced even normal words\footnote{
	For a visualization of how bad it was, see Figure \ref{fig:tag_features_raw} and \ref{fig:tag_features_external} in Appendix \ref{app:various_screenshots}, p.~\pageref{app:various_screenshots}.
	See also the files "\emph{training\_data\_10000\_unprocessed\_UP\_has\_tags.csv}" and "\emph{training\_data\_10000\_unprocessed\_has\_tags.csv}", found in \emph{./extraction\_sets/}.
} (e.g. "this", "can", "let", etc).
Since the external tags and the word 'assignment' was conflictive, these were only included when creating classifier models for the singular feature detectors\footnote{
	The singular feature detectors are based on the premise that each feature should be tested individually, and none of the other feature detectors should be used in the model creation process.
}.
\vspace{0.5em}\newline
To be able to compare if a feature would have an impact, the creation of classifier models was divided into three parts. 
In the first part, the raw, unprocessed data set was used to create a classifier model by using GridSearchCV. 
The only modification done was the removal of HTML-tags and reducing the text to lower-case. 
In the second part, the best parameter values from the unprocessed classifier model was used to create new models for the singular feature detectors.
This ensures that a correct comparison can be made, since the results for both models are based on the same values. 
If they were all trained separately with GridSearchCV, there is a risk that it would select other parameters for the model.
The last part consisted of using all the features, both with the values from the unprocessed classifier model, and using GridSearchCV to select the best parameters.
Porter stemming was also included, but this was only used for the last classifier model.
\vspace{0.5em}\newline
Before running the grid search, data was split by using Scikit-learns train\_test\_split.
20\% of the questions (4,000) was randomly assigned to a test set, which was not used during training. 
Cross-validation was done by using StratifiedKFold with 5-folds.
StratifiedKFold was selected because it is often used for classification tasks \cite{Kononenko2007}.
Cross-validation splits data into k-folds, where training is done on k-1 folds, and then evaluated against the last fold \cite{Bishop2006}.
This means that only 80\% of the 16,000 questions is used for training (12,800 questions for training, and 3,200 for evaluation).
After the training was completed, the classifier models prediction accuracy was tested by using the test data from train\_test\_split.
\vspace{0.5em}\newline
The results for the first and second part of the training is shown in Table \ref{tab:singular_feature_detector_so}.
In this table, all classifiers have been trained using the same parameter settings, which were found using GridSearchCV on the unprocessed data set.
Without doing anything to the data in the data set, the accuracy is already at 79.9\%. 
The only feature which increases the accuracy, is the numerical, whereas the Tag features reduce accuracy with over 3\%.
Although, as previously mentioned, this score is not that useful, since the external tags are not included in the classifier for all features. 
A better solution would have been to have added an additional classification  for the attached tags only. 
It does however show that using the external tags without any filtering is not a good idea.
The accuracy for Homework is equal to the unprocessed, but as with Tags, this is also a questionable accuracy (since 'assignment'is excluded from the classifier using all features). 
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c |}
		\hline
		~ 				& Accuracy Score	\\ \hline
		Code block 	& 79.17\%			\\ \hline
		Hexadecimal		& 79.90\%			\\ \hline
		Homework 		& 79.90\%			\\ \hline
		Links			& 79.35\%			\\ \hline				
		Numerical		& 80.55\%			\\ \hline
		Tags			& 76.17\%			\\ \hline
		All features	& 79.07\%			\\ \hline
		Unprocessed		& 79.90\%			\\ \hline
	\end{tabular}
	\caption{Classifier results based on the parameters found for the raw (unprocessed) questions. Classifier=SVC, with Kernel=RBF, C=1000 and Gamma=($\gamma$) 0.0001}	
	\label{tab:singular_feature_detector_so}
\end{table}
\vspace{0.5em}\newline
It would also be interesting to compare the unprocessed against each feature detector, for the question in which it appeared.
Therefore, new classifier models were made, following the same strategy as mentioned earlier, but now the classifier was re-trained for each feature.
The results from this training is displayed in Table \ref{tab:comparison_of_feature_occurences_only}.
This table also includes the parameter values that were selected from the grid search. 
Since the amount of questions would vary, a new classifier model was made using the unprocessed data set. 
However, this time the data set only contained the questions which had at least one occurrence of the given feature. 
These are represented in the column 'Rows', where the value indicates how many questions contained it (e.g. there were only 160 questions that contained at least one hexadecimal value).
This table also gives more insight into how many of the external tags were not included when using all the features. 
Both attached and external tags were present in 19,967 questions, but all the features (excluding 'assignment' and external tags) were only present in 17,558 (thus excluding 2409 questions).
Hexadecimal has not had any change, and is still equal to the unprocessed.
Numerical is now below the unprocessed, but in this case the classifier was based on 9,024 questions, instead of 20,000.
% what is a bit weird - why? is that all the features with amount > 9k is RBF, but codeblocks uses linear
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c | c | c | c |}
		\hline
		~				& Unprocessed			& Feature						& C				& Gamma ($\gamma$)	& Kernel	& Rows	 		\\ \hline
		Code block	 	& Accuracy: 78.84\%		& Accuracy: 79.04\%				& 1				& N/A				& Linear 	& 9,855 		\\ \hline
		Hexadecimal		& Accuracy: 81.25\%		& Accuracy: 81.25\%				& 1				& N/A				& Linear	& 160 			\\ \hline
		Homework 		& Accuracy: 84.00\%		& Accuracy: 82.67\%				& 1				& N/A				& Linear	& 374 			\\ \hline
		Links			& Accuracy: 83.72\%		& Accuracy: 81.78\%				& 1				& N/A				& Linear	& 2,575			\\ \hline		
		Numerical		& Accuracy: 80.22\%		& Accuracy: 79.66\%				& 1000			& 0.0001			& RBF		& 9,024			\\ \hline
		Tags			& Accuracy: 79.36\%		& Accuracy: \%					& 1000			& 0.0001			& RBF		& 19,967		\\ \hline
		All features	& Accuracy: 79.24\%		& Accuracy: 79.15\%				& 1000 			& 0.0001			& RBF 		& 17,558		\\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and singular features, for questions containing the given feature. Classifier: SVC.}
	\label{tab:comparison_of_feature_occurences_only}
\end{table}
\vspace{0.5em}\newline
Table \ref{tab:unprocessed_vs_all_feature_detectors_svc_so} shows a comparison between the classifier for unprocessed and all features. 
This also includes a comparison of stemmed vs non-stemmed features. 

There were two stemmers that were considered, Porter and Lancaster.
Lancaster is based on Porter, but is more aggressive, and what I experienced was that words which was not even of the same root (e.g. 'user' and 'using' both became 'us') \cite{Textprocessing.com2016}.
The expectation was that stemming would enhance the prediction and increase the prediction accuracy, but this was not the case. 
When compared with the unprocessed, the accuracy score was 3.93\% lower, and 3.15\% lower than the data set with all feature detectors without stemming.
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline
		~ 					& Unprocessed			& All features			& All features (no stemming)	\\ \hline
		Score 				& 79.90\%				& 75.97\%				& 79.12\%						\\ \hline
		Gamma ($\gamma$)	& 1e$^{-04}$ (0.0001)	& 1e$^{-03}$ (0.001)	& 1e$^{-03}$ (0.001) 			\\ \hline
	\end{tabular}
	\caption{Comparison of the classifier for the raw (unprocessed) questions vs. questions with all features. Classifier: SVC, Kernel=RBF and C=1000.}
	\label{tab:unprocessed_vs_all_feature_detectors_svc_so}
\end{table}
\vspace{0.5em}\newline
Table \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so} shows a comparison between unprocessed and all features (stemmed) using the \gls{sgd} classifier.
A full exhaustive search was also done, but this is omitted because it resulted in the same values as for loss='log'.
% a bit funky sentence; Re-write?
The reason that two exhaustive searches were used is because the possibility to get a prediction probability is only available for loss='log' when using \gls{sgd}.
Probability was a desired functionality, because then the user would upon entering a question, not only be told this is a good/bad question, but also get the probability.
As expected, when comparing Table \ref{tab:unprocessed_vs_all_feature_detectors_svc_so} with Table \ref{tab:unprocessed_vs_all_feature_detectors_sgd_so}, \gls{svc} did get a slightly higher score.
The difference is barely notable, since the classifier for the unprocessed \gls{svc} achieved a score of 79.90\% vs. \gls{sgd} which achieved 79.87\% (only 0.3\% difference).
Whereas for the classifier using all features, \gls{svc} achieved a score 75.97\% and \gls{sgd} achieved 75.55\% (0.42\% difference). 
This could indicate that if one were to increase the sample size to 30,000 or higher, \gls{sgd} could be a better classifier.
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c |}
		\hline
		~				& Unprocessed (loss='log')	& All features (loss='log')	\\ \hline
		Score 			& 79.87\%					& 75.55\%					\\ \hline
		Min DF 			& 0.01						& 0.01						\\ \hline
		Max DF 			& 0.5						& 0.75						\\ \hline
		Use IDF			& False						& True						\\ \hline
		Alpha 			& 1e$^{-05}$ (0.00001)		& 1e$^{-05}$ (0.00001)		\\ \hline
		Normalization 	& l2						& l2						\\ \hline		
		Penalty 		& elasticnet				& l2						\\ \hline
		Iteration 		& 50						& 100						\\ \hline
		Loss 			& log						& log						\\ \hline		
	\end{tabular}
	\caption{Comparison of the classifier for the raw (unprocessed) questions vs. questions with all features. Classifier: SGD.}
	\label{tab:unprocessed_vs_all_feature_detectors_sgd_so}
\end{table}
\vspace{0.5em}\newline
% TODO: merge this into the related paragraphs (if needed)
In Appendix \ref{app:confusion_matrix}, \pageref{app:confusion_matrix}, the confusion matrices for all these are listed. 
The confusion matrix is based on the test set that was created using train\_test\_split.
These show a clear indication that the system is much better at predicting bad questions, then good.
This also explains why it was easier to spot the bad questions, rather than a good one when attempting to find features. 
\\
When looking at the confusion matrices presented in Appendix \ref{app:confusion_matrix}, p.~\pageref{app:confusion_matrix}, the features worsens the predictability (numerical being the exception).
The results of Tags and Homework are questionable, simply because they contain features (external and homework) not represented in the classifier for all the features.
Based on the parameters that was used for the unprocessed data set, there is no improvement of prediction for good, and they worsen the prediction for bad questions.
One exception is the feature for Hexadecimal, but this has no impact on neither good or bad questions, and is only present in 160 questions. 
This means that if this were not used as a feature, it would not be present at all, since all words with a frequency less than 1\% will be excluded. 
The same can be said for homework, considering that this also includes the word 'assignment' (e.g. if 'assignment' was excluded, would it still be in over 1\% of the questions).
One thing that is interesting is the comparison of the stemmed and non-stemmed classifiers for the \gls{svc}.
The stemmed classifier has the worst prediction, but the non-stemmed classifier using its own parameters have better prediction for bad questions.
The \gls{sgd} is better at predicting bad questions, but performs worse for good.
When comparing the unprocessed \gls{sgd} classifier against the one using all features, it is much better at predicting good questions.
For the singular features, code samples, hexadecimal, homework and links do not have that much variation from the unprocessed (but the prediction is still worse).
The numerical feature is the only feature making a difference, with an equal classification prediction for bad, and even better for good (only 375 misclassified as bad).
\\
The comparison of singular features vs. unprocessed showed a worse results for most of the features, even numerical (the sconfusion matrices are shown in Appendix \ref{app:conf_matrix_singular_only}, 
 p.~\pageref{app:conf_matrix_singular_only}). 
One thing that needs to be taken into consideration is the total sample size that was used for training and testing. 
When all questions were used, a total of 16,000 questions was used for training and 4,000 used for evaluation.
Table \ref{tab:amount_of_singular_questions_processed} shows the number of good and bad questions containing at least one of the given feature detectors.
Table \ref{tab:questions_used_for_singular_training} shows the number of questions used for training and evaluation based on their class.
The feature for code blocks is better at classifying bad questions. 
This could indicate that bad questions include more code samples than those writing good questions (e.g. as shown in PAPER, where they found code samples should be short).
However, no analysis was done to the code samples, but some questions did use the <code> tag on words like 'Integer' and 'Double'.
If external tags were to be included as a feature detector, adjustments would need to be made, e.g. using only programming related terms and ignore the rest.
The feature detector using all has now a better predictability than the unprocessed for bad questions, but it is still worse on good questions.
As for numerical, there are more bad than good questions, which means that it will lean toward classifying questions as bad rather than good.
% table here
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline
		~ 					& Bad: -1			& Good: 1		& Total		\\ \hline
		Code block			& 5,090				& 4,765			& 9,855		\\ \hline
		Hexadecimal			& 109				& 51			& 160		\\ \hline
		Homework			& 261				& 113			& 374		\\ \hline
		Links				& 778				& 1,798			& 2,575		\\ \hline
		Numerical			& 5,804				& 3,220			& 9,024		\\ \hline
		Tags				& 9,987				& 9,980			& 19,967	\\ \hline
		All features		& 8,466				& 9,092			& 17,558	\\ \hline
	\end{tabular}
	\caption{The number of questions containing the given feature.}
	\label{tab:amount_of_singular_questions_processed}
\end{table}


% table here
\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c | c | c |}
		\hline
		~ 					& Training: -1		& Training: 1		& Evaluation: -1	& Evaluation: 1	\\ \hline
		Code block			& 4102				& 3782				& 988				& 983			\\ \hline
		Hexadecimal			& 88				& 40				& 21				& 11			\\ \hline
		Homework			& 205				& 94				& 56				& 19			\\ \hline
		Links				& 627				& 1433				& 151				& 365			\\ \hline
		Numerical			& 4650				& 2569				& 1154				& 651			\\ \hline
		Tags				& 8002				& 7971				& 1985				& 2009			\\ \hline
		All features		& 6795				& 7251				& 1671				& 1841			\\ \hline
	\end{tabular}
	\caption{The number of questions used for evaluation.}
	\label{tab:questions_used_for_singular_training}
\end{table}

\todo[inline]{this is not intended to be in the final version, just commentary before re-write}
To see if the system was indeed expandable for communities in \gls{se}, Tex.StackExchange was also tested to see which would be more predictive 
(tables are shown in Appendix \ref{app:tables_tex}, p.~\pageref{app:tables_tex}). 
The results clearly shows that \gls{so} is more predictive, given the amount of data available when compared to Tex.StackExchange\footnote{
	However, it should be noted that this dataset was downloaded in August 2015, but the latest post in that database was from 2014.
}




\clearpage
\section[Artificial Intelligence Methods]{\glsentrylong{ai} (\glsentryshort{ai}) Methods}
\label{sec:ai_methods}

\todo[inline]{main goal is to write about alternative methods and potential limitations with using svm, etc.}


\begin{comment}

For Discussion chapter:
--------------------

% Re-write and use what ever fits
In the \gls{bow} model, only single words or word stems are used as features for representing document content. 
The issue is that learning algorithms are restricted to detecting patterns in the used terminology only, while ignoring conceptual patterns. 
List of weaknesses with using \gls{bow} (1-3 addressed issues on a lexical level, and 4 conceptual level):
\begin{enumerate}
\item Multi-Word Expressions with an own meaning like "European Union" are chunked into pieces with possibly very different meanings like "union".
\item Synonymous Words like "tungsten" and "wolfram" are mapped into different features.
\item Polysemous Words are treated as one single feature while they may actually have multiple distinct meanings.
\item Lack of Generalization: there is no way to generalize similar terms like "beef" and "pork" to their common hypernym "meat".
\end{enumerate}
WordNet database organizes simple words and multi-word expressions of different syntactic categories into so called synonym sets (synsets), 
each of which represents an underlying concept and links these through semantic relations. \\
Conceptual Document Representation:
\begin{itemize}
\item Candidate Term Detection: Strategy built on the assumption that if you find the longest multi-word expressions in the text, 
the lexicon will lead to a mapping to the most specific concept for that word (instead of querying single words, which may lead to wrong mapping).
\item Syntactical Patterns:Analysis by using POS-tagging.
\item Morphological Transformations: Entry form, base form reduction. 
Stemming if the first query for the inflected forms on the original lexicon turned out unsuccessful.
\item Word Sense Disambiguation (WSD): A lexical entry for an expression does not necessarily imply a one-to-one mapping to a concept in the ontology. 
\item Disambiguate an expression versus multiple possible concepts.
\item Generalization: Going from specific concepts in the text to general concept representations. Mapping words based on generalization (up to a certain level).
\end{itemize}
\cite{Bloehdorn2004}

% Re-write and use what ever fits
\gls{qc}: predict the entity type of the answer of a natural language question, mostly achieved by using machine learning. 
Used Latent Semantic Analysis (LSA) technique to reduce the large feature space of questions to a much smaller and efficient feature space. 
Two different classifiers: Back-Propagation Neural Networks (BPNN) and Support Vector Machines (SVM). 
Found that using LSA on question classification made it more time efficient and improved classification accuracy by removing redundant features. 
Discovered that when the original feature space is compact and efficient, its reduced space performs better than a large feature space with a rich set of features. 
They also found that in the reduced feature space, BPNN was better than SVM.  
Competitive with state-of-the-art, even though they used smaller feature space.
\cite{Loni2011}

Sentiment and such: \cite{Maas2011}	

------------------------

Note to self: Map graph over feature impact (unprocessed, singular, all)
Also add in estimated training time for exhaustive search (e.g. ~120 minutes for SVC vs ~100 for SGD over 16k questions (since 4k = test)). 


% could be re-phrased into "in this paper, they define question analysis with two different approaches..."
In the paper by \textcite{Toba2011}, they experiment with the use of statistical learning to find the expected answer pattern for factoid \gls{qa} pairs. 
E.g. if you ask someone where a certain event took place, the answer pattern would be a location. 
They group question analysis into two approaches; pattern-based (high precision, low recall) and \gls{ml} (high recall, 
low precision\footnote{
Low precision can occur if the feature sets are not fitted well enough during classifier training 
\cite[p.~283]{Toba2011}.
}). 
Pattern-based would match word sequences against a set of patterns (e.g. regular expressions), whereas \gls{ml} would be based on the accuracy of the classifier 
(e.g. lexical or linguistic feature sets). 
% editor space
The retrieval of \gls{qa} pairs is done by using a statistical relation framework: Bayesian Analogical Reasoning (BAR). 
Features sets are extracted from the training set by use of binary values checking if the question contains a given question word. 
The BAR framework then learns the related features and computes the estimation for them. 
Thereafter \gls{qa} pairs are retrieved from the testing set and compared against the training set. 
Afterwards, the \gls{qa} pairs that have identical question words are identified, and overlapping pairs are grouped according their named entity group.
To retrieve named entities, they used two different recognizers. 
The first was Stanford (extracts the person, organization and location), and the second was dictionary based (extract number entities and fine-grained noun-based entities). 
Question words were extracted by building a question word list from the training set (achieved by using Stanford Part-of-Speech (POS) tagger). 
Then for each question, look for the appearance of the question word to create the feature set.
Mapped named-entities.
\cite{Toba2011}

Potentially move "all this failed and went wrong" here



%\section{Implementation Architecture}
%\label{sec:implemented_architecture}
discussion on the code that was written and its functionality \\
what worked, what should be updated/changed, etc.
\end{comment}

\clearpage
\section{Limitations and other issues}
\label{sec:limitations_and_issues}
One of the major issues was the fact that the latest development version was used instead of the stable one.
The question became at one point whether or not a switch should be made from using the development version into the stable version.
There were two things that needed to be taken into consideration. 
First of all, it was unknown when this development version would become the next stable one.
If the development version became the new stable version, a lot of alterations would have to be made to the source code.
Furthermore, for the long term, if this system would become successful, it would be easier to maintain in the future if it relied on the latest version.
\vspace{0.5em}\newline
\todo[inline]{Will be re-written based on feedback}
When you are only one person with only one computer there is a certain limitation to how much work can be done simultaneously.
A lot of time were spent having to rebuild the database and processing the data (e.g. finding features, replacing it, training classifiers).
To ensure expandability (for the \gls{se} community) and ensuring replicability (having a unprocessed data set), more than once code had to be updated or re-written. 
This easily caused a lot of unforeseen issues.
The worst one was the realization that after the update of the unprocessed data set (changing it to contain HTML like in the database), the HTML was not removed from the text.
The impact was that all the models that were created using the parameters and data from the unprocessed had to be re-trained to get correct results. 
\vspace{0.5em}\newline
One of the more peculiar issues (which I do not have an explanation for), was when the real training started (using the \gls{svc} and GridSearchCV).
The reason that this issue cannot be explained, is because I do not know what caused it to happen.
What happened was that when the training was started, no verbose was printed at all (which was weird considering I had used the same values before and it then gave a verbose output). 
The program ran for hours, without giving any feedback, errors or output. 
\vspace{0.5em}\newline
It took almost three days to find a solution, where part of the solution was switching to Windows (which had its own issues, since x64 is not supported by Numpy).
The main difference was that in Windows, at least verbose was printed, although it only printed verbose once\footnote{
	Verbose was printed once, sometime twice between the first 20 - 60 minutes, then nothing.
	The longest run time that was registered was around 12 hours without any verbose printed.
}.
There were two things that were changed, which finally made it both print verbose and complete the training.
The first part was changing the n\_jobs value to something else than -1. 
By setting n\_jobs=-1, GridSearchCV will run all jobs in parallel, using all logical cores (e.g. on a CPU with 4 physical cores, it will use all 8 logical cores). 
However, multi-threading is not supported in Windows \cite{GS2015}. 
To ensure that progress was made, and that the program had not frozen again, the verbose level was increased.
By increasing the verbose level, you can force the algorithm to print more information about progress, but with an increase in the time it takes to finish \cite{Manuel2015, user29912432014}. 
The most fascinating part was that after increasing the verbose level, it not only printed out continuously, but it even finished training in less than 3 hours!
This have not been tested in Linux, but the reason nothing happened may have been the same which happened for Windows, that it could not utilize all CPU cores\footnote{
	My assumption is that when all the logical cores are used, there is no processing power left for the Operative System (OS). 
	This in turn would then cause an infinite deadlock, since by using all the cores, there is no processing power left for the OS.
	There is also a known issue with parallelization in Linux, see: 
	\url{https://pythonhosted.org/joblib/parallel.html\#bad-interaction-of-multiprocessing-and-third-party-libraries}.
}.























