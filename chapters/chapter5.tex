%\chapter{Discussions}
\label{chap:chapter5}

\todo[inline]{for now this chapter is a bit messy due to the tables appearing all over the place. please ignore the "readability" issues these causes for now}
\todo[inline]{chapter \ref{sec:data_and_testing} is temporarily done, the same goes for \ref{sec:development} (might add more to that later on)}

\section{Data set and Question selection}
\label{sec:data_and_testing}
The dataset was retrieved from Stack Exchange Archive \cite{StackExchange2016}, and contains all the data posted from the beginning. 
In this thesis, only the data for \gls{so} was used, specifically only the posted questions.
A simplified overview is shown in Table \ref{tab:dataset_overview_so}, which lists questions based on score. 

\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c | c | c |}
		\hline
		~				& Amount		& Oldest		& Newest		& Vote (lowest)		& Vote (highest)	\\ \hline
		Votes < 0		& 659,955		& 06.08.2008	& 06.03.2016	& -147				& -1				\\ \hline
		Votes = 0		& 5,256,105		& 06.08.2008	& 06.03.2016	& 0					& 0					\\ \hline
		Votes > 0		& 5,286,971		& 31.07.2008	& 06.03.2016	& 1					& 13845				\\ \hline
		All questions	& 11,203,031	& 31.07.2008	& 06.03.2016	& -147				& 13845				\\ \hline
	\end{tabular}
	\caption{Overview of the Stack Overflow dataset.}
	\label{tab:dataset_overview_so}
\end{table}
As previously mentioned in Section \ref{sec:feature_sets}, the value used to extract the questions was set to -5 and +50. 
The problem with these values are that they were simply selected. 
Originally, the value for bad questions was -10, which were then changed to -5 because -10 only retrieved 683 rows (and I wanted 10,000 samples for both question types).
However, as can be seen from Table \ref{tab:dataset_overview_so}, +50 is most likely a to low value for the good questions. 
To get better results based on desired quality, there are three options to consider which could improve this.
\vspace{0.5em}\newline
The first is the easiest. If you are only interested in a set sample size, then you could simply retrieve the amount sorted by score.
If you then want 10,000 of each, you would then get the 10,000 that are scored highest (or lowest).
The second option would be to get a set limit based on the actual score, by using the mean or average.
For all the questions of a given type, retrieve the average and then select questions which has a score higher (or lower) than the average.
The third option would be to use quartiles. 
Quartiles is measured by a given percentage of the observations, where each quartile represents 25\%: Q$_{1}$ (25\%), M (median, 50\%) and Q$_{3}$ (75\%) \cite{Hagen2011}.
The equation to calculate the quartiles are shown in Equation \ref{eq:quartile1} - \ref{eq:quartile3}.
% equation: Quartile 1
\begin{equation}\label{eq:quartile1}
Q_{1} = \frac{(n + 1)}{4}
\end{equation}
% equation: Quartile 2
\begin{equation}\label{eq:quartile2}
M = \frac{(n + 1)}{2}
\end{equation}
% equation: Quartile 3
\begin{equation}\label{eq:quartile3}
Q_{3} = \frac{3 \cdot (n + 1)}{4}
\end{equation}
As previously mentioned in Section \ref{sec:feature_sets}, there was no clear indicator for the good questions. 
Some consisted only of two sentences, e.g. "I committed the wrong files to Git. How can I undo this commit?"\footnote{
	This question currently have a score of 10,406: \\ 
	\url{http://stackoverflow.com/questions/927358/how-do-you-undo-the-last-commit}.
}. Common for most of the top-voted are that they are mostly short, however you also have those that are long\footnote{
	One example is this question, with a score of 3,009: \\
	\url{http://stackoverflow.com/questions/826782/css-rule-to-disable-text-selection-highlighting}.
}.
There is also what I would call a bias factor. 
If enough people have the same problem, then it will automatically become a good question. 
Not because of the questions quality, but simply because many will encounter it (e.g. Bugs, \gls{ide} behaviour, tweaks, etc).
The same can be said for bad questions. 
In many cases, a question is bad not because of the question that is asked, but simply because it does not follow the guidelines. 
If a question is a duplicate, it is automatically voted for closure, and will in most cases receive multiple down-votes. 
Questions which gives a hint of being school related will also receive down-votes, mainly because \gls{so} targets professionals and experts. 
\gls{so} is neither a fan of unnecessary text like greetings and gratefulness \cite{CommunityWiki2016a,Heyer2012}.
\vspace{0.5em}\newline
Is \gls{so} fit for measuring question quality?
In a closed domain setting, the answer is yes. 
However, as mentioned, what type of quality being measured must be taken into consideration. 
As it is, the system would not be useful for an educational setting, because it bases its prediction on questions asked on \gls{so}.
If a student were to ask the system a question, it might respond saying that this is a bad question (but the result may be based on the fact that it is a duplicate).
For an educational setting, a better solution would be to develop a system similar to the one in \cite{Lezina2013}.
With the current state of the system (if it were to be used), it would be more appropriate to use it as a measurement tool for new \gls{so} questions, rather then general question quality.

\section{Development}
\label{sec:development}
There was a lot of back and forth during the development, with a lot of time being invested in properly understanding how to use Scikit-learn.
The most useful resource were of course Scikit-learns tutorials \cite{Scikitlearn.org2016j}\footnote{
	The examples that were worked through can be seen here (as stated in ReadMe, the only thing I have altered is comments and adjustments for v0.18.dev0): \\
	\url{https://github.com/klAndersen/scikit-learn_tutorials}.
	} and documentation \cite{Scikitlearn.org2016,Scikitlearn.org2016h}, 
but there were also two other tutorials that proved helpful (\cite{Rehurek2014} and \cite{Elahi2016}).
The main problem was that Scikit-learns documentation and most of the tutorials written by others is based on v0.17.1.
Due to various problems with installation in regards to Numpy and Scipy, Scikit-learn had to be installed from GitHub (which is v0.18.dev0).
In the development version, a lot of functions and modules had been re-written and moved, making many tutorials and examples outdated.
One example is the code snippet in \cite["In $\lbrack$42$\rbrack$"]{Rehurek2014}, where '\emph{cv=StratifiedKFold(label\_train, n\_folds=5)}' fails, 
because it expects the first argument to be n\_folds\footnote{
	Documentation for v0.17.1: \\
	\url{http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html}. \\
	Documentation for v0.18.dev0: \\
	\url{http://scikit-learn.org/dev/modules/generated/sklearn.model_selection.StratifiedKFold.html}.
}.
\vspace{0.5em}\newline
\textcite{Rehurek2014} used in his example \gls{svc} to train the \gls{svm}, but no explanation to why these parameter values\footnote{
	I later found that these were just the basic numbers used in one of their examples for the grid search: 
	\url{http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html}.
},
nor how he selected the best parameter from the printout\footnote{
	The score, parameters and estimator can be found by using these attributes: '\emph{best\_score\_}', '\emph{best\_params\_}' and '\emph{best\_estimator\_}'.
} (mine is shown in Listing \ref{lst:svc_tutorial}).

\begin{lstlisting}[caption={Output from SVC tutorial (testing with linear, rbf and sigmoid)}, label={lst:svc_tutorial}] 
[mean: 0.80232, std: 0.01056, params: {'classifier__kernel': 'linear', 'classifier__C': 1}, 
mean: 0.78451, std: 0.01750, params: {'classifier__kernel': 'linear', 'classifier__C': 10}, 
...(15 lines removed)...
mean: 0.71416, std: 0.00101, params: {'classifier__gamma': 0.0001, 'classifier__kernel': 
'sigmoid', 'classifier__C': 1000}]
\end{lstlisting}

\begin{comment}
Should this be used, is this relevant? Seems a bit like whining over everything that failed, etc.

Another issue was that since few tutorials had the data sets available, it was hard to compare my results against theirs to (considering version changes).
In Scikit-learns tutorial for text data \cite{Scikitlearn.org2016h}, they have at the end task suggestions, with attached solutions in the Scikit-learn library.
When looking at the solution for the second task (sentiment analysis), I saw that they used \gls{sgd} (see \cite{Scikitlearn.org2016f}).
Looking at the suggested "road map" (see Appendix \ref{app:ml_map}, Figure \ref{fig:ml_map} on p.~\pageref{app:ml_map}), 
I accidentally misread a zero and decided to go with \gls{sgd} instead of \gls{svc}.
\vspace{0.5em}\newline
When understanding more about how GridSearchCV worked and how one could select the best parameters from it, things got back on track. 
However, when comparing it against the \gls{svc}, it turned out that the \gls{svc} got a higher score\footnote{
A comparison was done on \gls{sgd}, \gls{svc} and LinearSVC using GridSearchCV.
}. 
\end{comment}

% something about parameter selection
% something about the results
% something about using probability prediction (e.g. taking twice the time, worse results for SGD, etc.)

\section{Parameter selection for GridSearchCV and the results}
\label{sec:grid_search_and_results}
There were two different classifiers that were used on the training data, \gls{svc} (parameters shown in Listing \ref{lst:param_svc}) and \gls{sgd} 
(parameters shown in Listing \ref{lst:param_sgd}).
The values for the \gls{svc} are based on the example from \textcite{Scikitlearn.org2016k}, the same goes for the values for the \gls{sgd} \textcite{Scikitlearn.org2016l} 
(\cite{Huang2008, Maas2011, Zhang2003} used the default values when training their system). 
The reason that these two were used were because of the "roadmap", shown in the Appendix \ref{app:ml_map}, Figure \ref{fig:ml_map} on p.~\pageref{app:ml_map}. 
The reason for including \gls{sgd} was partially because I misread a zero, but also because if one were to extend the sample size, \gls{sgd} could potentially get better results.
% Listing for the SVC parameters
\begin{lstlisting}[caption={Parameters for SVC}, label={lst:param_svc}] 
param_svm = [
{'clf__C': [1, 10, 100, 1000], 'clf__kernel': ['linear']},
{'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['rbf']},
{'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['sigmoid']},
]
\end{lstlisting}
% Listing for the SGD parameters
\begin{lstlisting}[caption={Parameters for SGD}, label={lst:param_sgd}] 
grid_parameters = {
'vect__min_df': (0.01, 0.025, 0.05, 0.075, 0.1),
'vect__max_df': (0.25, 0.5, 0.75, 0.95, 1.0),
'tfidf__use_idf': (True, False),
'tfidf__norm': ('l1', 'l2'),
'clf__alpha': (0.00001, 0.000001),
'clf__penalty': ('l1', 'l2', 'elasticnet'),
'clf__n_iter': (10, 50, 75, 100),
# 'clf__loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'),
}
\end{lstlisting}
% editor space
Only 20,000 questions were selected, and it would probably had been better to have selected more than this. 
However, one need to take in consideration the time available, and the time it takes to process and train all the classifiers. 
When using exhaustive search it matches all the parameters against each other, to see which one has the best combination. 
This means that if you have 30 different parameters, and a cross-validation of 10, then the total amount of fits is 300 \cite{Markham2015a}.
The fit count is shown in Equation \ref{eq:fit_svc} for \gls{svc} and in Equation \ref{eq:fit_sgd} for \gls{sgd}.
% Equation for the amount of fits for SVC
\begin{equation}\label{eq:fit_svc}
\begin{split}
\text{count(C)} + (\text{count(C)} \cdot \text{count(gamma)}) + (\text{count(C)} \cdot \text{count(gamma)}) \\
\Longrightarrow  \text{result} \cdot \text{cross-validation} = \text{fit\_amount} \\
\Longrightarrow  4 + (4 \cdot 2) + (4 \cdot 2) = 20 \cdot 5 = 100 
\end{split}
\end{equation}
% Equation for the amount of fits for SGD
\begin{equation}\label{eq:fit_sgd}
\begin{split}
\text{count(min\_df)} \cdot \text{count(max\_df)} \cdot \text{count(use\_idf)} \cdot \text{count(norm)} \cdot \text{count(alpha)} \\ 
\cdot ~ \text{count(penalty)} \cdot \text{count(n\_iter)} \cdot \text{count(loss)} \Longrightarrow  result \cdot \text{cross-validation} = \text{fit\_amount} \\
\Longrightarrow  5 \cdot 5 \cdot 2 \cdot 2 \cdot 2 \cdot 3 \cdot 4 = 2400 \cdot 5 = 12,000
\end{split}
\end{equation}
% editor space
%These results must then be multiplied with two, because if you want a probability for the prediction it needs to run twice to calculate this.
A faster alternative to GridSearchCV would be RandomizedSearchCV, but as stated in \cite{Markham2015a}, it may give lower scores. 
His suggestion is to start with GridSearchCV, and then compare the results against RandomizedSearchCV.
This was not done in this thesis, as it did not seem relevant for thesis.
However, for the process of fine-tuning the parameters, RandomizedSearchCV would perhaps have been a better option. 
One could then have started with a high spread of values to then narrow it down, and finally get the optimal ones using GridSearchCV.
 
\todo[inline]{will add discussions on features and results after all new models have been created - Issue: Unprocessed was trained with HTML in text}
To be written:
\begin{enumerate}
	\item Discussion on features
	\item Classifier and features; none, singular, all
	\item Classifier and features; none, singular, all - for questions that contain the given feature
	\item Classifier and estimator
	\item Site tags and "assignment": Excluded from training set using "all" features
	\item \gls{so} dataset vs. tex.stackexhange
	\item Stemming - did this improve or worsen the results? Regardless of results, why?
	\item Stemming - Porter vs. Lancaster (e.g. issue with certain words not being stemmed, vs. too much stemming (non-related words))
	\item The cost of probability (i.e. \gls{sgd})
	\item Presumption that max\_df=95\% was a smart move (was it really when comparing to SGD using all?)
\end{enumerate}

\todo[inline]{these tables and listings are temporarily placed here and might be moved into Appendix at a later point.}

\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c |}
		\hline
		~ 			& Unprocessed		& Features	\\ \hline
		Score 		& 0.993				& 0.993		\\ \hline
		C			& 1					& 1			\\ \hline
		Kernel		& Linear			& Linear	\\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and singular feature detectors for Tex.StackExhange (August 2015 data set).}
	\label{tab:singular_feature_detector_tex}
\end{table}


\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c |}
		\hline
		~ 				& Score	\\ \hline
		Code samples 	& 0.783	\\ \hline
		Numerical		& 0.796	\\ \hline
		Hexadecimal		& 0.793	\\ \hline
		Homework 		& 0.794	\\ \hline
		Link			& 0.795	\\ \hline
		Tags			& 0.757	\\ \hline
		Unprocessed		& 0.804 \\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and singular feature detectors. Using SVC, with Kernel=RBF, C=1000 and Gamma=($\gamma$) 0.001}
	\label{tab:singular_feature_detector_so}
\end{table}


\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline
		~ 					& Unprocessed (SVC)	& All features	& All features (no stemming)	\\ \hline
		Score 				& 0.804				& 0.793			& 0.784							\\ \hline
		C					& 1000				& 1000			& 1000							\\ \hline
		Gamma ($\gamma$)	& 0.001				& 0.001			&  0.001							\\ \hline
		Kernel				& RBF				& RBF			& RBF							\\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and all feature detectors (SVC)}
	\label{tab:unprocessed_vs_all_feature_detectors_svc_so}
\end{table}

\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c| c | c | c | c |}
		\hline
		~				& Unprocessed		& Unprocessed (loss='log')		& All features (loss='log')	\\ \hline
		Score 			& 0.8073125				& 0.8071875					& 0.79325					\\ \hline
		Min DF 			& 0.01					& 0.01						& 0.01						\\ \hline
		Max DF 			& 1.0					& 1.0						& 0.75						\\ \hline
		Use IDF			& False					& True						& True						\\ \hline
		Alpha 			& 1e-05					& 1e-05						& 1e-05						\\ \hline
		Normalization 	& l1					& l2						& l2						\\ \hline		
		Penalty 		& l1					& l2						& l2						\\ \hline
		Iteration 		& 100					& 75						& 100						\\ \hline
		Loss 			& hinge					& log						& log						\\ \hline		
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and all feature detectors (SGD)}
	\label{tab:unprocessed_vs_all_feature_detectors_sgd_so}
\end{table}


\begin{table}[!h]%[tbp]
	\centering
	\begin{tabular}{| c | c | c | c | c | c | c |}
		\hline
		~				& Unprocessed		& FEATURE		& C			& Gamma ($\gamma$)	& Kernel	& Amount	 	\\ \hline
		Code samples 	& Score: 0.802		& Score: 0.784	& 1000		& 0.001				& RBF 		& 9,855 		\\ \hline
		Numerical		& Score: 0.834		& Score: 0.815	& 1			& N/A				& Linear	& 9,024			\\ \hline
		Hexadecimal		& Score: 0.859		& Score: 0.883	& 1000		& 0.001				& RBF		& 160 			\\ \hline
		Homework 		& Score: 0.873		& Score: 0.859	& 1000		& 0.001				& RBF		& 374 			\\ \hline
		Link			& Score: 0.885		& Score: 0.838	& 1			& N/A				& Linear	& 2,575			\\ \hline
		Tags			& Score: 0.806		& Score: 0.757	& 1000		& 0.001				& Sigmoid	& 19,967		\\ \hline
		All features	& Score: 0.808		& Score: 0.788	& 1000		& 0.001				& RBF 		& 17,558		\\ \hline
	\end{tabular}
	\caption{Comparison of raw data set (unprocessed) and singular features, for questions that had it.}
	\label{tab:comparison_of_feature_occurences_only}
\end{table}


\clearpage
\section[Artificial Intelligence Methods]{\glsentrylong{ai} (\glsentryshort{ai}) Methods}
\label{sec:ai_methods}

\todo[inline]{main goal is to write about alternative methods and potential limitations with using svm, etc.}


\begin{comment}

For Discussion chapter:
--------------------

% Re-write and use what ever fits
In the \gls{bow} model, only single words or word stems are used as features for representing document content. 
The issue is that learning algorithms are restricted to detecting patterns in the used terminology only, while ignoring conceptual patterns. 
List of weaknesses with using \gls{bow} (1-3 addressed issues on a lexical level, and 4 conceptual level):
\begin{enumerate}
\item Multi-Word Expressions with an own meaning like "European Union" are chunked into pieces with possibly very different meanings like "union".
\item Synonymous Words like "tungsten" and "wolfram" are mapped into different features.
\item Polysemous Words are treated as one single feature while they may actually have multiple distinct meanings.
\item Lack of Generalization: there is no way to generalize similar terms like "beef" and "pork" to their common hypernym "meat".
\end{enumerate}
WordNet database organizes simple words and multi-word expressions of different syntactic categories into so called synonym sets (synsets), 
each of which represents an underlying concept and links these through semantic relations. \\
Conceptual Document Representation:
\begin{itemize}
\item Candidate Term Detection: Strategy built on the assumption that if you find the longest multi-word expressions in the text, 
the lexicon will lead to a mapping to the most specific concept for that word (instead of querying single words, which may lead to wrong mapping).
\item Syntactical Patterns:Analysis by using POS-tagging.
\item Morphological Transformations: Entry form, base form reduction. 
Stemming if the first query for the inflected forms on the original lexicon turned out unsuccessful.
\item Word Sense Disambiguation (WSD): A lexical entry for an expression does not necessarily imply a one-to-one mapping to a concept in the ontology. 
\item Disambiguate an expression versus multiple possible concepts.
\item Generalization: Going from specific concepts in the text to general concept representations. Mapping words based on generalization (up to a certain level).
\end{itemize}
\cite{Bloehdorn2004}

% Re-write and use what ever fits
\gls{qc}: predict the entity type of the answer of a natural language question, mostly achieved by using machine learning. 
Used Latent Semantic Analysis (LSA) technique to reduce the large feature space of questions to a much smaller and efficient feature space. 
Two different classifiers: Back-Propagation Neural Networks (BPNN) and Support Vector Machines (SVM). 
Found that using LSA on question classification made it more time efficient and improved classification accuracy by removing redundant features. 
Discovered that when the original feature space is compact and efficient, its reduced space performs better than a large feature space with a rich set of features. 
They also found that in the reduced feature space, BPNN was better than SVM.  
Competitive with state-of-the-art, even though they used smaller feature space.
\cite{Loni2011}

Sentiment and such: \cite{Maas2011}	

------------------------

Note to self: Map graph over feature impact (unprocessed, singular, all)
Also add in estimated training time for exhaustive search (e.g. ~120 minutes for SVC vs ~100 for SGD over 16k questions (since 4k = test)). 


% could be re-phrased into "in this paper, they define question analysis with two different approaches..."
In the paper by \textcite{Toba2011}, they experiment with the use of statistical learning to find the expected answer pattern for factoid \gls{qa} pairs. 
E.g. if you ask someone where a certain event took place, the answer pattern would be a location. 
They group question analysis into two approaches; pattern-based (high precision, low recall) and \gls{ml} (high recall, 
low precision\footnote{
Low precision can occur if the feature sets are not fitted well enough during classifier training 
\cite[p.~283]{Toba2011}.
}). 
Pattern-based would match word sequences against a set of patterns (e.g. regular expressions), whereas \gls{ml} would be based on the accuracy of the classifier 
(e.g. lexical or linguistic feature sets). 
% editor space
The retrieval of \gls{qa} pairs is done by using a statistical relation framework: Bayesian Analogical Reasoning (BAR). 
Features sets are extracted from the training set by use of binary values checking if the question contains a given question word. 
The BAR framework then learns the related features and computes the estimation for them. 
Thereafter \gls{qa} pairs are retrieved from the testing set and compared against the training set. 
Afterwards, the \gls{qa} pairs that have identical question words are identified, and overlapping pairs are grouped according their named entity group.
To retrieve named entities, they used two different recognizers. 
The first was Stanford (extracts the person, organization and location), and the second was dictionary based (extract number entities and fine-grained noun-based entities). 
Question words were extracted by building a question word list from the training set (achieved by using Stanford Part-of-Speech (POS) tagger). 
Then for each question, look for the appearance of the question word to create the feature set.
Mapped named-entities.
\cite{Toba2011}

Potentially move "all this failed and went wrong" here



%\section{Implementation Architecture}
%\label{sec:implemented_architecture}
discussion on the code that was written and its functionality \\
what worked, what should be updated/changed, etc.
\end{comment}

\clearpage
\section{Limitations and other issues}
\label{sec:limitations_and_issues}
One of the major issues was the fact that the latest development version was used instead of the stable one.
The question became at one point whether or not a switch should be made from using the development version into the stable version.
There were two things that needed to be taken into consideration. 
First of all, it was unknown when this development version would become the next stable one.
If the development version became the new stable version, a lot of alterations would have to be made to the source code.
Furthermore, for the long term, if this system would become successful, it would be easier to maintain in the future if it relied on the latest version.
\vspace{0.5em}\newline
Time was also a major limitation for the development. 
When you are only one person with only one computer there is a certain limitation to how much work can be done simultaneously.
A lot of time were spent having to rebuild the database and processing the data (e.g. finding features, replacing it, training classifiers).
To ensure expandability (for the \gls{se} community) and ensuring replicability (having a unprocessed data set), more than once code had to be updated or re-written. 
This easily caused a lot of unforeseen issues.
The worst one was the realization that after the update of the unprocessed data set (changing it to contain HTML like in the database), the HTML was not removed from the text.
The impact was that all the models that were created using the parameters and data from the unprocessed had to be re-trained to get correct results. 
\vspace{0.5em}\newline
One of the more peculiar issues (which I do not have an explanation for), was when the real training started (using the \gls{svc} and GridSearchCV).
The reason that this issue cannot be explained, is because I do not know what caused it to happen.
What happened was that when the training was started, no verbose was printed at all (which was weird considering I had used the same values before and it then gave a verbose output). 
The program ran for hours, without giving any feedback, errors or output. 
\vspace{0.5em}\newline
It took almost three days to find a solution, where part of the solution was switching to Windows (which had its own issues, since x64 is not supported by Numpy).
The main difference was that in Windows, at least verbose was printed, although it only printed verbose once\footnote{
	Verbose was printed once, sometime twice between the first 20 - 60 minutes, then nothing.
	The longest run time that was registered was around 12 hours without any verbose printed.
}.
There were two things that were changed, which finally made it both print verbose and complete the training.
The first part was changing the n\_jobs value to something else than -1. 
By setting n\_jobs=-1, GridSearchCV will run all jobs in parallel, using all logical cores (e.g. on a CPU with 4 physical cores, it will use all 8 logical cores). 
However, multi-threading is not supported in Windows \cite{GS2015}. 
To ensure that progress was made, and that the program had not frozen again, the verbose level was increased.
By increasing the verbose level, you can force the algorithm to print more information about progress, but with an increase in the time it takes to finish \cite{Manuel2015, user29912432014}. 
The most fascinating part was that after increasing the verbose level, it not only printed out continuously, but it even finished training in less than 3 hours!
It has not been tested in Linux, but the reason nothing happened may have been the same which happened for Windows, that it could not utilize all cores\footnote{
	My assumption is that when all the logical cores are used, there is no processing power left for the Operative System (OS). 
	This in turn would then cause an infinite deadlock, since by using all the cores, there is no processing power left for the OS.
	There is also a known issue with parallelization in Linux, see: 
	\url{https://pythonhosted.org/joblib/parallel.html\#bad-interaction-of-multiprocessing-and-third-party-libraries}.
}.























