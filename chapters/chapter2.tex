%\chapter{State of the art}
\label{chap:chapter2}
The goal of this target is to document the current state of the art technologies, and research 
that has been done by others. This is needed to both understand how to use text classification and 
classify questions. Furthermore, some of the papers have also done comparisons on \gls{svm} and 
other types of \gls{ai} and \gls{ml} algorithms, which concluded that for text classification, 
\gls{svm} had the highest accuracy.

Citation list (content to come):
\begin{itemize}
	\item \cite{ChangLin2011}: Libsvm
	\item \cite{XuZhouWang2012}: Question classification, SVM, semantics
	\item \cite{HsuChangLinEtAl2003}: SVC
	\item \cite{LinLin2003}: Sigmoid kernels SVM
	\item \cite{YenWuYangEtAl2013}: SVM, QA, Context ranking
	\item \cite{NielsenBuckinghamKnollEtAl2008}: Question taxonomy
	\item \cite{Isozaki2005}: Japanese Q/A System
	\item \cite{Movshovitz-AttiasMovshovitz-AttiasSteenkisteEtAl2013}: Reputation system SO
	\item \cite{BloehdornHotho2004}: Text classification, semantics
	\item \cite{HanDingLing-Feng2006}: fuzzy sigmoid svm
	\item \cite{ZhangWuLan2015}: sentiment analysis, linguistics
	\item \cite{ShahPomerantz2010}: Answer quality in qa community
	\item \cite{TobaAdrianiManurung2011}: Answer type construction
	\item \cite{TreudeBarzilayStorey2011}: QA on the web (programmers)
	\item \cite{SlowiaczekKlaymanShermanEtAl1992}: What is a good Q/A?
	\item \cite{LezinaKuznetsov2013}: predict closed questions on SO
	\item \cite{StanleyByrne2013}: tag prediction on SO
	\item \cite{HuangThintQin2008}: Question classification
	\item \cite{ZhangLee2003}: Question classification, svm
	\item \cite{BergstraBengio2012}: random search, hyperparameters
	\item \cite{TongKoller2002}: SVM, text classification
	\item \cite{HearstDumaisOsmanEtAl1998}: svm
	\item \cite{Kaestner2013}: svm, text processing
	\item \cite{Dubin2004}: Salton	
	\item \cite{NguyenNguyenShimazu2008}: semi-supervised, question classification
	\item \cite{RagonisShilo2013}: problem solving question, cs
	\item \cite{Miller1995}: wordnet	
	\item \cite{NasehiSillitoMaurerEtAl2012}: good code example, SO
	\item \cite{HanrahanConvertinoNelson2012}: SO expertise \& knowledge
	\item \cite{WangLoJiang2013}: dev interaction SO
	\item \cite{YangTaoBozzonEtAl2014}: expertise vs activity on SO
	\item \cite{ChengSchiffWu2013}: question feature for answer attraction SO
	\item \cite{ShortWongZeng2014}: tag recommendation SO
	\item \cite{PosnettWarburgDevanbuEtAl2012}: questions, answers, activity and reputation on SO
	\item \cite{AhmedYangJohri2015}: stackexchange, technical vs non-technical communities activity
	\item \cite{FuJiaXu2008}: domain ontology, qa
	\item \cite{LiRoth2002,LiRoth,LiRothSmall} qa classification, semantics
	\item \cite{AndersonHuttenlocherKleinbergEtAl2012} so, qa, post quality	
	\item \cite{SinhaManiGupta2013} qa, design, anwer response	
	\item \cite{Klein2016,SpaceMachine.net2016,Wissner-Gross2016} datasets
%	\item \cite{AndersonHuttenlocherKleinbergEtAl2012}
\end{itemize}

% could be re-phrased into "in this paper, they define question analysis with two different approaches..."
In the paper by \cite{TobaAdrianiManurung2011}, they experiment with the use of statistical 
learning to find the expected answer pattern for factoid \gls{qa} pairs. E.g. if you ask 
someone where a certain event took place, the answer pattern would be a location. They group question analysis into two approaches; pattern-based (high precision, low recall) and \gls{ml} 
(high recall, low precision\footnote{Low precision can occur if the feature sets are not fitted 
	well enough during 
	classifier training \cite[p.~283]{TobaAdrianiManurung2011}.}). Pattern-based would match 
word sequences against a set of patterns (e.g. regular expressions), whereas \gls{ml} would be 
based on the accuracy of the classifier (e.g. lexical or linguistic feature sets). 
\begin{comment}
% this paragraph may not be relevant for this report
The retrieval of \gls{qa} pairs is done by using a statistical relation framework: Bayesian 
Analogical Reasoning (BAR). Features sets are then extracted from the training set by use of binary 
values checking if the question contains a given question word. The BAR framework then learns the 
related features and computes the estimation for them. Thereafter \gls{qa} pairs are retrieved from 
the testing set and compared against the training set. Afterwards, the \gls{qa} pairs that have 
identical question words are identified, and overlapping pairs are grouped according their named 
entity group.

To retrieve named entities, they used two different recognizers. The first was Stanford (extracts the
person, organization and location), and the second was dictionary based (extract number
entities and fine-grained noun-based entities). 

Question words were extracted by building a question word list from the training set (achieved 
by using Stanford Part-of-Speech (POS) tagger). Then for each question, look for the appearance 
of the question word to create the feature set.

Mapped named-entities; e.g. ORGANIZATION became NEorganization.

\end{comment}

\cite{XuZhouWang2012} used \gls{svm} to create an online \gls{qa} tourism system in Chinese. The 
system included question analysis, \gls{ir} and answer extraction. The question classification 
accuracy was important to the overall performance of the system. The system was built using 
\gls{svm} and question semantic similarity, and the feature selection was based on lexical features 
and domain terms hierarchy. The original method for question classification is mainly rule-based, 
where the rules decide the category (difficult to extract all the rules for the question category). 
Another method is using statistics, as was done in \cite{ZhangLee2003} ("to classify questions in English. It uses tree kernel to extract features and classify questions with SVM classifier."). 
Questions were divided into 13 coarse categories and 150 sub-categories. The difference between 
these were that the sub-categories was built on sentences.



\section{Text classification}
\label{sec:text_classification}
Text classification can be done in many different ways, since the content and size rarely will be 
equal. The classification is also based on what you want to retrieve from the text. Do you want 
an answer to a question, or do you want to see which documents are most relevant for the problem 
you are currently working on (e.g. searching for research papers that are relevant to your work).

\section{Question classification}
\label{sec:question_answering}
Question classification and analysis can be seen as a sub-topic of text classification. In most 
papers, the basis is that the user has a question they need an answer to. What is the best way 
to find that answer, and in some cases what is the quickest way to display said answer?
 
\begin{itemize}
	\item question-answering techniques and analyses
	\item ontology and semantics
	\item StackExchange and other online communities for qa (potentially separate section)
\end{itemize}

\section{\glsentrylong{svm} (\glsentryshort{svm})}
\label{sec:svm}
\gls{svm} is the chosen \gls{ml} algorithm that was chosen to use for the question analysis. This 
section is mainly intended as a light introduction to what \gls{svm} is, and what the most common 
uses are. Furthermore, in what way can this be utilized for text and question classification?

\section{Dataset}
\label{sec:dataset}
short about the datasets, others who has used it, and datasets in general, e.g. 
\cite{Klein2016,SpaceMachine.net2016,Wissner-Gross2016}
